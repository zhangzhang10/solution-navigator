{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BKM:\n",
    "*****\n",
    ">1. Config slaves as \"cluster palcement group\", to get best and stable throughput among nodes.\n",
    "2. Open port accesses internally, don't expose port access to public\n",
    "3. Configure security.authorization and ip control through hadoop\n",
    "4. Network throughput depends on instance type. If master node is small instance, avoid to copy large file from master to slaves or HDFS, using one of slavers\n",
    "5. If you want to cache the files to memory, set replication to 1. Otherwise you can't make sure Yarn schedule the same task to the same node always.\n",
    "6. If you copy large file from one slave to whole HDFS, the distribution is biased. The slave holds most of the data in its HDFS. Solution is to copy the file from HDFS to HDFS again\n",
    "7. vcpu isn't map to physical CPU with the same index, so you can't make sure two vcpu doesn't share the same physical core. So pin executors to cores through the native OS policy leads to poor performance. We need to make clear how vcpu share the same core firstly\n",
    "8. \n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients=!cat /etc/hosts\n",
    "clients=[l.split(\" \")[0] for l in clients if l.startswith(\"172\")]\n",
    "masterip='172.31.31.159'\n",
    "hclients=clients.copy()\n",
    "hclients.append(masterip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"sparkuser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh-keygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !ssh root@$l adduser {user}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clients:\n",
    "    !ssh root@$i cp -r .ssh /home/{user}/\n",
    "    !ssh root@$i chown -R {user}:{user} /home/{user}/.ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxvf spark-3.0.0-bin-hadoop2.7.tgz >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.0/hadoop-2.7.0.tar.gz > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxvf hadoop-2.7.0.tar.gz > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://download.java.net/openjdk/jdk8u40/ri/openjdk-8u40-b25-linux-x64-10_feb_2015.tar.gz > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar zxvf jdk1.8.0_212.tgz > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## same performance using openjdk and jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    !ssh root@$l \"mkdir -p /data/1/{user}; chown {user}:{user} /data/1/{user}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    !ssh root@$l \"mkdir -p /data/1/tmp; chown {user}:{user} /data/1/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    !ssh $l \"mkdir -p /data/1/{user}/hdfs/data; mkdir -p /data/1/{user}/yarn; \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    !ssh $l \"mkdir -p /data/1/{user}/hdfs/name; mkdir -p /data/1/{user}/name; \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !scp hadoop-2.7.0.tar.gz $l:~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !scp jdk1.8.0_212.tgz $l:~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !ssh $l \"tar zxvf hadoop-2.7.0.tar.gz > /dev/null 2>&1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !ssh $l \"tar zxvf jdk1.8.0_212.tgz > /dev/null 2>&1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    !ssh $l \"mkdir spark \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg='''export HADOOP_HOME=~/hadoop-2.7.0\n",
    "export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
    "\n",
    "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
    "export SPARK_LOCAL_DIRS=~/spark\n",
    "\n",
    "export JAVA_HOME=~/jdk1.8.0_212\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "export SPARK_HOME=~/spark-3.0.0-bin-hadoop2.7\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmpcfg\",'w') as f:\n",
    "    f.writelines(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    !scp tmpcfg $l:~/tmpcfg.in\n",
    "    !ssh $l \"cat ~/tmpcfg.in >> ~/.bashrc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enable security.authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coresite='''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "  <property>\n",
    "      <name>fs.default.name</name>\n",
    "      <value>hdfs://{:s}:8020</value>\n",
    "      <final>true</final>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>hadoop.security.authentication</name>\n",
    "      <value>simple</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>hadoop.security.authorization</name>\n",
    "      <value>true</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "'''.format(masterip)\n",
    "\n",
    "with open('hadoop-2.7.0/etc/hadoop/core-site.xml','w') as f:\n",
    "    f.writelines(coresite)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop-2.7.0/etc/hadoop/core-site.xml $l:~/hadoop-2.7.0/etc/hadoop/core-site.xml >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set IP check, note the command <font color=red>\", \"</font>.join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadooppolicy='''<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "\n",
    " Licensed to the Apache Software Foundation (ASF) under one\n",
    " or more contributor license agreements.  See the NOTICE file\n",
    " distributed with this work for additional information\n",
    " regarding copyright ownership.  The ASF licenses this file\n",
    " to you under the Apache License, Version 2.0 (the\n",
    " \"License\"); you may not use this file except in compliance\n",
    " with the License.  You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License.\n",
    "\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>security.service.authorization.default.hosts</name>\n",
    "    <value>{:s}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>security.service.authorization.default.acl</name>\n",
    "    <value>{:s} {:s}</value>\n",
    "  </property>\n",
    "  \n",
    "  \n",
    "    <property>\n",
    "    <name>security.client.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ClientProtocol, which is used by user code\n",
    "    via the DistributedFileSystem.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.client.datanode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ClientDatanodeProtocol, the client-to-datanode protocol\n",
    "    for block recovery.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.datanode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for DatanodeProtocol, which is used by datanodes to\n",
    "    communicate with the namenode.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.inter.datanode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for InterDatanodeProtocol, the inter-datanode protocol\n",
    "    for updating generation timestamp.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.namenode.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for NamenodeProtocol, the protocol used by the secondary\n",
    "    namenode to communicate with the namenode.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    " <property>\n",
    "    <name>security.admin.operations.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for AdminOperationsProtocol. Used for admin commands.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.refresh.user.mappings.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for RefreshUserMappingsProtocol. Used to refresh\n",
    "    users mappings. The ACL is a comma-separated list of user and\n",
    "    group names. The user and group list is separated by a blank. For\n",
    "    e.g. \"alice,bob users,wheel\".  A special value of \"*\" means all\n",
    "    users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.refresh.policy.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for RefreshAuthorizationPolicyProtocol, used by the\n",
    "    dfsadmin and mradmin commands to refresh the security policy in-effect.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.ha.service.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for HAService protocol used by HAAdmin to manage the\n",
    "      active and stand-by states of namenode.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.zkfc.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for access to the ZK Failover Controller\n",
    "    </description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.qjournal.service.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for QJournalProtocol, used by the NN to communicate with\n",
    "    JNs when using the QuorumJournalManager for edit logs.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.mrhs.client.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for HSClientProtocol, used by job clients to\n",
    "    communciate with the MR History Server job status etc.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <!-- YARN Protocols -->\n",
    "\n",
    "  <property>\n",
    "    <name>security.resourcetracker.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ResourceTrackerProtocol, used by the\n",
    "    ResourceManager and NodeManager to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.resourcemanager-administration.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ResourceManagerAdministrationProtocol, for admin commands.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.applicationclient.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ApplicationClientProtocol, used by the ResourceManager\n",
    "    and applications submission clients to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.applicationmaster.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ApplicationMasterProtocol, used by the ResourceManager\n",
    "    and ApplicationMasters to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.containermanagement.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ContainerManagementProtocol protocol, used by the NodeManager\n",
    "    and ApplicationMasters to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.resourcelocalizer.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ResourceLocalizer protocol, used by the NodeManager\n",
    "    and ResourceLocalizer to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.job.task.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for TaskUmbilicalProtocol, used by the map and reduce\n",
    "    tasks to communicate with the parent tasktracker.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.job.client.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for MRClientProtocol, used by job clients to\n",
    "    communciate with the MR ApplicationMaster to query job status etc.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>security.applicationhistory.protocol.acl</name>\n",
    "    <value>*</value>\n",
    "    <description>ACL for ApplicationHistoryProtocol, used by the timeline\n",
    "    server and the generic history service client to communicate with each other.\n",
    "    The ACL is a comma-separated list of user and group names. The user and\n",
    "    group list is separated by a blank. For e.g. \"alice,bob users,wheel\".\n",
    "    A special value of \"*\" means all users are allowed.</description>\n",
    "  </property>\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "</configuration>\n",
    "'''.format((\", \").join(hclients),user,user)\n",
    "\n",
    "with open('hadoop-2.7.0/etc/hadoop/hadoop-policy.xml','w') as f:\n",
    "    f.writelines(hadooppolicy)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop-2.7.0/etc/hadoop/hadoop-policy.xml $l:~/hadoop-2.7.0/etc/hadoop/hadoop-policy.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=4\n",
    "f\"{:s}x{:d}\".format(\"ss\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hdfs config, set replication to 1 to cache all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_site=f'''<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.namenode.secondary.http-address</name>\n",
    "    <value>{masterip}:50090</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/data/1/{user}/hdfs/name</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "        <value>/data/1/{user}/hdfs/data</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>dfs.namenode.checkpoint.dir</name>\n",
    "    <value>/data/1/{user}/hdfs/namesecondary</value>\n",
    "    <final>true</final>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.name.handler.count</name>\n",
    "    <value>100</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.blocksize</name>\n",
    "    <value>128m</value>\n",
    "</property>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "</property>\n",
    "</configuration>\n",
    "'''\n",
    "\n",
    "\n",
    "with open('hadoop-2.7.0/etc/hadoop/hdfs-site.xml','w') as f:\n",
    "    f.writelines(hdfs_site)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop-2.7.0/etc/hadoop/hdfs-site.xml $l:~/hadoop-2.7.0/etc/hadoop/hdfs-site.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapreduce config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce='''<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "\n",
    "     <property>\n",
    "         <name>mapreduce.job.maps</name>\n",
    "         <value>288</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.job.reduces</name>\n",
    "         <value>64</value>\n",
    "     </property>\n",
    "\n",
    "     <property>\n",
    "         <name>mapreduce.map.java.opts</name>\n",
    "         <value>-Xmx5120M -DpreferIPv4Stack=true</value>\n",
    "     </property>\n",
    "      <property>\n",
    "         <name>mapreduce.map.memory.mb</name>\n",
    "         <value>6144</value>\n",
    "         </property>\n",
    "\n",
    "     <property>\n",
    "         <name>mapreduce.reduce.java.opts</name>\n",
    "         <value>-Xmx5120M -DpreferIPv4Stack=true</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.reduce.memory.mb</name>\n",
    "         <value>6144</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>yarn.app.mapreduce.am.staging-dir</name>\n",
    "         <value>/user</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.task.io.sort.mb</name>\n",
    "         <value>2000</value>\n",
    "     </property>\n",
    "     <property>\n",
    "         <name>mapreduce.task.timeout</name>\n",
    "         <value>3600000</value>\n",
    "     </property>\n",
    "<!-- MapReduce Job History Server security configs -->\n",
    "<property>\n",
    "  <name>mapreduce.jobhistory.address</name>\n",
    "  <value>{:s}:10020</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "'''.format(masterip)\n",
    "\n",
    "\n",
    "with open('hadoop-2.7.0/etc/hadoop/mapred-site.xml','w') as f:\n",
    "    f.writelines(mapreduce)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop-2.7.0/etc/hadoop/mapred-site.xml $l:~/hadoop-2.7.0/etc/hadoop/mapred-site.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yarn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarn_site=f'''<?xml version=\"1.0\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "<configuration>\n",
    "  <property>\n",
    "      <name>yarn.resourcemanager.hostname</name>\n",
    "      <value>{masterip}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.resourcemanager.address</name>\n",
    "      <value>{masterip}:8032</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "      <value>190000</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
    "      <value>96</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.pmem-check-enabled</name>\n",
    "      <value>false</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "      <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
    "      <value>4.1</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.aux-services</name>\n",
    "      <value>mapreduce_shuffle,spark_shuffle</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "      <value>1024</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "      <value>190000</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
    "      <value>1</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
    "      <value>96</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.log-aggregation-enable</name>\n",
    "      <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.log.retain-seconds</name>\n",
    "      <value>36000</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.delete.debug-delay-sec</name>\n",
    "      <value>3600</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.log.server.url</name>\n",
    "      <value>http://{masterip}:19888/jobhistory/logs/</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.log-dirs</name>\n",
    "      <value>/home/{user}/hadoop-2.7.0/logs/userlogs</value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.local-dirs</name>\n",
    "      <value>/data/1/{user}/yarn/local\n",
    "      </value>\n",
    "  </property>\n",
    "  <property>\n",
    "      <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>\n",
    "      <value>org.apache.spark.network.yarn.YarnShuffleService</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "'''\n",
    "\n",
    "\n",
    "with open('hadoop-2.7.0/etc/hadoop/yarn-site.xml','w') as f:\n",
    "    f.writelines(yarn_site)\n",
    "    \n",
    "for l in clients:\n",
    "    !scp hadoop-2.7.0/etc/hadoop/yarn-site.xml $l:~/hadoop-2.7.0/etc/hadoop/yarn-site.xml >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slave config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hadoop-2.7.0/etc/hadoop/slaves','w') as f:\n",
    "    f.writelines(\"\\n\".join(clients))\n",
    "\n",
    "for l in clients:\n",
    "    !scp hadoop-2.7.0/etc/hadoop/slaves $l:~/hadoop-2.7.0/etc/hadoop/ >/dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in hclients:\n",
    "    !scp ./spark-3.0.0-bin-hadoop2.7/yarn/spark-3.0.0-yarn-shuffle.jar $l:~/hadoop-2.7.0/share/hadoop/common/lib/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start dfs, yarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop-2.7.0/bin/hadoop namenode -format > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop-2.7.0/bin/hadoop datanode -format > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "    !ssh $l \"ps aux | grep java\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop-2.7.0/sbin/start-dfs.sh >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop-2.7.0/sbin/start-yarn.sh >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop-2.7.0/sbin/stop-all.sh >/dev/null 2>&1\n",
    "!hadoop-2.7.0/sbin/start-all.sh >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir -p /tmp/sparkEventLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client1=clients[0]\n",
    "#!ssh !client1 \"mkdir /data/1/mortgage; cd /data/1/mortgage; wget http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000-2016.tgz\"\n",
    "!ssh $client1 \"cd /data/1/spark/mortgage; tar zxvf mortgage_2000-2016.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!ssh $client1 \"/home/spark/hadoop-2.7.0/bin/hadoop fs -copyFromLocal /data/1/spark/mortgage /\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l in clients:\n",
    "     !ssh root@$l \"echo 3 > /proc/sys/vm/drop_caches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!cat /etc/hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
