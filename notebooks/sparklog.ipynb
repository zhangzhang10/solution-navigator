{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import nested_scopes\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML('<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import builtins\n",
    "from itertools import chain\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, DateType,\n",
    "    TimestampType, StringType, LongType, IntegerType, DoubleType,FloatType)\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "import re\n",
    "import collections\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas\n",
    "import numpy\n",
    "import time\n",
    "from pandasql import sqldf\n",
    "\n",
    "pandas.options.display.max_rows=50\n",
    "pandas.options.display.max_columns=200\n",
    "pandas.options.display.float_format = '{:,}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress,Layout\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhdfs\n",
    "\n",
    "\n",
    "fs = pyhdfs.HdfsClient(hosts='10.0.2.125:50070', user_name='yuzhou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# monitor backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyhdfs\n",
    "fs = pyhdfs.HdfsClient(hosts='sr525:50070', user_name='yuzhou')\n",
    "\n",
    "\n",
    "basedif=\"dgx-2\"\n",
    "\n",
    "emon_events='''\n",
    "    -q -c -experimental -t0.5 -l100000 -u\n",
    "    -C (\n",
    "\n",
    "    INST_RETIRED.ANY\n",
    "    CPU_CLK_UNHALTED.REF_TSC\n",
    "    CPU_CLK_UNHALTED.THREAD\n",
    "    CYCLE_ACTIVITY.STALLS_L3_MISS\n",
    "    CYCLE_ACTIVITY.CYCLES_MEM_ANY\n",
    "    CYCLE_ACTIVITY.STALLS_TOTAL\n",
    "    DTLB_LOAD_MISSES.WALK_ACTIVE\n",
    "    UNC_CHA_TOR_OCCUPANCY.IA_MISS:filter1=0x40432\n",
    "    UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40432\n",
    "    UNC_CHA_CLOCKTICKS\n",
    "    UNC_M_CAS_COUNT.RD\n",
    "    UNC_M_CAS_COUNT.WR\n",
    "    UNC_IIO_DATA_REQ_OF_CPU.MEM_WRITE.PART0\n",
    "    UNC_IIO_DATA_REQ_OF_CPU.MEM_READ.PART0\n",
    "    UNC_IIO_DATA_REQ_BY_CPU.MEM_WRITE.PART0\n",
    "    UNC_IIO_DATA_REQ_BY_CPU.MEM_READ.PART0\n",
    "\n",
    "\n",
    "    INST_RETIRED.ANY\n",
    "    CPU_CLK_UNHALTED.REF_TSC\n",
    "    CPU_CLK_UNHALTED.THREAD\n",
    "    OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3fB80007f7\n",
    "    OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3f840007f7\n",
    "    MEM_LOAD_RETIRED.L3_MISS\n",
    "    UOPS_ISSUED.STALL_CYCLES\n",
    "    UNC_CHA_TOR_OCCUPANCY.IA_MISS:filter1=0x40431\n",
    "    UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40431\n",
    "    UNC_CHA_CLOCKTICKS\n",
    "    )\n",
    "'''\n",
    "\n",
    "def killsar(clients):\n",
    "    for l in clients:\n",
    "        out=!ssh $l \"ps aux | grep -w sar | grep -v grep | tr -s ' ' | cut -d' ' -f2\"\n",
    "        for x in out:\n",
    "            !ssh $l \"kill $x > /dev/null 2>&1\"\n",
    "    for l in clients:\n",
    "        out=!ssh $l \"ps aux | grep -w pidstat | grep -v grep | tr -s ' ' | cut -d' ' -f2\"\n",
    "        for x in out:\n",
    "            !ssh $l \"kill $x > /dev/null 2>&1\"\n",
    "    for l in clients:\n",
    "        out=!ssh $l \"ps aux | grep -w perf | grep -v grep | tr -s ' ' | cut -d' ' -f2\"\n",
    "        for x in out:\n",
    "            !ssh root@$l \"kill $x > /dev/null 2>&1\"\n",
    "    for l in clients:\n",
    "        out=!ssh $l \"ps aux | grep -w nvidia-smi | grep -v grep | tr -s ' ' | cut -d' ' -f2\"\n",
    "        for x in out:\n",
    "            !ssh root@$l \"kill $x > /dev/null 2>&1\"\n",
    "    for l in clients:\n",
    "        !ssh $l \"source ~/sep_install/sep_vars.sh>/dev/null 2>&1; emon -stop > /dev/null 2>&1\"\n",
    "            \n",
    "def startmonitor(clients,appid,**kwargs):\n",
    "    local_profile_dir=home+\"/profile/\"\n",
    "    prof=local_profile_dir+appid+\"/\"\n",
    "    !mkdir -p $prof\n",
    "    !echo \"\" > /tmp/training_time\n",
    "    \n",
    "    for l in clients:\n",
    "        !ssh root@$l date\n",
    "    \n",
    "    killsar(clients)\n",
    "    \n",
    "    if kwargs.get(\"collect_emon\",False):\n",
    "        with open(home+\"/emon.list\",'w+') as f:\n",
    "            f.write(emon_events)\n",
    "        for l in clients:\n",
    "            !scp {home}/emon.list {l}:{home}/emon.list  > /dev/null 2>&1\n",
    "    \n",
    "    perfsyscalls=kwargs.get(\"collect_perf_syscall\",None)\n",
    "    \n",
    "    for l in clients:\n",
    "        !mkdir -p $prof/$l/\n",
    "        !ssh $l mkdir -p $prof/$l/\n",
    "        !ssh $l \"sar -o {prof}/{l}/sar.bin -r -u -d -n DEV 1 >/dev/null 2>&1 &\"\n",
    "        if kwargs.get(\"collect_pid\",False):\n",
    "            !ssh $l \"{java_home}/bin/jps | grep CoarseGrainedExecutorBackend | head -n 1 | cut -d' ' -f 1 | xargs  -I % pidstat -h -t -p % 1  > {prof}/{l}/pidstat.out  2>/dev/null &\"\n",
    "        !ssh root@{l} 'cat /proc/uptime  | cut -d\" \" -f 1 | xargs -I ^ date -d \"- ^ seconds\"  +%s.%N' > $prof/$l/uptime.txt\n",
    "        if kwargs.get(\"collect_sched\",False):\n",
    "            !ssh root@{l} 'perf trace -e \"sched:sched_switch\" -C 8-15 -o {prof}/{l}/sched.txt -T -- sleep 10000 >/dev/null 2>/dev/null &'\n",
    "        if kwargs.get(\"collect_emon\",False):\n",
    "            !ssh {l} \"source ~/sep_install/sep_vars.sh>/dev/null 2>&1; emon -i {home}/emon.list -f {prof}/{l}/emon.rst >/dev/null 2>&1 & \"\n",
    "        if perfsyscalls is not None:\n",
    "            !ssh root@{l} \"perf stat -e 'syscalls:sys_exit_poll,syscalls:sys_exit_epoll_wait' -a -I 1000 -o {prof}/{l}/perfstat.txt  >/dev/null 2>&1 & \"\n",
    "        if kwargs.get(\"collect_nv_smi\",False):\n",
    "            !ssh {l} \"nvidia-smi --query-gpu=timestamp,index,utilization.gpu,utilization.memory,memory.used, -lms 500  --format=csv -f  {prof}/{l}/gpu.txt >/dev/null 2>&1 & \"\n",
    "    return prof\n",
    "\n",
    "def stopmonitor(clients, sc, appid,**kwargs):\n",
    "    %cd ~\n",
    "    \n",
    "    local_profile_dir=home+\"/profile/\"\n",
    "    prof=local_profile_dir+appid+\"/\"\n",
    "    !mkdir -p $prof\n",
    "    \n",
    "    killsar(clients)\n",
    "    \n",
    "    with open(f\"{prof}/starttime\",\"w\"):\n",
    "        f.write(time()*1000)\n",
    "    \n",
    "    for l in clients:\n",
    "        !ssh $l \"sar -f {prof}/{l}/sar.bin -r > {prof}/{l}/sar_mem.sar;sar -f {prof}/{l}/sar.bin -u > {prof}/{l}/sar_cpu.sar;sar -f {prof}/{l}/sar.bin -d > {prof}/{l}/sar_disk.sar;sar -f {prof}/{l}/sar.bin -n DEV > {prof}/{l}/sar_nic.sar;\" \n",
    "        !ssh $l \"grep -rI xgbtck --no-filename {spark_home}/work/{appid}/* | sed 's/^ //g'  > {prof}/{l}/xgbtck.txt\"\n",
    "        if l!= socket.gethostname():\n",
    "            !scp -r $l:$prof/$l $prof/ > /dev/null 2>&1\n",
    "        !ssh $l \"{java_home}/bin/jps | grep CoarseGrainedExecutorBackend | head -n 2 | tail -n 1 | cut -d' ' -f 1  | xargs -I % ps -To tid p %\" > $prof/$l/sched_threads.txt\n",
    "        !ssh $l \"source ~/sep_install/sep_vars.sh>/dev/null 2>&1; emon -v \" > $prof/$l/emonv.txt\n",
    "        !ssh $l \"sar -V \" > $prof/$l/sarv.txt\n",
    "        !test -f $prof/$l/perfstat.txt && head -n 1 $prof/$l/perfstat.txt > $prof/$l/perfstarttime\n",
    "    if sc is not None:\n",
    "        sc.stop()\n",
    "    \n",
    "    xgbfiles=!find {home}/profile/{appid} | grep xgbtck\n",
    "    !cat /tmp/training_time >> {xgbfiles[0]}\n",
    "    \n",
    "#    !hadoop fs -copyToLocal /tmp/sparkEventLog/$appid $prof/app.log\n",
    "#    !cp {home}/spark_events/{appid}  $prof/app.log\n",
    "    fs.mkdirs(f\"/{basedif}/\")\n",
    "    v=[os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(local_profile_dir+appid)) for f in fn]\n",
    "    for f in v:\n",
    "        paths=os.path.split(f)\n",
    "        fs.mkdirs(\"/\"+basedif+\"/\"+paths[0][len(local_profile_dir):])\n",
    "        fs.copy_from_local(f,\"/\"+basedif+\"/\"+paths[0][len(local_profile_dir):]+\"/\"+paths[1],overwrite=True)\n",
    "#    !HADOOP_USER_NAME=yuzhou hadoop fs -put  file://{prof} hdfs://10.1.2.125:8020/{basedif}\n",
    "    print('http://10.0.0.34:18080/history/{:s}/jobs'.format(appid))\n",
    "    print('http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{:s}.json'.format(appid))\n",
    "    print(appid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pinexecutor(clients):\n",
    "    for client in clients:\n",
    "        pids=!ssh $client \"jps | grep CoarseGrainedExecutorBackend | cut -d' ' -f1\"\n",
    "        print(client,\":\",len(pids),\" \",\"\\t\".join(map(str,pids)))\n",
    "        t1=[l for l in range(0,cores_per_executor)]\n",
    "        t2=[executors_per_node*2+l for l in range(0,cores_per_executor)]\n",
    "        node1=True\n",
    "        for l in pids:\n",
    "            t=t1 if node1==True else t2\n",
    "            cpus=','.join(map(str, t))\n",
    "            for i in range(0,cores_per_executor):\n",
    "                t[i]=t[i]+cores_per_executor\n",
    "            node1= not node1\n",
    "            !ssh $client \"taskset -a -p -c $cpus $l > /dev/null 2>&1 \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SparkLog_Analysis:\n",
    "    def __init__(self, appid,jobids,clients):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self,file):\n",
    "        self.file=file\n",
    "        self.starttime=0\n",
    "        self.df=None\n",
    "    \n",
    "    def load_data(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        if self.df==None:\n",
    "            self.load_data()\n",
    "        trace_events=[]\n",
    "        node=kwargs.get('node',\"node\")\n",
    "        trace_events.append(json.dumps({\"name\": \"process_name\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"name\":\" \"+node}}))\n",
    "        return trace_events\n",
    "\n",
    "    \n",
    "    def generate_trace_view(self, trace_output, **kwargs):\n",
    "        traces=[]\n",
    "        traces.extend(self.generate_trace_view_list(0,**kwargs))\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ],\n",
    "            \"displayTimeUnit\": \"ns\"\n",
    "        }'''\n",
    "\n",
    "        with open('/home/yuzhou/trace_result/'+trace_output+'.json', 'w') as outfile: \n",
    "            outfile.write(output)\n",
    "\n",
    "        print(\"http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+trace_output+\".json\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# EMON process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## emon metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metrics='''<root>\n",
    "\n",
    "    <metric name=\"metric_CPI\">\n",
    "        <throughput-metric-name>metric_cycles per txn</throughput-metric-name>\n",
    "        <event alias=\"a\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "        <event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "        <formula>a/b</formula>\n",
    "    </metric>\n",
    "\n",
    " \t<metric name=\"metric_branch mispredict ratio\">\n",
    "\t\t<event alias=\"a\">BR_MISP_RETIRED.ALL_BRANCHES</event>\n",
    "\t\t<event alias=\"b\">BR_INST_RETIRED.ALL_BRANCHES</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_loads per instr\">\n",
    "        <throughput-metric-name>metric_loads per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">MEM_INST_RETIRED.ALL_LOADS</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_stores per instr\">\n",
    "        <throughput-metric-name>metric_stores per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">MEM_INST_RETIRED.ALL_STORES</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_locks retired per instr\">\n",
    "        <throughput-metric-name>metric_locks retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">MEM_INST_RETIRED.LOCK_LOADS</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_uncacheable reads per instr\">\n",
    "        <throughput-metric-name>metric_uncacheable reads per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40e33</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/c</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_streaming stores (full line) per instr\">\n",
    "        <throughput-metric-name>metric_streaming stores (full line) per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x41833</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_streaming stores (partial line) per instr\">\n",
    "        <throughput-metric-name>metric_streaming stores (partial line) per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x41a33</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_L1D MPI (includes data+rfo w/ prefetches)\">\n",
    "        <throughput-metric-name>metric_L1D misses per txn (includes data+rfo w/ prefetches)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">L1D.REPLACEMENT</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_L1D demand data read hits per instr\">\n",
    "        <throughput-metric-name>metric_L1D demand data read hits per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">MEM_LOAD_RETIRED.L1_HIT</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_L1-I code read misses (w/ prefetches) per instr\">\n",
    "        <throughput-metric-name>metric_L1I code read misses (includes prefetches) per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">L2_RQSTS.ALL_CODE_RD</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_L2 demand data read hits per instr\">\n",
    "        <throughput-metric-name>metric_L2 demand data read hits per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">MEM_LOAD_RETIRED.L2_HIT</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_L2 MPI (includes code+data+rfo w/ prefetches)\">\n",
    "        <throughput-metric-name>metric_L2 misses per txn (includes code+data+rfo w/ prefetches)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">L2_LINES_IN.ALL</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\n",
    " \t<metric name=\"metric_L2 demand data read MPI\">\n",
    "        <throughput-metric-name>metric_L2 demand data read misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">MEM_LOAD_RETIRED.L2_MISS</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_L2 demand code MPI\">\n",
    "        <throughput-metric-name>metric_L2 demand code misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">L2_RQSTS.CODE_RD_MISS</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_L2 Any local request that HITM in a sibling core (per instr)\">\n",
    "        <throughput-metric-name>metric_L2 Any local request that HITM in a sibling core per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_HIT.HITM_OTHER_CORE</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/c</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_L2 Any local request that HIT in a sibling core and forwarded(per instr)\">\n",
    "        <throughput-metric-name>metric_L2 Any local request that HIT in a sibling core and forwarded per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_HIT.HIT_OTHER_CORE_FWD</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/c</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_L2 all L2 prefetches(per instr)\">\n",
    "        <throughput-metric-name>metric_L2 all L2 prefetches per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">L2_RQSTS.ALL_PF</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/c</formula>\n",
    "\t</metric>\t\n",
    "\t\n",
    "\t<metric name=\"metric_L2 % of L2 evictions that are allocated into L3\">\n",
    "\t\t<event alias=\"a\">L2_LINES_OUT.NON_SILENT</event>\n",
    "\t\t<event alias=\"b\">IDI_MISC.WB_DOWNGRADE</event>\n",
    "\t\t<formula>100*(a-b)/a</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_L2 % of L2 evictions that are NOT allocated into L3\">\n",
    "\t\t<event alias=\"a\">L2_LINES_OUT.NON_SILENT</event>\n",
    "\t\t<event alias=\"b\">IDI_MISC.WB_DOWNGRADE</event>\n",
    "\t\t<formula>100*b/a</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC code references per instr (L3 prefetch excluded)\">\n",
    "        <throughput-metric-name>metric_LLC code references per txn (L3 prefetch excluded)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA:filter1=0x40233</event>\n",
    "\t\t<event alias=\"d\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/d</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC data read references per instr (L3 prefetch excluded)\">\n",
    "        <throughput-metric-name>metric_LLC data read references per txn (L3 prefetch excluded)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA:filter1=0x40433</event>\n",
    "\t\t<event alias=\"d\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/d</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC RFO references per instr (L3 prefetch excluded)\">\n",
    "        <throughput-metric-name>metric_LLC RFO references per txn (L3 prefetch excluded)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA:filter1=0x40033</event>\n",
    "\t\t<event alias=\"d\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/d</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_LLC MPI (includes code+data+rfo w/ prefetches)\">\n",
    "        <throughput-metric-name>metric_LLC misses per txn (includes code+data+rfo w/ prefetches)</throughput-metric-name>\n",
    "\t\t<!-- L3 misses of L2 requests and L3 prefetches for data read -->\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12D40433</event>\n",
    "\t\t<!-- L3 misses of L2 requests + L3 prefetches for code read -->\n",
    "\t\t<event alias=\"b\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12CC0233</event>\n",
    "\t\t<!-- L3 misses of L2 requests + L3 prefetches for RFO -->\n",
    "\t\t<event alias=\"c\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033</event>\n",
    "\t\t<event alias=\"d\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>(a+b+c)/d</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC data read MPI (demand+prefetch)\">\n",
    "        <throughput-metric-name>metric_LLC data read (demand+prefetch) misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12D40433</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC RFO read MPI (demand+prefetch)\">\n",
    "        <throughput-metric-name>metric_LLC RFO read (demand+prefetch) misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC code read MPI (demand+prefetch)\">\n",
    "        <throughput-metric-name>metric_LLC code read (demand+prefetch) misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12CC0233</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC all LLC prefetches (per instr)\">\n",
    "        <throughput-metric-name>metric_LLC LLC prefetches per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C4B433</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_LLC total HITM (per instr) (excludes LLC prefetches)\">\n",
    "        <throughput-metric-name>metric_LLC total HITM per txn (excludes LLC prefetches)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HITM</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_LLC total HIT clean line forwards (per instr) (excludes LLC prefetches)\">\n",
    "        <throughput-metric-name>metric_LLC total HIT clean line forwards per txn (excludes LLC prefetches)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HIT_FORWARD</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_LLC % of LLC misses satisfied by remote caches\">\n",
    "\t\t<!-- L3 misses of L2 requests and L3 prefetches for data read -->\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12D40433</event>\n",
    "\t\t<!-- L3 misses of L2 requests + L3 prefetches for code read -->\n",
    "\t\t<event alias=\"b\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12CC0233</event>\n",
    "\t\t<!-- L3 misses of L2 requests + L3 prefetches for RFO -->\n",
    "\t\t<event alias=\"c\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033</event>\n",
    "\t\t<!-- L3 misses of L3 prefetches only -->\n",
    "\t\t<event alias=\"d\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C4B433</event>\n",
    "\t\t<event alias=\"e\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HIT_FORWARD</event>\n",
    "\t\t<event alias=\"f\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HITM</event>\n",
    "\t\t<formula>100*(e+f)/(a+b+c-d)</formula>\n",
    "\t</metric>\n",
    "\n",
    "\n",
    "\t<metric name=\"metric_SF snoop filter capacity evictions (per instr)\">\n",
    "        <throughput-metric-name>metric_SF snoop filter capacity evictions per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_SF_EVICTION.M_STATE</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_SF_EVICTION.S_STATE</event>\n",
    "\t\t<event alias=\"c\">UNC_CHA_SF_EVICTION.E_STATE</event>\n",
    "\t\t<event alias=\"d\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>(a+b+c)/d</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_SF % of L3 accesses that result in SF capacity evictions\">\n",
    "        <throughput-metric-name>metric_SF snoop filter capacity evictions per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_SF_EVICTION.M_STATE</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_SF_EVICTION.S_STATE</event>\n",
    "\t\t<event alias=\"c\">UNC_CHA_SF_EVICTION.E_STATE</event>\n",
    "\t\t<event alias=\"d\">L2_LINES_IN.ALL</event>\n",
    "\t\t<formula>100*(a+b+c)/d</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_ITLB MPI\">\n",
    "        <throughput-metric-name>metric_ITLB misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">ITLB_MISSES.WALK_COMPLETED</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_ITLB large page MPI\">\n",
    "        <throughput-metric-name>metric_ITLB large page misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">ITLB_MISSES.WALK_COMPLETED_2M_4M</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_DTLB load MPI\">\n",
    "        <throughput-metric-name>metric_DTLB load misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">DTLB_LOAD_MISSES.WALK_COMPLETED</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_DTLB 4KB page load MPI\">\n",
    "        <throughput-metric-name>metric_DTLB 4KB page load misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">DTLB_LOAD_MISSES.WALK_COMPLETED</event>\n",
    "\t\t<event alias=\"b\">DTLB_LOAD_MISSES.WALK_COMPLETED_2M_4M</event>\n",
    "\t\t<event alias=\"c\">DTLB_LOAD_MISSES.WALK_COMPLETED_1G</event>\n",
    "\t\t<event alias=\"d\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>(a-b-c)/d</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_DTLB 2MB large page load MPI\">\n",
    "        <throughput-metric-name>metric_DTLB 2MB large page load misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">DTLB_LOAD_MISSES.WALK_COMPLETED_2M_4M</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_DTLB 1GB large page load MPI\">\n",
    "        <throughput-metric-name>metric_DTLB 1GB large page load misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">DTLB_LOAD_MISSES.WALK_COMPLETED_1G</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_DTLB store MPI\">\n",
    "        <throughput-metric-name>metric_DTLB store misses per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">DTLB_STORE_MISSES.WALK_COMPLETED</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_DTLB load miss latency (in core clks)\">\n",
    "\t\t<event alias=\"a\">DTLB_LOAD_MISSES.WALK_ACTIVE</event>\n",
    "\t\t<event alias=\"b\">DTLB_LOAD_MISSES.WALK_COMPLETED</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_DTLB store miss latency (in core clks)\">\n",
    "\t\t<event alias=\"a\">DTLB_STORE_MISSES.WALK_ACTIVE</event>\n",
    "\t\t<event alias=\"b\">DTLB_STORE_MISSES.WALK_COMPLETED</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_ITLB miss latency (in core clks)\">\n",
    "\t\t<event alias=\"a\">ITLB_MISSES.WALK_ACTIVE</event>\n",
    "\t\t<event alias=\"b\">ITLB_MISSES.WALK_COMPLETED</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_NUMA %_Reads addressed to local DRAM\">\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40432</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40431</event>\n",
    "\t\t<formula>100*a/(a+b)</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_NUMA %_Reads addressed to remote DRAM\">\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40432</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40431</event>\n",
    "\t\t<formula>100*b/(a+b)</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_NUMA %_RFOs addressed to local DRAM\">\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40031</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033</event>\n",
    "\t\t<formula>100*(b-a)/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_NUMA %_RFOs addressed to remote DRAM\">\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40031</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033</event>\n",
    "\t\t<formula>100*a/b</formula>\n",
    "\t</metric>\n",
    "    \n",
    "\t<metric name=\"metric_uncore frequency GHz\">\n",
    "        <event alias=\"a\">UNC_CHA_CLOCKTICKS</event>\n",
    "        <constant alias=\"b\">system.sockets[0].cores.count</constant>\n",
    "        <constant alias=\"socket_count\">2</constant>\n",
    "        <formula>a/(b*socket_count)/1000000000</formula>\n",
    "        <formula socket=\"0\">a[0]/b/1000000000</formula>\n",
    "        <formula socket=\"1\">a[1]/b/1000000000</formula>\n",
    "    </metric>\n",
    "\n",
    "     <metric name=\"metric_UPI speed - GT/s\">\n",
    "        <constant alias=\"socket_count\">2</constant>\n",
    "        <constant alias=\"links_per_socket\">2</constant>\n",
    "        <event alias=\"a\">UNC_UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"tsc\">TSC</event>\n",
    "        <event alias=\"f\">UNC_UPI_L1_POWER_CYCLES</event>\n",
    "        <event alias=\"c6\">MSR_EVENT:msr=0x3F9:type=FREERUN:scope=PACKAGE</event>\n",
    "       <constant alias=\"d\">system.sockets[0].cores.count</constant>\n",
    "  \t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "        <formula>(((tsc/(d*threads)) / ((tsc/(d*threads)) - c6))*(a-f))  /(socket_count*links_per_socket)*8/1000000000</formula>\n",
    "        <formula socket=\"0\">((tsc/(d*threads)) / ((tsc/(d*threads)) - c6[0])*(a[0]-f[0]))/links_per_socket*8/1000000000</formula>\n",
    "        <formula socket=\"1\">((tsc/(d*threads)) / ((tsc/(d*threads)) - c6[1])*(a[1]-f[1]))/links_per_socket*8/1000000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_UPI Data transmit BW (MB/sec) (only data)\">\n",
    "        <event alias=\"a\">UNC_UPI_TxL_FLITS.ALL_DATA</event>\n",
    "       <!-- 9 flits are needed to transmit a full cache line -->\n",
    "        <formula>a*(64/9)/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_UPI Transmit utilization_% (includes control)\">\n",
    "       <constant alias=\"socket_count\">2</constant>\n",
    "        <constant alias=\"links_per_socket\">2</constant>\n",
    "        <event alias=\"a\">UNC_UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"tsc\">TSC</event>\n",
    "        <event alias=\"f\">UNC_UPI_L1_POWER_CYCLES</event>\n",
    "        <event alias=\"c6\">MSR_EVENT:msr=0x3F9:type=FREERUN:scope=PACKAGE</event>\n",
    "        <event alias=\"g\">UNC_UPI_TxL_FLITS.ALL_DATA</event>\n",
    "        <event alias=\"h\">UNC_UPI_TxL_FLITS.NON_DATA</event>\n",
    "       <constant alias=\"d\">system.sockets[0].cores.count</constant>\n",
    "  \t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "        <!-- 5 flits are sent in every 6 clocks; NULL flits are sent across all 3 slots and hence b/3 -->\n",
    "\t\t<!-- So useful flits (non-idle) == (clocks*5/6) - (null_flits/3) -->\t\n",
    "        <formula>100*((g+h)/3)/(((((tsc/(d*threads)) / ((tsc/(d*threads)) - c6))*(a-f))*5/6))</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_UPI % cycles transmit link is half-width (L0p)\">\n",
    "        <event alias=\"a\">UNC_UPI_TxL0P_POWER_CYCLES</event>\n",
    "        <event alias=\"b\">UNC_UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"f\">UNC_UPI_L1_POWER_CYCLES</event>\n",
    "        <formula>100*(a/(b-f))</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_UPI % cycles receive link is half-width (L0p)\">\n",
    "        <event alias=\"a\">UNC_UPI_RxL0P_POWER_CYCLES</event>\n",
    "        <event alias=\"b\">UNC_UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"f\">UNC_UPI_L1_POWER_CYCLES</event>\n",
    "        <formula>100*(a/(b-f))</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_HA - Reads vs. all requests\">\n",
    "      <event alias=\"a\">UNC_CHA_REQUESTS.READS</event>\n",
    "      <event alias=\"b\">UNC_CHA_REQUESTS.WRITES</event>\n",
    "      <formula>a/(a+b)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_HA - Writes vs. all requests\">\n",
    "      <event alias=\"a\">UNC_CHA_REQUESTS.READS</event>\n",
    "      <event alias=\"b\">UNC_CHA_REQUESTS.WRITES</event>\n",
    "      <formula>b/(a+b)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_HA % of all reads that are local\">\n",
    "      <event alias=\"a\">UNC_CHA_REQUESTS.READS_LOCAL</event>\n",
    "      <event alias=\"b\">UNC_CHA_REQUESTS.READS</event>\n",
    "      <formula>100*a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_HA % of all writes that are local\">\n",
    "      <event alias=\"a\">UNC_CHA_REQUESTS.WRITES_LOCAL</event>\n",
    "      <event alias=\"b\">UNC_CHA_REQUESTS.WRITES</event>\n",
    "      <formula>100*a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_HA conflict responses per instr\">\n",
    "        <throughput-metric-name>metric_HA conflict responses per txn</throughput-metric-name>\n",
    "      <event alias=\"a\">UNC_CHA_SNOOP_RESP.RSPCNFLCTS</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_HA directory lookups that spawned a snoop (per instr)\">\n",
    "        <throughput-metric-name>metric_HA directory lookups that spawned a snoop (per txn)</throughput-metric-name>\n",
    "      <event alias=\"a\">UNC_CHA_DIR_LOOKUP.SNP</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_HA directory lookups that did not spawn a snoop (per instr)\">\n",
    "        <throughput-metric-name>metric_HA directory lookups that did not spawn a snoop (per txn)</throughput-metric-name>\n",
    "      <event alias=\"a\">UNC_CHA_DIR_LOOKUP.NO_SNP</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_M2M directory updates (per instr)\">\n",
    "        <throughput-metric-name>metric_M2M directory updates (per txn)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_DIR_UPDATE.HA</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_DIR_UPDATE.TOR</event>\n",
    "\t\t<event alias=\"c\">UNC_M2M_DIRECTORY_UPDATE.ANY</event>\n",
    "\t\t<event alias=\"d\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>(a+b+c)/d</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_M2M XPT prefetches (per instr)\">\n",
    "        <throughput-metric-name>metric_M2M XPT prefetches (per txn)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_M2M_PREFCAM_INSERTS</event>\n",
    "\t\t<event alias=\"b\">UNC_M3UPI_UPI_PREFETCH_SPAWN</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>(a-b)/c</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_M3UPI UPI prefetches (per instr)\">\n",
    "        <throughput-metric-name>metric_M3UPI UPI prefetches (per txn)</throughput-metric-name>\n",
    "\t\t<event alias=\"b\">UNC_M3UPI_UPI_PREFETCH_SPAWN</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>b/c</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_M2M extra reads from XPT-UPI prefetches (per instr)\">\n",
    "        <throughput-metric-name>metric_M2M useless XPT-UPI prefetches (per txn)</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_M2M_PREFCAM_INSERTS</event>\n",
    "\t\t<event alias=\"b\">UNC_M2M_PREFCAM_DEMAND_PROMOTIONS</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>(a-b)/c</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_DDR data rate (MT/sec)\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <constant alias=\"socket_count\">2</constant>\n",
    "        <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <formula>2*a/(socket_count*channels_populated_per_socket)/1000000</formula>\n",
    "        <formula socket=\"0\">2*a[0]/channels_populated_per_socket/1000000</formula>\n",
    "\t\t<formula socket=\"1\">2*a[1]/channels_populated_per_socket/1000000</formula>\n",
    "     </metric>\n",
    "\n",
    "    <metric name=\"metric_memory bandwidth read (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_M_CAS_COUNT.RD</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory bandwidth write (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_M_CAS_COUNT.WR</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory bandwidth total (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_M_CAS_COUNT.RD</event>\n",
    "        <event alias=\"b\">UNC_M_CAS_COUNT.WR</event>\n",
    "        <formula>(a+b)*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory extra read b/w due to XPT prefetches (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_M2M_PREFCAM_INSERTS</event>\n",
    "        <event alias=\"b\">UNC_M2M_PREFCAM_DEMAND_PROMOTIONS</event>\n",
    "        <formula>(a-b)*64/1000000</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_memory extra write b/w due to directory updates (MB/sec)\">\n",
    "\t\t<event alias=\"a\">UNC_CHA_DIR_UPDATE.HA</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_DIR_UPDATE.TOR</event>\n",
    "\t\t<event alias=\"c\">UNC_M2M_DIRECTORY_UPDATE.ANY</event>\n",
    "        <formula>(a+b+c)*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_2LM % of non-inclusive writes to near memory\">\n",
    "\t\t<event alias=\"a\">UNC_M2M_IMC_WRITES.NI</event>\n",
    "\t\t<event alias=\"b\">UNC_M_DDRT_RDQ_INSERTS</event>\n",
    "\t\t<event alias=\"c\">UNC_M_CAS_COUNT.WR</event>\n",
    "        <formula>100*a/(c-b)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_2LM near memory cache read miss rate%\">\n",
    "\t\t<event alias=\"a\">UNC_M2M_TAG_HIT.NM_RD_HIT_CLEAN</event>\n",
    "\t\t<event alias=\"b\">UNC_M2M_TAG_HIT.NM_RD_HIT_DIRTY</event>\n",
    "\t\t<event alias=\"c\">UNC_M_DDRT_RDQ_INSERTS</event>\n",
    "        <formula>100*c/(a+b+c)</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_memory RPQ read latency (ns)\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_RPQ_INSERTS</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY</event>\n",
    "        <constant alias=\"socket_count\">2</constant>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "       <formula>((c/b)/(a/(socket_count*channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"0\">((c[0]/b[0])/(a[0]/(channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"1\">((c[1]/b[1])/(a[1]/(channels_populated_per_socket)))*1000000000</formula> \n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory WPQ write latency (ns)\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_WPQ_INSERTS</event>\n",
    "        <event alias=\"c\">UNC_M_WPQ_OCCUPANCY</event>\n",
    "         <constant alias=\"socket_count\">2</constant>\n",
    "        <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "       <formula>((c/b)/(a/(socket_count*channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"0\">((c[0]/b[0])/(a[0]/(channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"1\">((c[1]/b[1])/(a[1]/(channels_populated_per_socket)))*1000000000</formula> \n",
    "    </metric>\n",
    "\n",
    "\n",
    "    <metric name=\"metric_memory avg entries in RPQ\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY</event>\n",
    "       <formula>c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory avg entries in RPQ when not empty\">\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY</event>\n",
    "        <event alias=\"a\">UNC_M_RPQ_OCCUPANCY:t=1</event>\n",
    "       <formula>c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory % cycles when RPQ is empty\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=1</event>\n",
    "       <formula>100*(1-c/a)</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory % cycles when RPQ has 1 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=1</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory % cycles when RPQ has 10 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=10</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory % cycles when RPQ has 20 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=20</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory % cycles when RPQ has 40 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=40</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory avg time (dclk) RPQ not empty\">\n",
    "        <event alias=\"a\">UNC_M_RPQ_OCCUPANCY:t=1:e1</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=1</event>\n",
    "       <formula>c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory avg time (dclk) RPQ empty\">\n",
    "        <event alias=\"a\">UNC_M_RPQ_OCCUPANCY:t=1:e1</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=1</event>\n",
    "        <event alias=\"d\">UNC_M_CLOCKTICKS</event>\n",
    "       <formula>(d-c)/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory avg time with 40 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_RPQ_OCCUPANCY:t=40:e1</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=40</event>\n",
    "       <formula>c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_memory avg time with less than 40 entries)\">\n",
    "        <event alias=\"a\">UNC_M_RPQ_OCCUPANCY:t=40:e1</event>\n",
    "        <event alias=\"c\">UNC_M_RPQ_OCCUPANCY:t=40</event>\n",
    "        <event alias=\"d\">UNC_M_CLOCKTICKS</event>\n",
    "       <formula>(d-c)/a</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_3DXP_memory bandwidth read (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_RDQ_INSERTS</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_3DXP_memory bandwidth write (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_WPQ_INSERTS</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_3DXP_memory bandwidth total (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_RDQ_INSERTS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_INSERTS</event>\n",
    "        <formula>(a+b)*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_3DXP memory RPQ read latency (ns)\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_INSERTS</event>\n",
    "        <event alias=\"c\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL</event>\n",
    "        <constant alias=\"socket_count\">2</constant>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "       <formula>((c/b)/(a/(socket_count*channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"0\">((c[0]/b[0])/(a[0]/(channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"1\">((c[1]/b[1])/(a[1]/(channels_populated_per_socket)))*1000000000</formula> \n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_3DXP avg entries in RPQ\">\n",
    "        <event alias=\"b\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL</event>\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>c/(b*aep_dimms_per_socket/channels_populated_per_socket)</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_3DXP avg entries in RPQ when not empty\">\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1</event>\n",
    "        <event alias=\"c\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL</event>\n",
    "       <formula>c/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_3DXP % cycles when RPQ is empty\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1</event>\n",
    "      <formula>100*(1-(b/a))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP % cycles when RPQ has 1 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1</event>\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP % cycles when RPQ has 10 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=10</event>\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP % cycles when RPQ has 24 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=24</event>\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP % cycles when RPQ has 36 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=36</event>\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP avg time (dclk) RPQ not empty\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1</event>\n",
    "       <formula>b/a</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP avg time (dclk) RPQ empty\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1</event>\n",
    "        <event alias=\"c\">UNC_M_CLOCKTICKS</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constants below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>((c*aep_dimms_per_socket/channels_populated_per_socket)-b)/a</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP avg time (dclk) with 36 or more entries in RPQ\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=36:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=36</event>\n",
    "       <formula>b/a</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP avg time (dclk) with less than 36 entries in RPQ\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=36:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=36</event>\n",
    "        <event alias=\"c\">UNC_M_CLOCKTICKS</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>((c*aep_dimms_per_socket/channels_populated_per_socket)-b)/a</formula>\n",
    "    </metric>\n",
    "\t\n",
    "\t<metric name=\"metric_3DXP memory WPQ write latency (ns)\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_INSERTS</event>\n",
    "        <event alias=\"c\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL</event>\n",
    "        <constant alias=\"socket_count\">2</constant>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "       <formula>((c/b)/(a/(socket_count*channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"0\">((c[0]/b[0])/(a[0]/(channels_populated_per_socket)))*1000000000</formula>\n",
    "       <formula socket=\"1\">((c[1]/b[1])/(a[1]/(channels_populated_per_socket)))*1000000000</formula> \n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_3DXP avg entries in WPQ\">\n",
    "        <event alias=\"b\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL</event>\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>c/(b*aep_dimms_per_socket/channels_populated_per_socket)</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_3DXP avg entries in WPQ when not empty\">\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1</event>\n",
    "        <event alias=\"c\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL</event>\n",
    "       <formula>c/b</formula>\n",
    "    </metric>\n",
    "\t\n",
    "   <metric name=\"metric_3DXP % cycles when WPQ has 1 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP % cycles when WPQ has 10 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=10</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "      <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_3DXP % cycles when WPQ is empty\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>100*(1-(b/(a*aep_dimms_per_socket/channels_populated_per_socket)))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP % cycles when WPQ has 20 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=20</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "      <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP % cycles when WPQ has 30 or more entries\">\n",
    "        <event alias=\"a\">UNC_M_CLOCKTICKS</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=30</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "      <formula>100*(b/(a*aep_dimms_per_socket/channels_populated_per_socket))</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP avg time (dclk) WPQ not empty\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1</event>\n",
    "       <formula>b/a</formula>\n",
    "    </metric>\n",
    "\t\n",
    "   <metric name=\"metric_3DXP avg time (dclk) WPQ empty\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1</event>\n",
    "        <event alias=\"c\">UNC_M_CLOCKTICKS</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>((c*aep_dimms_per_socket/channels_populated_per_socket)-b)/a</formula>\n",
    "    </metric>\n",
    "\n",
    "   <metric name=\"metric_3DXP avg time (dclk) with 30 or more entries in WPQ\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=30:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=30</event>\n",
    "       <formula>b/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_CHA % cyles Fast asserted\">\n",
    "\t\t<event alias=\"a\">UNC_CHA_FAST_ASSERTED.HORZ:u0x1</event>\n",
    "        <event alias=\"c\">UNC_CHA_CLOCKTICKS</event>\n",
    "\t\t<formula>100*a/c</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "   <metric name=\"metric_3DXP avg time (dclk) with less than 30 entries in WPQ\">\n",
    "        <event alias=\"a\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=30:e1</event>\n",
    "        <event alias=\"b\">UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=30</event>\n",
    "        <event alias=\"c\">UNC_M_CLOCKTICKS</event>\n",
    "         <!-- Assumed 6 memory channels populated in each socket; if not change the constant below -->\n",
    "        <constant alias=\"channels_populated_per_socket\">6</constant>\n",
    "        <constant alias=\"aep_dimms_per_socket\">6</constant>\n",
    "       <formula>((c*aep_dimms_per_socket/channels_populated_per_socket)-b)/a</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_CHA RxC IRQ latency (ns)\">\n",
    "        <event alias=\"a\">UNC_CHA_RxC_OCCUPANCY.IRQ</event>\n",
    "        <event alias=\"b\">UNC_CHA_RxC_INSERTS.IRQ</event>\n",
    "        <event alias=\"c\">UNC_CHA_CLOCKTICKS</event>\n",
    "       <constant alias=\"socket_count\">2</constant>\n",
    "\t\t<!-- In case the # of cache slices is not the same as the # of cores (as in the case of offlining some cores,\n",
    "\t\t\tchange the cores.count below to the actual number of slices present (which is the original core count -->\n",
    "       <constant alias=\"d\">system.sockets[0].cores.count</constant>\n",
    "         <formula>1000000000*(a/b)/(c/(d*socket_count))</formula>\n",
    "        <formula socket=\"0\">1000000000*(a[0]/b[0])/(c[0]/d)</formula>\n",
    "        <formula socket=\"1\">1000000000*(a[1]/b[1])/(c[1]/d)</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_CHA RxC IRQ avg entries\">\n",
    "        <event alias=\"a\">UNC_CHA_RxC_OCCUPANCY.IRQ</event>\n",
    "        <event alias=\"c\">UNC_CHA_CLOCKTICKS</event>\n",
    "        <formula>a/c</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_CHA RxC IRQ % cycles when Q has 18 or more entries\">\n",
    "        <event alias=\"a\">UNC_CHA_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_CHA_RxC_OCCUPANCY.IRQ:t=18</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_M2M avg entries in TxC AD Q\">\n",
    "        <event alias=\"a\">UNC_CHA_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M2M_TxC_AD_OCCUPANCY</event>\n",
    "       <formula>c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_M2M avg entries in TxC BL Q\">\n",
    "        <event alias=\"a\">UNC_CHA_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M2M_TxC_BL_OCCUPANCY.ALL</event>\n",
    "       <formula>c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_M2M RxC AD latency (ns)\">\n",
    "        <event alias=\"a\">UNC_M2M_RxC_AD_OCCUPANCY</event>\n",
    "        <event alias=\"b\">UNC_M2M_RxC_AD_INSERTS</event>\n",
    "        <event alias=\"c\">UNC_CHA_CLOCKTICKS</event>\n",
    "       <constant alias=\"socket_count\">2</constant>\n",
    "\t\t<!-- In case the # of cache slices is not the same as the # of cores (as in the case of offlining some cores,\n",
    "\t\t\tchange the cores.count below to the actual number of slices present (which is the original core count -->\n",
    "       <constant alias=\"d\">system.sockets[0].cores.count</constant>\n",
    "         <formula>1000000000*(a/b)/(c/(d*socket_count))</formula>\n",
    "        <formula socket=\"0\">1000000000*(a[0]/b[0])/(c[0]/d)</formula>\n",
    "        <formula socket=\"1\">1000000000*(a[1]/b[1])/(c[1]/d)</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_M2M RxC AD avg entries\">\n",
    "        <event alias=\"a\">UNC_M2M_RxC_AD_OCCUPANCY</event>\n",
    "        <event alias=\"c\">UNC_CHA_CLOCKTICKS</event>\n",
    "        <formula>a/c</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_M2M RxC BL latency (ns)\">\n",
    "        <event alias=\"a\">UNC_M2M_RxC_BL_OCCUPANCY</event>\n",
    "        <event alias=\"b\">UNC_M2M_RxC_BL_INSERTS</event>\n",
    "        <event alias=\"c\">UNC_CHA_CLOCKTICKS</event>\n",
    "       <constant alias=\"socket_count\">2</constant>\n",
    "\t\t<!-- In case the # of cache slices is not the same as the # of cores (as in the case of offlining some cores,\n",
    "\t\t\tchange the cores.count below to the actual number of slices present (which is the original core count -->\n",
    "       <constant alias=\"d\">system.sockets[0].cores.count</constant>\n",
    "         <formula>1000000000*(a/b)/(c/(d*socket_count))</formula>\n",
    "        <formula socket=\"0\">1000000000*(a[0]/b[0])/(c[0]/d)</formula>\n",
    "        <formula socket=\"1\">1000000000*(a[1]/b[1])/(c[1]/d)</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_M2M RxC BL avg entries\">\n",
    "        <event alias=\"a\">UNC_M2M_RxC_BL_OCCUPANCY</event>\n",
    "        <event alias=\"c\">UNC_CHA_CLOCKTICKS</event>\n",
    "        <formula>a/c</formula>\n",
    "    </metric>\n",
    "\t\n",
    "    <metric name=\"metric_IO_bandwidth_disk_or_network_writes (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART0</event>\n",
    "        <event alias=\"b\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART1</event>\n",
    "        <event alias=\"c\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART2</event>\n",
    "        <event alias=\"d\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART3</event>\n",
    "        <formula>(a+b+c+d)*4/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_IO_bandwidth_disk_or_network_reads (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART0</event>\n",
    "        <event alias=\"b\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART1</event>\n",
    "        <event alias=\"c\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART2</event>\n",
    "        <event alias=\"d\">UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART3</event>\n",
    "        <formula>(a+b+c+d)*4/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_IO_number of partial PCI writes per sec\">\n",
    "        <event alias=\"a\">UNC_CHA_TOR_INSERTS.IO_HIT:filter1=0x40033</event>\n",
    "        <event alias=\"b\">UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x40033</event>\n",
    "        <formula>a+b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_IO_read cache miss(disk/network writes) bandwidth (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x43c33</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_IO_write cache miss(disk/network reads) bandwidth (MB/sec)\">\n",
    "        <event alias=\"a\">UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x49033</event>\n",
    "        <event alias=\"b\">UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x40033</event>\n",
    "        <formula>(a+b)*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_IONUMA % disk/network reads addressed to local memory\">\n",
    "        <event alias=\"a\">UNC_CHA_TOR_INSERTS.IO:filter1=0x49031</event>\n",
    "        <event alias=\"b\">UNC_CHA_TOR_INSERTS.IO:filter1=0x49032</event>\n",
    "        <event alias=\"c\">UNC_CHA_TOR_INSERTS.IO:filter1=0x40031</event>\n",
    "        <event alias=\"d\">UNC_CHA_TOR_INSERTS.IO:filter1=0x40032</event>\n",
    "        <formula>100*(b+d)/(a+b+c+d)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_IONUMA % disk/network reads addressed to remote memory\">\n",
    "        <event alias=\"a\">UNC_CHA_TOR_INSERTS.IO:filter1=0x49031</event>\n",
    "        <event alias=\"b\">UNC_CHA_TOR_INSERTS.IO:filter1=0x49032</event>\n",
    "        <event alias=\"c\">UNC_CHA_TOR_INSERTS.IO:filter1=0x40031</event>\n",
    "        <event alias=\"d\">UNC_CHA_TOR_INSERTS.IO:filter1=0x40032</event>\n",
    "        <formula>100*(a+c)/(a+b+c+d)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_MMIO reads per instr\">\n",
    "       <throughput-metric-name>metric_MMIO reads per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40040e33</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "        <formula>a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_MMIO writes per instr\">\n",
    "       <throughput-metric-name>metric_MMIO writes per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40041e33</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "        <formula>a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory reads vs. all requests\">\n",
    "        <event alias=\"a\">UNC_M_CAS_COUNT.RD</event>\n",
    "        <event alias=\"b\">UNC_M_CAS_COUNT.WR</event>\n",
    "        <formula>a/(a+b)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory Page Empty vs. all requests\">\n",
    "        <event alias=\"a\">UNC_M_PRE_COUNT.RD:u0xc</event>\n",
    "        <event alias=\"c\">UNC_M_PRE_COUNT.PAGE_MISS</event>\n",
    "        <event alias=\"d\">UNC_M_CAS_COUNT.RD</event>\n",
    "        <event alias=\"e\">UNC_M_CAS_COUNT.WR</event>\n",
    "        <formula>(a-c)/(d+e)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory Page Misses vs. all requests\">\n",
    "        <event alias=\"b\">UNC_M_PRE_COUNT.PAGE_MISS</event>\n",
    "        <event alias=\"c\">UNC_M_CAS_COUNT.RD</event>\n",
    "        <event alias=\"d\">UNC_M_CAS_COUNT.WR</event>\n",
    "        <formula>b/(c+d)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory Page Hits vs. all requests\">\n",
    "        <event alias=\"a\">UNC_M_PRE_COUNT.RD:u0xc</event>\n",
    "        <event alias=\"c\">UNC_M_CAS_COUNT.RD</event>\n",
    "        <event alias=\"d\">UNC_M_CAS_COUNT.WR</event>\n",
    "        <formula>1-(a/(c+d))</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory % Cycles where all DRAM ranks are in PPD mode\">\n",
    "        <event alias=\"a\">UNC_M_POWER_CHANNEL_PPD</event>\n",
    "        <event alias=\"b\">UNC_M_CLOCKTICKS</event>\n",
    "        <formula>100*a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_memory % Cycles Memory is in self refresh power mode\">\n",
    "        <event alias=\"a\">UNC_M_POWER_SELF_REFRESH</event>\n",
    "        <event alias=\"b\">UNC_M_CLOCKTICKS</event>\n",
    "        <formula>100*a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_ItoM operations (fast strings) that reference LLC per instr\">\n",
    "        <throughput-metric-name>metric_ItoM operations (fast strings) that reference LLC per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_HIT:filter1=0x49033</event>\n",
    "\t\t<event alias=\"b\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x49033</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>(a+b)/c</formula>\n",
    "\t</metric>\n",
    "\t\t\n",
    "\t<metric name=\"metric_ItoM operations (fast strings) that miss LLC per instr\">\n",
    "        <throughput-metric-name>metric_ItoM operations (fast strings) that miss LLC per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x49033</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/c</formula>\n",
    "\t</metric>\n",
    "\t\t\n",
    "    <metric name=\"metric_% Uops delivered from decoded Icache (DSB)\">\n",
    "        <event alias=\"a\">IDQ.DSB_UOPS</event>\n",
    "        <event alias=\"b\">UOPS_ISSUED.ANY</event>\n",
    "        <formula>100*(a/b)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_% Uops delivered from legacy decode pipeline (MITE)\">\n",
    "        <event alias=\"a\">IDQ.MITE_UOPS</event>\n",
    "        <event alias=\"b\">UOPS_ISSUED.ANY</event>\n",
    "        <formula>100*(a/b)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_% Uops delivered from microcode sequencer (MS)\">\n",
    "        <event alias=\"a\">IDQ.MS_UOPS</event>\n",
    "        <event alias=\"b\">UOPS_ISSUED.ANY</event>\n",
    "        <formula>100*(a/b)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_% Uops delivered from loop stream detector (LSD)\">\n",
    "        <event alias=\"a\">LSD.UOPS</event>\n",
    "        <event alias=\"b\">UOPS_ISSUED.ANY</event>\n",
    "        <formula>100*(a/b)</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_FP scalar single-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP scalar single-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.SCALAR_SINGLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_FP scalar double-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP scalar double-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.SCALAR_DOUBLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_FP 128-bit packed single-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP 128-bit packed single-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.128B_PACKED_SINGLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\t\n",
    "\t<metric name=\"metric_FP 128-bit packed double-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP 128-bit packed double-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.128B_PACKED_DOUBLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\t\n",
    "\t<metric name=\"metric_FP 256-bit packed single-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP 256-bit packed single-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.256B_PACKED_SINGLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\t\n",
    "\t<metric name=\"metric_FP 256-bit packed double-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP 256-bit packed double-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.256B_PACKED_DOUBLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_FP 512-bit packed single-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP 512-bit packed single-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.512B_PACKED_SINGLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\t\t\n",
    "\t<metric name=\"metric_FP 512-bit packed double-precision FP instructions retired per instr\">\n",
    "        <throughput-metric-name>metric_FP 512-bit packed double-precision FP instructions retired per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">FP_ARITH_INST_RETIRED.512B_PACKED_DOUBLE</event>\n",
    "\t\t<event alias=\"b\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_DRAM power (watts)\">\n",
    "\t\t<event alias=\"a\">MSR_EVENT:msr=0x619:type=FREERUN:scope=PACKAGE</event>\n",
    "\t\t<formula>a*15.3/1000000</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_package power (watts)\">\n",
    "\t\t<event alias=\"a\">MSR_EVENT:msr=0x611:type=FREERUN:scope=PACKAGE</event>\n",
    "\t\t<formula>a*61/1000000</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_core c6 residency %\">\n",
    "\t\t<event alias=\"a\">MSR_EVENT:msr=0x3FD:type=FREERUN:scope=THREAD</event>\n",
    "\t\t<event alias=\"b\">TSC</event>\n",
    "\t\t<formula>100*a/b</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_package c2 residency %\">\n",
    "\t\t<event alias=\"a\">MSR_EVENT:msr=0x60D:type=FREERUN:scope=PACKAGE</event>\n",
    "\t\t<event alias=\"b\">TSC</event>\n",
    "\t\t<constant alias=\"cpu_count\">system.sockets[0].cpus.count</constant>      \n",
    "\t\t<formula>100*a*cpu_count/b</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_package c6 residency %\">\n",
    "\t\t<event alias=\"a\">MSR_EVENT:msr=0x3F9:type=FREERUN:scope=PACKAGE</event>\n",
    "\t\t<event alias=\"b\">TSC</event>\n",
    "\t\t<constant alias=\"cpu_count\">system.sockets[0].cpus.count</constant>      \n",
    "\t\t<formula>100*a*cpu_count/b</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_core SW prefetch NTA per instr\">\n",
    "        <throughput-metric-name>metric_core SW prefetch NTA per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">SW_PREFETCH_ACCESS.NTA</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/c</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_core uncacheable access and clflushes per instr\">\n",
    "        <throughput-metric-name>metric_core uncacheable access and clflushes per txn</throughput-metric-name>\n",
    "\t\t<event alias=\"a\">OFFCORE_REQUESTS.MEM_UC</event>\n",
    "\t\t<event alias=\"c\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>a/c</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_core % cycles core power throttled\">\n",
    "\t\t<event alias=\"a\">CORE_POWER.THROTTLE</event>\n",
    "\t\t<event alias=\"c\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*a/c</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_core % cycles in non AVX license\">\n",
    "\t\t<event alias=\"a\">CORE_POWER.LVL0_TURBO_LICENSE</event>\n",
    "\t\t<event alias=\"b\">CORE_POWER.LVL1_TURBO_LICENSE</event>\n",
    "\t\t<event alias=\"c\">CORE_POWER.LVL2_TURBO_LICENSE</event>\n",
    "\t\t<formula>100*a/(a+b+c)</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_core % cycles in AVX2 license\">\n",
    "\t\t<event alias=\"a\">CORE_POWER.LVL0_TURBO_LICENSE</event>\n",
    "\t\t<event alias=\"b\">CORE_POWER.LVL1_TURBO_LICENSE</event>\n",
    "\t\t<event alias=\"c\">CORE_POWER.LVL2_TURBO_LICENSE</event>\n",
    "\t\t<formula>100*b/(a+b+c)</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_core % cycles in AVX-512 license\">\n",
    "\t\t<event alias=\"a\">CORE_POWER.LVL0_TURBO_LICENSE</event>\n",
    "\t\t<event alias=\"b\">CORE_POWER.LVL1_TURBO_LICENSE</event>\n",
    "\t\t<event alias=\"c\">CORE_POWER.LVL2_TURBO_LICENSE</event>\n",
    "\t\t<formula>100*c/(a+b+c)</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_SMI number of SMIs per sec\">\n",
    "\t\t<event alias=\"a\">MSR_EVENT:msr=0x34:type=FREERUN:scope=PACKAGE</event>\n",
    "\t\t<formula>a+0</formula>\n",
    "\t</metric>\n",
    "\n",
    "    <metric name=\"metric_core initiated local dram read bandwidth (MB/sec)\">\n",
    "         <!-- Includes demand+L2pref+L3 pref for all data read, code read and rfo that hit local dram-->\n",
    "        <event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3f840007f7</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_core initiated remote dram read bandwidth (MB/sec)\">\n",
    "         <!-- Includes demand+L2pref+L3 pref for all data read, code read and rfo that hit remote dram -->\n",
    "        <event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3fB80007f7</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_core initiated local DCPMEM read bandwidth (MB/sec)\">\n",
    "         <!-- Includes demand+L2pref+L3 pref for all data read, code read and rfo that hit local DCPMEM-->\n",
    "        <event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3f804007f7</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_core initiated remote DCPMEM read bandwidth (MB/sec)\">\n",
    "         <!-- Includes demand+L2pref+L3 pref for all data read, code read and rfo that hit remote DCPMEM -->\n",
    "        <event alias=\"a\">OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3f838007f7</event>\n",
    "        <formula>a*64/1000000</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_M3UPI avg all VN0 entries\">\n",
    "        <event alias=\"a\">UNC_M3UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f</event>\n",
    "       <formula>c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_M3UPI % cycles when all VN0 has 1 or more entries\">\n",
    "        <event alias=\"a\">UNC_M3UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=1</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_M3UPI % cycles when all VN0 has 10 or more entries\">\n",
    "        <event alias=\"a\">UNC_M3UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=10</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_M3UPI % cycles when all VN0 has 30 or more entries\">\n",
    "        <event alias=\"a\">UNC_M3UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=30</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_M3UPI % cycles when all VN0 has 50 or more entries\">\n",
    "        <event alias=\"a\">UNC_M3UPI_CLOCKTICKS</event>\n",
    "        <event alias=\"c\">UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=50</event>\n",
    "       <formula>100*c/a</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_Info_CoreIPC\">\n",
    "        <event alias=\"a\">INST_RETIRED.ANY</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>(a/(b/threads))</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_TMAM_Info_Memory Level Parallelism\">\n",
    "\t\t<event alias=\"a\">L1D_PEND_MISS.PENDING</event>\n",
    "\t\t<event alias=\"b\">L1D_PEND_MISS.PENDING_CYCLES_ANY</event>\t\t\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>(a/(b/threads))</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_TMAM_Info_L1D_Load_Miss_Latency(ns)\">\n",
    "\t\t<event alias=\"a\">L1D_PEND_MISS.PENDING</event>\n",
    "\t\t<event alias=\"f\">MEM_LOAD_RETIRED.FB_HIT</event>\t\t\n",
    "\t\t<event alias=\"g\">MEM_LOAD_RETIRED.L1_MISS</event>\t\t\n",
    "        <event alias=\"c\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.REF_TSC</event>\n",
    "        <constant alias=\"e\">system.tsc_freq</constant>\n",
    "        <formula>1000000000*(a/(f+g))/(c/d*e)</formula>\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_TMAM_Info_cycles_both_threads_active(%)\">\n",
    "\t\t<event alias=\"a\">CPU_CLK_THREAD_UNHALTED.ONE_THREAD_ACTIVE</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_THREAD_UNHALTED.REF_XCLK_ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(((threads)&lt;2)?0:(1-a/(b/2)))</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_Frontend_Bound(%)\">\n",
    "\t\t<event alias=\"a\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<event alias=\"c\">IDQ_UOPS_NOT_DELIVERED.CORE</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*c/(4*(a/threads))</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_..Frontend_Latency(%)\">\n",
    "\t\t<event alias=\"a\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<event alias=\"c\">IDQ_UOPS_NOT_DELIVERED.CYCLES_0_UOPS_DELIV.CORE</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*c/(a/threads)</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_....ICache_Misses(%)\">\n",
    "\t\t<event alias=\"a\">ICACHE_16B.IFDATA_STALL</event>\n",
    "\t\t<event alias=\"b\">ICACHE_16B.IFDATA_STALL:c1:e1</event>\n",
    "\t\t<event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(a+2*b)/d</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....ITLB_Misses(%)\">\n",
    "\t\t<event alias=\"a\">ICACHE_64B.IFTAG_STALL</event>\n",
    "\t\t<event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*a/d</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....Branch_Resteers(%)\">\n",
    "        <event alias=\"a\">INT_MISC.CLEAR_RESTEER_CYCLES</event>\n",
    "        <event alias=\"b\">BACLEARS.ANY</event>\n",
    "\t\t<event alias=\"j\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(a+9*b)/j</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......Mispredicts_Resteers(%)\">\n",
    "        <event alias=\"a\">INT_MISC.CLEAR_RESTEER_CYCLES</event>\n",
    "        <event alias=\"b\">MACHINE_CLEARS.COUNT</event>\n",
    "        <event alias=\"c\">BR_MISP_RETIRED.ALL_BRANCHES</event>\n",
    "\t\t<event alias=\"j\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*a*(c/(b+c))/j</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_......Clears_Resteers(%)\">\n",
    "        <event alias=\"a\">INT_MISC.CLEAR_RESTEER_CYCLES</event>\n",
    "        <event alias=\"b\">MACHINE_CLEARS.COUNT</event>\n",
    "        <event alias=\"c\">BR_MISP_RETIRED.ALL_BRANCHES</event>\n",
    "\t\t<event alias=\"j\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*a*(1-(c/(b+c)))/j</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......Unknown_Branches_Resteers(%)\">\n",
    "        <event alias=\"b\">BACLEARS.ANY</event>\n",
    "\t\t<event alias=\"j\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(9*b)/j</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....DSB_Switches(%)\">\n",
    "\t\t<event alias=\"a\">DSB2MITE_SWITCHES.PENALTY_CYCLES</event>\n",
    "\t\t<event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*a/d</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_....MS_Switches(%)\">\n",
    "\t\t<event alias=\"a\">IDQ.MS_SWITCHES</event>\n",
    "\t\t<event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<!-- 2 cycle is the MS switch cost -->\n",
    "\t\t<formula>100*2*a/d</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_..Frontend_Bandwidth(%)\">\n",
    "\t\t<event alias=\"a\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<event alias=\"c\">IDQ_UOPS_NOT_DELIVERED.CORE</event>\n",
    "\t\t<event alias=\"d\">IDQ_UOPS_NOT_DELIVERED.CYCLES_0_UOPS_DELIV.CORE</event>\t\t\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(c-4*d)/(4*(a/threads))</formula>\n",
    "\t</metric>\n",
    "\n",
    "    <metric name=\"metric_TMAM_Bad_Speculation(%)\">\n",
    "        <event alias=\"a\">UOPS_ISSUED.ANY</event>\n",
    "        <event alias=\"b\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"c\">INT_MISC.RECOVERY_CYCLES_ANY</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "        <formula>100*(a-b+(4*c/threads))/(4*d/threads)</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_..Branch_Mispredicts(%)\">\n",
    "        <event alias=\"a\">UOPS_ISSUED.ANY</event>\n",
    "        <event alias=\"b\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"c\">INT_MISC.RECOVERY_CYCLES_ANY</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"e\">BR_MISP_RETIRED.ALL_BRANCHES</event>\n",
    "        <event alias=\"f\">MACHINE_CLEARS.COUNT</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "        <formula>(e/(e+f))*100*(a-b+(4*c/threads))/(4*d/threads)</formula>\n",
    "    </metric>\n",
    "\n",
    "    <metric name=\"metric_TMAM_..Machine_Clears(%)\">\n",
    "        <event alias=\"a\">UOPS_ISSUED.ANY</event>\n",
    "        <event alias=\"b\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"c\">INT_MISC.RECOVERY_CYCLES_ANY</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"e\">BR_MISP_RETIRED.ALL_BRANCHES</event>\n",
    "        <event alias=\"f\">MACHINE_CLEARS.COUNT</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "        <formula>(f/(e+f))*100*(a-b+(4*c/threads))/(4*d/threads)</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_Backend_bound(%)\">\n",
    "\t\t<event alias=\"a\">IDQ_UOPS_NOT_DELIVERED.CORE</event>\n",
    "\t\t<event alias=\"b\">UOPS_ISSUED.ANY</event>\n",
    "\t\t<event alias=\"c\">INT_MISC.RECOVERY_CYCLES_ANY</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"e\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100-(100*(b-e+4*(c/threads)+a+e)/(4*d/threads))</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_..Memory_Bound(%)\">\n",
    "\t\t<event alias=\"a\">IDQ_UOPS_NOT_DELIVERED.CORE</event>\n",
    "\t\t<event alias=\"b\">UOPS_ISSUED.ANY</event>\n",
    "\t\t<event alias=\"c\">INT_MISC.RECOVERY_CYCLES_ANY</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"e\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"f\">CYCLE_ACTIVITY.STALLS_MEM_ANY</event>\n",
    "        <event alias=\"g\">EXE_ACTIVITY.BOUND_ON_STORES</event>\n",
    "        <event alias=\"j\">EXE_ACTIVITY.1_PORTS_UTIL</event>\n",
    "        <event alias=\"k\">EXE_ACTIVITY.2_PORTS_UTIL</event>\n",
    "        <event alias=\"m\">EXE_ACTIVITY.EXE_BOUND_0_PORTS</event>\n",
    "        <event alias=\"p\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "        <event alias=\"q\">INST_RETIRED.ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(1-((b-e+4*(c/threads)+a+e)/(4*d/threads)))*(f+g)/(m+j+(((q/p)&gt;1.8)?k:0)+f+g)</formula>\n",
    "\t</metric>\n",
    " \n",
    "\t<metric name=\"metric_TMAM_....L1_Bound(%)\">\n",
    " \t\t<event alias=\"a\">CYCLE_ACTIVITY.STALLS_MEM_ANY</event>\n",
    "        <event alias=\"b\">CYCLE_ACTIVITY.STALLS_L1D_MISS</event>\n",
    "        <event alias=\"c\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(a-b)/c</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......DTLB_Load(%)\">\n",
    "        <event alias=\"a\">DTLB_LOAD_MISSES.STLB_HIT</event>\n",
    "        <event alias=\"b\">DTLB_LOAD_MISSES.WALK_ACTIVE</event>\n",
    "        <event alias=\"c\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(7*a+b)/c</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......Store_Fwd_Blk(%)\">\n",
    "\t\t<event alias=\"a\">LD_BLOCKS.STORE_FORWARD</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*((13 * a) / b) </formula>\t\t\t\t\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......Lock_Latency(%)\">\n",
    "\t\t<event alias=\"a\">MEM_INST_RETIRED.LOCK_LOADS</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<event alias=\"c\">MEM_INST_RETIRED.ALL_STORES</event>\n",
    "\t\t<event alias=\"d\">OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_DEMAND_RFO</event>\n",
    "\t\t<formula>100*(((a / c)* [1*b ,1*d].min)/ b) </formula>\t\t\t\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....L2_Bound(%)\">\n",
    "        <event alias=\"a\">CYCLE_ACTIVITY.STALLS_L1D_MISS</event>\n",
    "        <event alias=\"b\">CYCLE_ACTIVITY.STALLS_L2_MISS</event>\n",
    "        <event alias=\"c\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(a-b)/c</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....L3_Bound(%)\">\n",
    "        <event alias=\"a\">CYCLE_ACTIVITY.STALLS_L2_MISS</event>\n",
    "        <event alias=\"c\">CYCLE_ACTIVITY.STALLS_L3_MISS</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(a-c)/d</formula>\n",
    "    </metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_......Contested_Accesses(%)\">\n",
    "\t\t<event alias=\"a\">MEM_LOAD_L3_HIT_RETIRED.XSNP_HITM</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<event alias=\"c\">MEM_LOAD_L3_HIT_RETIRED.XSNP_MISS</event>\n",
    "\t\t<formula>100*(60 * ( a + c ) / b) </formula>\t\t\t\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......Data_Sharing(%)\">\n",
    "\t\t<event alias=\"a\">MEM_LOAD_L3_HIT_RETIRED.XSNP_HIT</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(43 * a /b) </formula>\t\t\t\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......L3_Latency(%)\">\n",
    "        <event alias=\"b\">OFFCORE_REQUESTS_OUTSTANDING.DEMAND_DATA_RD_GE_6</event>\n",
    "        <event alias=\"c\">OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "        <event alias=\"e\">OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_DEMAND_DATA_RD</event>\n",
    "        <event alias=\"f\">OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_L3_MISS_DEMAND_DATA_RD</event>\n",
    "\t\t<formula>100*((([1*e ,1*d].min-[1*f ,1*d].min)/d)-(([1*b ,1*d].min-[1*c ,1*d].min)/d))</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......L3_Bandwidth(%)\">\n",
    "        <event alias=\"b\">OFFCORE_REQUESTS_OUTSTANDING.DEMAND_DATA_RD_GE_6</event>\n",
    "        <event alias=\"c\">OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*([1*b ,1*d].min-[1*c ,1*d].min)/d</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......SQ_Full(%)\">\n",
    "\t\t<event alias=\"a\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<event alias=\"b\">OFFCORE_REQUESTS_BUFFER.SQ_FULL</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(b/threads)/(a/threads)</formula>\n",
    "\t</metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_....MEM_Bound(%)\">\n",
    "        <event alias=\"a\">CYCLE_ACTIVITY.STALLS_L3_MISS</event>\n",
    "         <event alias=\"d\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(a/d)</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......MEM_Bandwidth(%)\">\n",
    "        <event alias=\"a\">OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6</event>\n",
    "        <event alias=\"b\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*([1*a, 1*b].min)/b</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......MEM_Latency(%)\">\n",
    "        <event alias=\"a\">OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6</event>\n",
    "        <event alias=\"b\">OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_L3_MISS_DEMAND_DATA_RD</event>\n",
    "        <event alias=\"c\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(([1*b, 1*c].min)-([1*a, 1*c].min))/c</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....Stores_Bound(%)\">\n",
    "        <event alias=\"a\">EXE_ACTIVITY.BOUND_ON_STORES</event>\n",
    "        <event alias=\"b\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*(a/b)</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......DTLB_Store(%)\">\n",
    "        <event alias=\"a\">DTLB_STORE_MISSES.STLB_HIT</event>\n",
    "        <event alias=\"b\">DTLB_STORE_MISSES.WALK_ACTIVE</event>\n",
    "        <event alias=\"c\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(7*a+b)/(c/threads)</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_..Core_Bound(%)\">\n",
    "\t\t<event alias=\"a\">IDQ_UOPS_NOT_DELIVERED.CORE</event>\n",
    "\t\t<event alias=\"b\">UOPS_ISSUED.ANY</event>\n",
    "\t\t<event alias=\"c\">INT_MISC.RECOVERY_CYCLES_ANY</event>\n",
    "        <event alias=\"d\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"e\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"f\">CYCLE_ACTIVITY.STALLS_MEM_ANY</event>\n",
    "        <event alias=\"g\">EXE_ACTIVITY.BOUND_ON_STORES</event>\n",
    "         <event alias=\"j\">EXE_ACTIVITY.1_PORTS_UTIL</event>\n",
    "        <event alias=\"k\">EXE_ACTIVITY.2_PORTS_UTIL</event>\n",
    "        <event alias=\"m\">EXE_ACTIVITY.EXE_BOUND_0_PORTS</event>\n",
    "        <event alias=\"p\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "        <event alias=\"q\">INST_RETIRED.ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(1-((b-e+4*(c/threads)+a+e)/(4*d/threads)))*(1-((f+g)/(m+j+(((q/p)&gt;1.8)?k:0)+f+g)))</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....Divider(%)\">\n",
    "        <event alias=\"a\">ARITH.DIVIDER_ACTIVE</event>\n",
    "        <event alias=\"b\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "\t\t<formula>100*a/b</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....Ports_Utilization(%)\">\n",
    "        <event alias=\"d\">EXE_ACTIVITY.EXE_BOUND_0_PORTS</event>\n",
    "        <event alias=\"f\">EXE_ACTIVITY.1_PORTS_UTIL</event>\n",
    "        <event alias=\"g\">CYCLE_ACTIVITY.STALLS_MEM_ANY</event>\n",
    "        <event alias=\"h\">EXE_ACTIVITY.BOUND_ON_STORES</event>\n",
    "        <event alias=\"j\">EXE_ACTIVITY.2_PORTS_UTIL</event>\n",
    "        <event alias=\"p\">CPU_CLK_UNHALTED.THREAD</event>\n",
    "        <event alias=\"q\">INST_RETIRED.ANY</event>\n",
    "\t\t<formula>100*((d+f+(((q/p)&gt;1.8)?j:0)+g+h)-g-h)/p</formula>\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......0_Ports_Utilized(%)\">\n",
    "\t\t<event alias=\"a\">UOPS_EXECUTED.CORE_CYCLES_NONE</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<event alias=\"c\">EXE_ACTIVITY.EXE_BOUND_0_PORTS</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*((((threads)&gt;1)?(a/2):c)/(b/threads))</formula>\t\t\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......1_Port_Utilized(%)\">\n",
    "\t\t<event alias=\"a\">UOPS_EXECUTED.CORE_CYCLES_GE_1</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<event alias=\"c\">UOPS_EXECUTED.CORE_CYCLES_GE_2</event>\n",
    "\t\t<event alias=\"d\">EXE_ACTIVITY.1_PORTS_UTIL</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*((((threads)&gt;1)?((a-c)/2):d)/(b/threads))</formula>\t\t\t\n",
    "\t</metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......2_Ports_Utilized(%)\">\n",
    "\t\t<event alias=\"a\">UOPS_EXECUTED.CORE_CYCLES_GE_2</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<event alias=\"c\">UOPS_EXECUTED.CORE_CYCLES_GE_3</event>\n",
    "\t\t<event alias=\"d\">EXE_ACTIVITY.2_PORTS_UTIL</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*((((threads)&gt;1)?((a-c)/2):d)/(b/threads))</formula>\t\t\t\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_TMAM_......3m_Ports_Utilized(%)\">\n",
    "\t\t<event alias=\"a\">UOPS_EXECUTED.CORE_CYCLES_GE_3</event>\n",
    "\t\t<event alias=\"b\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(a/threads)/(b/threads)</formula>\t\t\t\n",
    "\t</metric>\t\n",
    "\n",
    "\t<metric name=\"metric_TMAM_Retiring(%)\">\n",
    " \t\t<event alias=\"a\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"b\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*(b/(4*(a/threads)))</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_..Base(%)\">\n",
    " \t\t<event alias=\"a\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"b\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"c\">IDQ.MS_UOPS</event>\n",
    "        <event alias=\"d\">UOPS_ISSUED.ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*((b/(4*(a/threads)))-((b/d)*c/(4*(a/threads))))</formula>\n",
    "    </metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_....FP_Arith(%)\">\n",
    " \t\t<event alias=\"a\">INST_RETIRED.ANY</event>\n",
    "        <event alias=\"b\">UOPS_EXECUTED.X87</event>\n",
    "        <event alias=\"c\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"d\">FP_ARITH_INST_RETIRED.SCALAR_SINGLE</event>\n",
    "        <event alias=\"e\">FP_ARITH_INST_RETIRED.SCALAR_DOUBLE</event>\n",
    "        <event alias=\"f\">FP_ARITH_INST_RETIRED.128B_PACKED_DOUBLE</event>\n",
    "        <event alias=\"g\">FP_ARITH_INST_RETIRED.128B_PACKED_SINGLE</event>\n",
    "        <event alias=\"h\">FP_ARITH_INST_RETIRED.256B_PACKED_SINGLE</event>\n",
    "        <event alias=\"j\">FP_ARITH_INST_RETIRED.256B_PACKED_DOUBLE</event>\n",
    "        <event alias=\"k\">UOPS_EXECUTED.THREAD</event>\n",
    "        <event alias=\"m\">FP_ARITH_INST_RETIRED.512B_PACKED_SINGLE</event>\n",
    "        <event alias=\"n\">FP_ARITH_INST_RETIRED.512B_PACKED_DOUBLE</event>\n",
    "\t\t<formula>100*((b/k)+((d+e+f+g+h+j+m+n)/c))</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_TMAM_....Other(%)\">\n",
    " \t\t<event alias=\"a\">INST_RETIRED.ANY</event>\n",
    "        <event alias=\"b\">UOPS_EXECUTED.X87</event>\n",
    "        <event alias=\"c\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"d\">FP_ARITH_INST_RETIRED.SCALAR_SINGLE</event>\n",
    "        <event alias=\"e\">FP_ARITH_INST_RETIRED.SCALAR_DOUBLE</event>\n",
    "        <event alias=\"f\">FP_ARITH_INST_RETIRED.128B_PACKED_DOUBLE</event>\n",
    "        <event alias=\"g\">FP_ARITH_INST_RETIRED.128B_PACKED_SINGLE</event>\n",
    "        <event alias=\"h\">FP_ARITH_INST_RETIRED.256B_PACKED_SINGLE</event>\n",
    "        <event alias=\"j\">FP_ARITH_INST_RETIRED.256B_PACKED_DOUBLE</event>\n",
    "        <event alias=\"k\">UOPS_EXECUTED.THREAD</event>\n",
    "        <event alias=\"m\">FP_ARITH_INST_RETIRED.512B_PACKED_SINGLE</event>\n",
    "        <event alias=\"n\">FP_ARITH_INST_RETIRED.512B_PACKED_DOUBLE</event>\n",
    "\t\t<formula>100*(1-((b/k)+((d+e+f+g+h+j+m+n)/c)))</formula>\n",
    "    </metric>\n",
    "\t\n",
    "\t<metric name=\"metric_TMAM_..Microcode_Sequencer(%)\">\n",
    " \t\t<event alias=\"a\">CPU_CLK_UNHALTED.THREAD_ANY</event>\n",
    "        <event alias=\"b\">UOPS_RETIRED.RETIRE_SLOTS</event>\n",
    "        <event alias=\"c\">IDQ.MS_UOPS</event>\n",
    "        <event alias=\"d\">UOPS_ISSUED.ANY</event>\n",
    "\t\t<constant alias=\"threads\">system.sockets[0][0].size</constant>      \n",
    "\t\t<formula>100*((b/d)*c/(4*(a/threads)))</formula>\n",
    "    </metric>\n",
    "\n",
    "\t<metric name=\"metric_EDP SKX XML version\">\n",
    " \t\t<constant alias=\"version\">3.9</constant>      \n",
    "\t\t<formula>version</formula>\n",
    "   </metric>\n",
    "\t\n",
    "</root>'''\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.fromstring(metrics)\n",
    "lst = tree.findall('metric')\n",
    "metrics_list=[]\n",
    "for item in lst:\n",
    "    if len(item.findall('constant'))==0:\n",
    "        out='''             '{:s}':{{\n",
    "                                'sum_func':.self.cores_sum,   \n",
    "                                'events':{{\n",
    "                                      {:s}\n",
    "                                }},\n",
    "                                'formula':{{\n",
    "                                       '{:s}':'{:s}'\n",
    "                                }},\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            }}'''.format(item.get('name'),','.join([\"'{:s}':'{:s}'\".format(l.get('alias'),l.text) for l in item.findall('event')]),item.get('name'),item.findtext('formula'))\n",
    "        metrics_list.append(out)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_alias_name(metric,func):\n",
    "    return metric+\"_\"+func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def splits_fill0(x):\n",
    "    fi=[]\n",
    "    for l in x:\n",
    "        li=re.split(r'\\s+',l.strip())\n",
    "        li=[l.replace(\",\",\"\") for l in li]\n",
    "        for j in range(len(li),256):\n",
    "            li.append('0')\n",
    "        fi.append(li)\n",
    "    return iter(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssdd'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ss,dd\".replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## emon analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-675f568dc058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEmon_Analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mAnalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Analysis' is not defined"
     ]
    }
   ],
   "source": [
    "class Emon_Analysis(Analysis):\n",
    "    def __init__(self,emon_file):\n",
    "        Analysis.__init__(self,emon_file)\n",
    "        \n",
    "        paths=os.path.split(self.file)\n",
    "        if fs.exists(paths[0]+\"/emonv.txt\"):\n",
    "            with fs.open(paths[0]+\"/emonv.txt\") as f:\n",
    "                allcnt = f.read().decode('ascii')\n",
    "            for l in allcnt.split(\"\\n\"):\n",
    "                if l.startswith(\"total_number_of_processors\"):\n",
    "                    self.totalcores=int(re.split(\" +\",l)[2])\n",
    "                elif re.search(\"Number of Packages: +(\\d+)\",l):\n",
    "                    self.numberofpackages=int(re.search(\"Number of Packages: +(\\d+)\",l).group(1))\n",
    "                elif re.search(\"Cores Per Package: +(\\d+)\",l):\n",
    "                    self.coresperpackage=int(re.search(\"Cores Per Package: +(\\d+)\",l).group(1))\n",
    "                elif re.search(\"Threads Per Package: +(\\d+)\",l):\n",
    "                    self.threadsperpackage=int(re.search(\"Threads Per Package: +(\\d+)\",l).group(1))\n",
    "                elif re.search(\"TSC Freq +[.]+ +([0-9.]+)\",l):\n",
    "                    self.tsc=int(float(re.search(\"TSC Freq +[.]+ +([0-9.]+)\",l).group(1))*1000000)\n",
    "        else:\n",
    "            print(\"Wrong, no emonv specified\")\n",
    "\n",
    "\n",
    "        if fs.exists(paths[0]+\"/emon.rst\"):\n",
    "            with fs.open(paths[0]+\"/emon.rst\") as f:\n",
    "                self.unc_cha_cnt = len(next((l.decode('ascii').split(\"\\t\") for l in f if \"UNC_CHA_CLOCKTICKS\" in l.decode('ascii')),\"\"))-3\n",
    "            \n",
    "        self.begin_clk=0\n",
    "        self.end_clk=0\n",
    "\n",
    "#        paths=os.path.split(self.file)\n",
    "#        if fs.exists(paths[0]+\"/xgbtck.txt\"):\n",
    "#            with fs.open(paths[0]+\"/xgbtck.txt\") as f:\n",
    "#                allcnt = f.read().decode('ascii')\n",
    "#            if len(allcnt)>0:\n",
    "#                begins=[]\n",
    "#                ends=[]\n",
    "#                for l in allcnt.split(\"\\n\"):\n",
    "#                    if (\"begin\" in l):\n",
    "#                        begins.append(l.split(\" \"))\n",
    "#                    if ('end' in l):\n",
    "#                        ends.append(l.split(\" \"))\n",
    "#                if len(begins)>0:\n",
    "#                    self.begin_clk=min([int(l[3]) for l in begins])\n",
    "#                if len(ends)>0:\n",
    "#                    self.end_clk=max([int(l[3]) for l in ends])\n",
    "            \n",
    "        self.corecnt=self.totalcores\n",
    "        \n",
    "        self.emon_metrics=collections.OrderedDict({\n",
    "            'emon_cpuutil':{\n",
    "                'sum_func':self.cores_sum,   \n",
    "                'events':{\n",
    "                    'a':'CPU_CLK_UNHALTED.REF_TSC'\n",
    "                },\n",
    "                'formula':{\n",
    "                    'cpu%':'a/({:f}*{:d})'.format(self.tsc,self.corecnt)\n",
    "                },\n",
    "                'fmt':lambda l: F.round(l, 3)\n",
    "            },\n",
    "            'emon_cpufreq':{\n",
    "                'sum_func':self.cores_sum,   \n",
    "                'events':{\n",
    "                    'a':'CPU_CLK_UNHALTED.THREAD',\n",
    "                    'b':'CPU_CLK_UNHALTED.REF_TSC'\n",
    "                },\n",
    "                'formula':{\n",
    "                    'cpu freq':'a/b*{:f}'.format(self.tsc/1000000)\n",
    "                },\n",
    "                'fmt':lambda l: F.round(l, 3)\n",
    "            },\n",
    "            'emon_instr_retired':{\n",
    "                'sum_func':self.cores_sum,   \n",
    "                'events':{\n",
    "                    'a':'INST_RETIRED.ANY'\n",
    "                },\n",
    "                'formula':{\n",
    "                    'pathlength':'a/1000000000'\n",
    "                },\n",
    "                'fmt':lambda l: F.round(l, 0)\n",
    "            },\n",
    "            'emon_ipc':{\n",
    "                'sum_func':self.cores_sum,   \n",
    "                'events':{\n",
    "                    'a':'CPU_CLK_UNHALTED.THREAD',\n",
    "                    'b':'INST_RETIRED.ANY'\n",
    "                },\n",
    "                'formula':{\n",
    "                    'ipc':'b/a'\n",
    "                },\n",
    "                'fmt':lambda l: F.round(l, 3)\n",
    "            },\n",
    "            'emon_L3 stall':{'sum_func':self.cores_sum,   'events':{'a':'CYCLE_ACTIVITY.STALLS_L3_MISS','b':'CPU_CLK_UNHALTED.THREAD'},'formula':{'l3 stall':'a/b'},                         'fmt':lambda l: F.round(l, 3)},\n",
    "            'emon_mem stall':{'sum_func':self.cores_sum,   'events':{'a':'CYCLE_ACTIVITY.CYCLES_MEM_ANY','b':'CPU_CLK_UNHALTED.THREAD'},'formula':{'mem stall':'a/b'},                         'fmt':lambda l: F.round(l, 3)},\n",
    "            'emon_total stall':{'sum_func':self.cores_sum,   'events':{'a':'CYCLE_ACTIVITY.STALLS_TOTAL','b':'CPU_CLK_UNHALTED.THREAD'},'formula':{'total stall':'a/b'},                         'fmt':lambda l: F.round(l, 3)},\n",
    "            'emon_dtlb walk':{'sum_func':self.cores_sum,   'events':{'a':'DTLB_LOAD_MISSES.WALK_ACTIVE','b':'CPU_CLK_UNHALTED.THREAD'},'formula':{'dtlb walk':'a/b'},                         'fmt':lambda l: F.round(l, 3)},\n",
    "            'emon_uop issue stall':{'sum_func':self.cores_sum,   'events':{'a':'UOPS_ISSUED.STALL_CYCLES','b':'CPU_CLK_UNHALTED.THREAD'},'formula':{'issue stall':'a/b'},                         'fmt':lambda l: F.round(l, 3)},\n",
    "           \n",
    "            'emon_mem_bw':  {'sum_func':self.mem_sum,     'events':{'a':'UNC_M_CAS_COUNT.RD','b':'UNC_M_CAS_COUNT.WR'},                'formula':{'mem_bw_rd':'a*64/1000000','mem_bw_wr':'b*64/1000000'},   'fmt':lambda l: F.round(l, 0)},\n",
    "            'emon_remote ratio':  {'sum_func':self.cores_sum,   'events':{'a':'UNC_CHA_TOR_INSERTS.IA_MISS_DRD_REMOTE','b':'UNC_CHA_TOR_INSERTS.IA_MISS_DRD_LOCAL'},  'formula':{'remote ratio':'a/(a+b)'},   'fmt':lambda l: F.round(l, 1)},\n",
    "            \n",
    "            \n",
    "            'emon_local lat':  {'sum_func':self.cores_sum,   'events':{'a':'UNC_CHA_TOR_OCCUPANCY.IA_MISS_DRD_LOCAL','b':'UNC_CHA_TOR_INSERTS.IA_MISS_DRD_LOCAL','c':'UNC_CHA_CLOCKTICKS'},  'formula':{'local_mem_lat(ns)':'1000000000*a/b/c*{:d}'.format(self.unc_cha_cnt)},   'fmt':lambda l: F.round(l, 1)},\n",
    "            'emon_remote lat':  {'sum_func':self.cores_sum,   'events':{'a':'UNC_CHA_TOR_OCCUPANCY.IA_MISS_DRD_REMOTE','b':'UNC_CHA_TOR_INSERTS.IA_MISS_DRD_REMOTE','c':'UNC_CHA_CLOCKTICKS'},  'formula':{'remote_mem_lat(ns)':'1000000000*a/b/c*{:d}'.format(self.unc_cha_cnt)},   'fmt':lambda l: F.round(l, 1)},\n",
    "            \n",
    "            'emon_fp_vec':  {'sum_func':self.cores_sum,   'events':{'a':'FP_ARITH_INST_RETIRED.VECTOR','b':'INST_RETIRED.ANY'},  'formula':{'fp_vec/all inst':'a/b'},   'fmt':lambda l: F.round(l, 3)},\n",
    "            'emon_l3 miss':  {'sum_func':self.cores_sum,   'events':{'a':'MEM_LOAD_RETIRED.L3_MISS','b':'INST_RETIRED.ANY'},  'formula':{'l3 miss per 1K inst':'1000*a/b'},   'fmt':lambda l: F.round(l, 3)},\n",
    "            'emon_l3 read miss bw':{'sum_func':self.cores_sum,   'events':{'a':'OCR.READS_TO_CORE.REMOTE_DRAM:ocr_msr_val=0x3fB80007f7','b':'OCR.READS_TO_CORE.LOCAL_DRAM:ocr_msr_val=0x3f840007f7'},  'formula':{'remote miss':'a*64/1000000','local miss':\"b*64/1000000\"},   'fmt':lambda l: F.round(l, 3)},\n",
    "            \n",
    "            \n",
    "            'emon_faststring bw':  {'sum_func':self.cores_sum,   'events':{'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x49033'},  'formula':{'fast store bw':'a*64/1000000'},   'fmt':lambda l: F.round(l, 0)},\n",
    "            'emon_pcie_bw': {\n",
    "                'sum_func':self.pcie_sum,   \n",
    "                'events':{\n",
    "                    'a':'UNC_IIO_DATA_REQ_OF_CPU.MEM_WRITE.PART0',\n",
    "                    'b':'UNC_IIO_DATA_REQ_OF_CPU.MEM_READ.PART0',\n",
    "                    'c':'UNC_IIO_DATA_REQ_BY_CPU.MEM_WRITE.PART0',\n",
    "                    'd':'UNC_IIO_DATA_REQ_BY_CPU.MEM_READ.PART0'\n",
    "                },  \n",
    "                'formula':{\n",
    "                    'inbound write':'a*4/1000000',\n",
    "                    'inbound read':'b*4/1000000',\n",
    "                    'outbound write':'c*4/1000000',\n",
    "                    'outbound read':'d*4/1000000'},   \n",
    "                'fmt':lambda l: F.round(l, 0)\n",
    "            },\n",
    "            'emon_pcie_util':{\n",
    "                'sum_func':self.pcie_sum,   \n",
    "                'events':{\n",
    "                    'a':'UNC_IIO_UTIL_IN.PORT0',\n",
    "                    'b':'UNC_IIO_UTIL_OUT.PORT0',\n",
    "                    'c':'UNC_IIO_CLOCKTICKS'\n",
    "                },  \n",
    "                'formula':{\n",
    "                    'pcie %':'(a+b)/c'},   \n",
    "                'fmt':lambda l: F.round(l, 2)\n",
    "            },\n",
    "            'emon_streaming bw':  {'sum_func':self.cores_sum,   'events':{'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x41833'},  'formula':{'streaming store bw':'a*64/1000000'},   'fmt':lambda l: F.round(l, 0)},\n",
    "                         'metric_CPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CPU_CLK_UNHALTED.THREAD','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_CPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_branch mispredict ratio':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'BR_MISP_RETIRED.ALL_BRANCHES','b':'BR_INST_RETIRED.ALL_BRANCHES'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_branch mispredict ratio':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_loads per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_INST_RETIRED.ALL_LOADS','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_loads per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_stores per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_INST_RETIRED.ALL_STORES','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_stores per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_locks retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_INST_RETIRED.LOCK_LOADS','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_locks retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_uncacheable reads per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40e33','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_uncacheable reads per instr':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_streaming stores (full line) per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x41833','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_streaming stores (full line) per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_streaming stores (partial line) per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x41a33','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_streaming stores (partial line) per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L1D MPI (includes data+rfo w/ prefetches)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'L1D.REPLACEMENT','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L1D MPI (includes data+rfo w/ prefetches)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L1D demand data read hits per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_LOAD_RETIRED.L1_HIT','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L1D demand data read hits per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L1-I code read misses (w/ prefetches) per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'L2_RQSTS.ALL_CODE_RD','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L1-I code read misses (w/ prefetches) per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 demand data read hits per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_LOAD_RETIRED.L2_HIT','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 demand data read hits per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 MPI (includes code+data+rfo w/ prefetches)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'L2_LINES_IN.ALL','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 MPI (includes code+data+rfo w/ prefetches)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 demand data read MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_LOAD_RETIRED.L2_MISS','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 demand data read MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 demand code MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'L2_RQSTS.CODE_RD_MISS','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 demand code MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 Any local request that HITM in a sibling core (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_HIT.HITM_OTHER_CORE','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 Any local request that HITM in a sibling core (per instr)':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 Any local request that HIT in a sibling core and forwarded(per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_HIT.HIT_OTHER_CORE_FWD','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 Any local request that HIT in a sibling core and forwarded(per instr)':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 all L2 prefetches(per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'L2_RQSTS.ALL_PF','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 all L2 prefetches(per instr)':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 % of L2 evictions that are allocated into L3':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'L2_LINES_OUT.NON_SILENT','b':'IDI_MISC.WB_DOWNGRADE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 % of L2 evictions that are allocated into L3':'100*(a-b)/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_L2 % of L2 evictions that are NOT allocated into L3':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'L2_LINES_OUT.NON_SILENT','b':'IDI_MISC.WB_DOWNGRADE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_L2 % of L2 evictions that are NOT allocated into L3':'100*b/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC code references per instr (L3 prefetch excluded)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA:filter1=0x40233','d':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC code references per instr (L3 prefetch excluded)':'a/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC data read references per instr (L3 prefetch excluded)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA:filter1=0x40433','d':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC data read references per instr (L3 prefetch excluded)':'a/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC RFO references per instr (L3 prefetch excluded)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA:filter1=0x40033','d':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC RFO references per instr (L3 prefetch excluded)':'a/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC MPI (includes code+data+rfo w/ prefetches)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12D40433','b':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12CC0233','c':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033','d':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC MPI (includes code+data+rfo w/ prefetches)':'(a+b+c)/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC data read MPI (demand+prefetch)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12D40433','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC data read MPI (demand+prefetch)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC RFO read MPI (demand+prefetch)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC RFO read MPI (demand+prefetch)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC code read MPI (demand+prefetch)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12CC0233','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC code read MPI (demand+prefetch)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC all LLC prefetches (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C4B433','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC all LLC prefetches (per instr)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC total HITM (per instr) (excludes LLC prefetches)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HITM','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC total HITM (per instr) (excludes LLC prefetches)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC total HIT clean line forwards (per instr) (excludes LLC prefetches)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HIT_FORWARD','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC total HIT clean line forwards (per instr) (excludes LLC prefetches)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_LLC % of LLC misses satisfied by remote caches':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12D40433','b':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12CC0233','c':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033','d':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C4B433','e':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HIT_FORWARD','f':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.REMOTE_HITM'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_LLC % of LLC misses satisfied by remote caches':'100*(e+f)/(a+b+c-d)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_SF snoop filter capacity evictions (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_SF_EVICTION.M_STATE','b':'UNC_CHA_SF_EVICTION.S_STATE','c':'UNC_CHA_SF_EVICTION.E_STATE','d':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_SF snoop filter capacity evictions (per instr)':'(a+b+c)/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_SF % of L3 accesses that result in SF capacity evictions':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_SF_EVICTION.M_STATE','b':'UNC_CHA_SF_EVICTION.S_STATE','c':'UNC_CHA_SF_EVICTION.E_STATE','d':'L2_LINES_IN.ALL'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_SF % of L3 accesses that result in SF capacity evictions':'100*(a+b+c)/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_ITLB MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'ITLB_MISSES.WALK_COMPLETED','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_ITLB MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_ITLB large page MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'ITLB_MISSES.WALK_COMPLETED_2M_4M','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_ITLB large page MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DTLB load MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_LOAD_MISSES.WALK_COMPLETED','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DTLB load MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DTLB 4KB page load MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_LOAD_MISSES.WALK_COMPLETED','b':'DTLB_LOAD_MISSES.WALK_COMPLETED_2M_4M','c':'DTLB_LOAD_MISSES.WALK_COMPLETED_1G','d':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DTLB 4KB page load MPI':'(a-b-c)/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DTLB 2MB large page load MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_LOAD_MISSES.WALK_COMPLETED_2M_4M','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DTLB 2MB large page load MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DTLB 1GB large page load MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_LOAD_MISSES.WALK_COMPLETED_1G','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DTLB 1GB large page load MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DTLB store MPI':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_STORE_MISSES.WALK_COMPLETED','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DTLB store MPI':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DTLB load miss latency (in core clks)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_LOAD_MISSES.WALK_ACTIVE','b':'DTLB_LOAD_MISSES.WALK_COMPLETED'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DTLB load miss latency (in core clks)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DTLB store miss latency (in core clks)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_STORE_MISSES.WALK_ACTIVE','b':'DTLB_STORE_MISSES.WALK_COMPLETED'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DTLB store miss latency (in core clks)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_ITLB miss latency (in core clks)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'ITLB_MISSES.WALK_ACTIVE','b':'ITLB_MISSES.WALK_COMPLETED'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_ITLB miss latency (in core clks)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_NUMA %_Reads addressed to local DRAM':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40432','b':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40431'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_NUMA %_Reads addressed to local DRAM':'100*a/(a+b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_NUMA %_Reads addressed to remote DRAM':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40432','b':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40431'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_NUMA %_Reads addressed to remote DRAM':'100*b/(a+b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_NUMA %_RFOs addressed to local DRAM':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40031','b':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_NUMA %_RFOs addressed to local DRAM':'100*(b-a)/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_NUMA %_RFOs addressed to remote DRAM':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40031','b':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x12C40033'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_NUMA %_RFOs addressed to remote DRAM':'100*a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_UPI Data transmit BW (MB/sec) (only data)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_UPI_TxL_FLITS.ALL_DATA'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_UPI Data transmit BW (MB/sec) (only data)':'a*(64/9)/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_UPI % cycles transmit link is half-width (L0p)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_UPI_TxL0P_POWER_CYCLES','b':'UNC_UPI_CLOCKTICKS','f':'UNC_UPI_L1_POWER_CYCLES'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_UPI % cycles transmit link is half-width (L0p)':'100*(a/(b-f))'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_UPI % cycles receive link is half-width (L0p)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_UPI_RxL0P_POWER_CYCLES','b':'UNC_UPI_CLOCKTICKS','f':'UNC_UPI_L1_POWER_CYCLES'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_UPI % cycles receive link is half-width (L0p)':'100*(a/(b-f))'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_HA - Reads vs. all requests':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_REQUESTS.READS','b':'UNC_CHA_REQUESTS.WRITES'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_HA - Reads vs. all requests':'a/(a+b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_HA - Writes vs. all requests':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_REQUESTS.READS','b':'UNC_CHA_REQUESTS.WRITES'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_HA - Writes vs. all requests':'b/(a+b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_HA % of all reads that are local':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_REQUESTS.READS_LOCAL','b':'UNC_CHA_REQUESTS.READS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_HA % of all reads that are local':'100*a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_HA % of all writes that are local':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_REQUESTS.WRITES_LOCAL','b':'UNC_CHA_REQUESTS.WRITES'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_HA % of all writes that are local':'100*a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_HA conflict responses per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_SNOOP_RESP.RSPCNFLCTS','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_HA conflict responses per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_HA directory lookups that spawned a snoop (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_DIR_LOOKUP.SNP','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_HA directory lookups that spawned a snoop (per instr)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_HA directory lookups that did not spawn a snoop (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_DIR_LOOKUP.NO_SNP','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_HA directory lookups that did not spawn a snoop (per instr)':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M2M directory updates (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_DIR_UPDATE.HA','b':'UNC_CHA_DIR_UPDATE.TOR','c':'UNC_M2M_DIRECTORY_UPDATE.ANY','d':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M2M directory updates (per instr)':'(a+b+c)/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M2M XPT prefetches (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M2M_PREFCAM_INSERTS','b':'UNC_M3UPI_UPI_PREFETCH_SPAWN','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M2M XPT prefetches (per instr)':'(a-b)/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M3UPI UPI prefetches (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'b':'UNC_M3UPI_UPI_PREFETCH_SPAWN','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M3UPI UPI prefetches (per instr)':'b/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M2M extra reads from XPT-UPI prefetches (per instr)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M2M_PREFCAM_INSERTS','b':'UNC_M2M_PREFCAM_DEMAND_PROMOTIONS','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M2M extra reads from XPT-UPI prefetches (per instr)':'(a-b)/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory bandwidth read (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CAS_COUNT.RD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory bandwidth read (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory bandwidth write (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CAS_COUNT.WR'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory bandwidth write (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory bandwidth total (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CAS_COUNT.RD','b':'UNC_M_CAS_COUNT.WR'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory bandwidth total (MB/sec)':'(a+b)*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory extra read b/w due to XPT prefetches (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M2M_PREFCAM_INSERTS','b':'UNC_M2M_PREFCAM_DEMAND_PROMOTIONS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory extra read b/w due to XPT prefetches (MB/sec)':'(a-b)*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory extra write b/w due to directory updates (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_DIR_UPDATE.HA','b':'UNC_CHA_DIR_UPDATE.TOR','c':'UNC_M2M_DIRECTORY_UPDATE.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory extra write b/w due to directory updates (MB/sec)':'(a+b+c)*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_2LM % of non-inclusive writes to near memory':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M2M_IMC_WRITES.NI','b':'UNC_M_DDRT_RDQ_INSERTS','c':'UNC_M_CAS_COUNT.WR'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_2LM % of non-inclusive writes to near memory':'100*a/(c-b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_2LM near memory cache read miss rate%':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M2M_TAG_HIT.NM_RD_HIT_CLEAN','b':'UNC_M2M_TAG_HIT.NM_RD_HIT_DIRTY','c':'UNC_M_DDRT_RDQ_INSERTS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_2LM near memory cache read miss rate%':'100*c/(a+b+c)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory avg entries in RPQ':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CLOCKTICKS','c':'UNC_M_RPQ_OCCUPANCY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory avg entries in RPQ':'c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory avg entries in RPQ when not empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'c':'UNC_M_RPQ_OCCUPANCY','a':'UNC_M_RPQ_OCCUPANCY:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory avg entries in RPQ when not empty':'c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory % cycles when RPQ is empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CLOCKTICKS','c':'UNC_M_RPQ_OCCUPANCY:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory % cycles when RPQ is empty':'100*(1-c/a)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory % cycles when RPQ has 1 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CLOCKTICKS','c':'UNC_M_RPQ_OCCUPANCY:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory % cycles when RPQ has 1 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory % cycles when RPQ has 10 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CLOCKTICKS','c':'UNC_M_RPQ_OCCUPANCY:t=10'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory % cycles when RPQ has 10 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory % cycles when RPQ has 20 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CLOCKTICKS','c':'UNC_M_RPQ_OCCUPANCY:t=20'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory % cycles when RPQ has 20 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory % cycles when RPQ has 40 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CLOCKTICKS','c':'UNC_M_RPQ_OCCUPANCY:t=40'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory % cycles when RPQ has 40 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory avg time (dclk) RPQ not empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_RPQ_OCCUPANCY:t=1:e1','c':'UNC_M_RPQ_OCCUPANCY:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory avg time (dclk) RPQ not empty':'c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory avg time (dclk) RPQ empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_RPQ_OCCUPANCY:t=1:e1','c':'UNC_M_RPQ_OCCUPANCY:t=1','d':'UNC_M_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory avg time (dclk) RPQ empty':'(d-c)/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory avg time with 40 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_RPQ_OCCUPANCY:t=40:e1','c':'UNC_M_RPQ_OCCUPANCY:t=40'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory avg time with 40 or more entries':'c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory avg time with less than 40 entries)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_RPQ_OCCUPANCY:t=40:e1','c':'UNC_M_RPQ_OCCUPANCY:t=40','d':'UNC_M_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory avg time with less than 40 entries)':'(d-c)/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP_memory bandwidth read (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_DDRT_RDQ_INSERTS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP_memory bandwidth read (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP_memory bandwidth write (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_DDRT_WPQ_INSERTS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP_memory bandwidth write (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP_memory bandwidth total (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_DDRT_RDQ_INSERTS','b':'UNC_M_DDRT_WPQ_INSERTS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP_memory bandwidth total (MB/sec)':'(a+b)*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP avg entries in RPQ when not empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'b':'UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1','c':'UNC_M_DDRT_RDQ_OCCUPANCY.ALL'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP avg entries in RPQ when not empty':'c/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP % cycles when RPQ is empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CLOCKTICKS','b':'UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP % cycles when RPQ is empty':'100*(1-(b/a))'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP avg time (dclk) RPQ not empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1:e1','b':'UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP avg time (dclk) RPQ not empty':'b/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP avg time (dclk) with 36 or more entries in RPQ':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=36:e1','b':'UNC_M_DDRT_RDQ_OCCUPANCY.ALL:t=36'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP avg time (dclk) with 36 or more entries in RPQ':'b/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP avg entries in WPQ when not empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'b':'UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1','c':'UNC_M_DDRT_WPQ_OCCUPANCY.ALL'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP avg entries in WPQ when not empty':'c/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP avg time (dclk) WPQ not empty':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1:e1','b':'UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP avg time (dclk) WPQ not empty':'b/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_3DXP avg time (dclk) with 30 or more entries in WPQ':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=30:e1','b':'UNC_M_DDRT_WPQ_OCCUPANCY.ALL:t=30'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_3DXP avg time (dclk) with 30 or more entries in WPQ':'b/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_CHA % cyles Fast asserted':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_FAST_ASSERTED.HORZ:u0x1','c':'UNC_CHA_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_CHA % cyles Fast asserted':'100*a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_CHA RxC IRQ avg entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_RxC_OCCUPANCY.IRQ','c':'UNC_CHA_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_CHA RxC IRQ avg entries':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_CHA RxC IRQ % cycles when Q has 18 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_CLOCKTICKS','c':'UNC_CHA_RxC_OCCUPANCY.IRQ:t=18'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_CHA RxC IRQ % cycles when Q has 18 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M2M avg entries in TxC AD Q':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_CLOCKTICKS','c':'UNC_M2M_TxC_AD_OCCUPANCY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M2M avg entries in TxC AD Q':'c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M2M avg entries in TxC BL Q':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_CLOCKTICKS','c':'UNC_M2M_TxC_BL_OCCUPANCY.ALL'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M2M avg entries in TxC BL Q':'c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M2M RxC AD avg entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M2M_RxC_AD_OCCUPANCY','c':'UNC_CHA_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M2M RxC AD avg entries':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M2M RxC BL avg entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M2M_RxC_BL_OCCUPANCY','c':'UNC_CHA_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M2M RxC BL avg entries':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_IO_bandwidth_disk_or_network_writes (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART0','b':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART1','c':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART2','d':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_READ.PART3'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_IO_bandwidth_disk_or_network_writes (MB/sec)':'(a+b+c+d)*4/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_IO_bandwidth_disk_or_network_reads (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART0','b':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART1','c':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART2','d':'UNC_IIO_PAYLOAD_BYTES_IN.MEM_WRITE.PART3'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_IO_bandwidth_disk_or_network_reads (MB/sec)':'(a+b+c+d)*4/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_IO_number of partial PCI writes per sec':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IO_HIT:filter1=0x40033','b':'UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x40033'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_IO_number of partial PCI writes per sec':'a+b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_IO_read cache miss(disk/network writes) bandwidth (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x43c33'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_IO_read cache miss(disk/network writes) bandwidth (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_IO_write cache miss(disk/network reads) bandwidth (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x49033','b':'UNC_CHA_TOR_INSERTS.IO_MISS:filter1=0x40033'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_IO_write cache miss(disk/network reads) bandwidth (MB/sec)':'(a+b)*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_IONUMA % disk/network reads addressed to local memory':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IO:filter1=0x49031','b':'UNC_CHA_TOR_INSERTS.IO:filter1=0x49032','c':'UNC_CHA_TOR_INSERTS.IO:filter1=0x40031','d':'UNC_CHA_TOR_INSERTS.IO:filter1=0x40032'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_IONUMA % disk/network reads addressed to local memory':'100*(b+d)/(a+b+c+d)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_IONUMA % disk/network reads addressed to remote memory':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IO:filter1=0x49031','b':'UNC_CHA_TOR_INSERTS.IO:filter1=0x49032','c':'UNC_CHA_TOR_INSERTS.IO:filter1=0x40031','d':'UNC_CHA_TOR_INSERTS.IO:filter1=0x40032'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_IONUMA % disk/network reads addressed to remote memory':'100*(a+c)/(a+b+c+d)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_MMIO reads per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40040e33','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_MMIO reads per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_MMIO writes per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x40041e33','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_MMIO writes per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory reads vs. all requests':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_CAS_COUNT.RD','b':'UNC_M_CAS_COUNT.WR'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory reads vs. all requests':'a/(a+b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory Page Empty vs. all requests':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_PRE_COUNT.RD:u0xc','c':'UNC_M_PRE_COUNT.PAGE_MISS','d':'UNC_M_CAS_COUNT.RD','e':'UNC_M_CAS_COUNT.WR'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory Page Empty vs. all requests':'(a-c)/(d+e)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory Page Misses vs. all requests':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'b':'UNC_M_PRE_COUNT.PAGE_MISS','c':'UNC_M_CAS_COUNT.RD','d':'UNC_M_CAS_COUNT.WR'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory Page Misses vs. all requests':'b/(c+d)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory Page Hits vs. all requests':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_PRE_COUNT.RD:u0xc','c':'UNC_M_CAS_COUNT.RD','d':'UNC_M_CAS_COUNT.WR'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory Page Hits vs. all requests':'1-(a/(c+d))'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory % Cycles where all DRAM ranks are in PPD mode':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_POWER_CHANNEL_PPD','b':'UNC_M_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory % Cycles where all DRAM ranks are in PPD mode':'100*a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_memory % Cycles Memory is in self refresh power mode':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M_POWER_SELF_REFRESH','b':'UNC_M_CLOCKTICKS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_memory % Cycles Memory is in self refresh power mode':'100*a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_ItoM operations (fast strings) that reference LLC per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_HIT:filter1=0x49033','b':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x49033','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_ItoM operations (fast strings) that reference LLC per instr':'(a+b)/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_ItoM operations (fast strings) that miss LLC per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_CHA_TOR_INSERTS.IA_MISS:filter1=0x49033','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_ItoM operations (fast strings) that miss LLC per instr':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_% Uops delivered from decoded Icache (DSB)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'IDQ.DSB_UOPS','b':'UOPS_ISSUED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_% Uops delivered from decoded Icache (DSB)':'100*(a/b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_% Uops delivered from legacy decode pipeline (MITE)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'IDQ.MITE_UOPS','b':'UOPS_ISSUED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_% Uops delivered from legacy decode pipeline (MITE)':'100*(a/b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_% Uops delivered from microcode sequencer (MS)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'IDQ.MS_UOPS','b':'UOPS_ISSUED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_% Uops delivered from microcode sequencer (MS)':'100*(a/b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_% Uops delivered from loop stream detector (LSD)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'LSD.UOPS','b':'UOPS_ISSUED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_% Uops delivered from loop stream detector (LSD)':'100*(a/b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP scalar single-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.SCALAR_SINGLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP scalar single-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP scalar double-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.SCALAR_DOUBLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP scalar double-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP 128-bit packed single-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.128B_PACKED_SINGLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP 128-bit packed single-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP 128-bit packed double-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.128B_PACKED_DOUBLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP 128-bit packed double-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP 256-bit packed single-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.256B_PACKED_SINGLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP 256-bit packed single-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP 256-bit packed double-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.256B_PACKED_DOUBLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP 256-bit packed double-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP 512-bit packed single-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.512B_PACKED_SINGLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP 512-bit packed single-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_FP 512-bit packed double-precision FP instructions retired per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'FP_ARITH_INST_RETIRED.512B_PACKED_DOUBLE','b':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_FP 512-bit packed double-precision FP instructions retired per instr':'a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_DRAM power (watts)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MSR_EVENT:msr=0x619:type=FREERUN:scope=PACKAGE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_DRAM power (watts)':'a*15.3/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_package power (watts)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MSR_EVENT:msr=0x611:type=FREERUN:scope=PACKAGE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_package power (watts)':'a*61/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core c6 residency %':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MSR_EVENT:msr=0x3FD:type=FREERUN:scope=THREAD','b':'TSC'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core c6 residency %':'100*a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core SW prefetch NTA per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'SW_PREFETCH_ACCESS.NTA','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core SW prefetch NTA per instr':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core uncacheable access and clflushes per instr':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_REQUESTS.MEM_UC','c':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core uncacheable access and clflushes per instr':'a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core % cycles core power throttled':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CORE_POWER.THROTTLE','c':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core % cycles core power throttled':'100*a/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core % cycles in non AVX license':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CORE_POWER.LVL0_TURBO_LICENSE','b':'CORE_POWER.LVL1_TURBO_LICENSE','c':'CORE_POWER.LVL2_TURBO_LICENSE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core % cycles in non AVX license':'100*a/(a+b+c)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core % cycles in AVX2 license':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CORE_POWER.LVL0_TURBO_LICENSE','b':'CORE_POWER.LVL1_TURBO_LICENSE','c':'CORE_POWER.LVL2_TURBO_LICENSE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core % cycles in AVX2 license':'100*b/(a+b+c)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core % cycles in AVX-512 license':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CORE_POWER.LVL0_TURBO_LICENSE','b':'CORE_POWER.LVL1_TURBO_LICENSE','c':'CORE_POWER.LVL2_TURBO_LICENSE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core % cycles in AVX-512 license':'100*c/(a+b+c)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_SMI number of SMIs per sec':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MSR_EVENT:msr=0x34:type=FREERUN:scope=PACKAGE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_SMI number of SMIs per sec':'a+0'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core initiated local dram read bandwidth (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3f840007f7'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core initiated local dram read bandwidth (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core initiated remote dram read bandwidth (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3fB80007f7'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core initiated remote dram read bandwidth (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core initiated local DCPMEM read bandwidth (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3f804007f7'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core initiated local DCPMEM read bandwidth (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_core initiated remote DCPMEM read bandwidth (MB/sec)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_RESPONSE:request=ALL_READS:response=L3_MISS.ANY_SNOOP:ocr_msr_val=0x3f838007f7'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_core initiated remote DCPMEM read bandwidth (MB/sec)':'a*64/1000000'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M3UPI avg all VN0 entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M3UPI_CLOCKTICKS','c':'UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M3UPI avg all VN0 entries':'c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M3UPI % cycles when all VN0 has 1 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M3UPI_CLOCKTICKS','c':'UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=1'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M3UPI % cycles when all VN0 has 1 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M3UPI % cycles when all VN0 has 10 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M3UPI_CLOCKTICKS','c':'UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=10'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M3UPI % cycles when all VN0 has 10 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M3UPI % cycles when all VN0 has 30 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M3UPI_CLOCKTICKS','c':'UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=30'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M3UPI % cycles when all VN0 has 30 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_M3UPI % cycles when all VN0 has 50 or more entries':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'UNC_M3UPI_CLOCKTICKS','c':'UNC_M3UPI_RxC_OCCUPANCY_VN0.BL_RSP:u0x7f:t=50'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_M3UPI % cycles when all VN0 has 50 or more entries':'100*c/a'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....ICache_Misses(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'ICACHE_16B.IFDATA_STALL','b':'ICACHE_16B.IFDATA_STALL:c1:e1','d':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....ICache_Misses(%)':'100*(a+2*b)/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....ITLB_Misses(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'ICACHE_64B.IFTAG_STALL','d':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....ITLB_Misses(%)':'100*a/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....Branch_Resteers(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'INT_MISC.CLEAR_RESTEER_CYCLES','b':'BACLEARS.ANY','j':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....Branch_Resteers(%)':'100*(a+9*b)/j'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......Mispredicts_Resteers(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'INT_MISC.CLEAR_RESTEER_CYCLES','b':'MACHINE_CLEARS.COUNT','c':'BR_MISP_RETIRED.ALL_BRANCHES','j':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......Mispredicts_Resteers(%)':'100*a*(c/(b+c))/j'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......Clears_Resteers(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'INT_MISC.CLEAR_RESTEER_CYCLES','b':'MACHINE_CLEARS.COUNT','c':'BR_MISP_RETIRED.ALL_BRANCHES','j':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......Clears_Resteers(%)':'100*a*(1-(c/(b+c)))/j'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......Unknown_Branches_Resteers(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'b':'BACLEARS.ANY','j':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......Unknown_Branches_Resteers(%)':'100*(9*b)/j'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....DSB_Switches(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DSB2MITE_SWITCHES.PENALTY_CYCLES','d':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....DSB_Switches(%)':'100*a/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....MS_Switches(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'IDQ.MS_SWITCHES','d':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....MS_Switches(%)':'100*2*a/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....L1_Bound(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CYCLE_ACTIVITY.STALLS_MEM_ANY','b':'CYCLE_ACTIVITY.STALLS_L1D_MISS','c':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....L1_Bound(%)':'100*(a-b)/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......DTLB_Load(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'DTLB_LOAD_MISSES.STLB_HIT','b':'DTLB_LOAD_MISSES.WALK_ACTIVE','c':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......DTLB_Load(%)':'100*(7*a+b)/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......Store_Fwd_Blk(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'LD_BLOCKS.STORE_FORWARD','b':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......Store_Fwd_Blk(%)':'100*((13 * a) / b) '\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......Lock_Latency(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_INST_RETIRED.LOCK_LOADS','b':'CPU_CLK_UNHALTED.THREAD','c':'MEM_INST_RETIRED.ALL_STORES','d':'OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_DEMAND_RFO'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......Lock_Latency(%)':'100*(((a / c)* (case when b > d then d else b end))/ b) '\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....L2_Bound(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CYCLE_ACTIVITY.STALLS_L1D_MISS','b':'CYCLE_ACTIVITY.STALLS_L2_MISS','c':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....L2_Bound(%)':'100*(a-b)/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....L3_Bound(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CYCLE_ACTIVITY.STALLS_L2_MISS','c':'CYCLE_ACTIVITY.STALLS_L3_MISS','d':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....L3_Bound(%)':'100*(a-c)/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......Contested_Accesses(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_LOAD_L3_HIT_RETIRED.XSNP_HITM','b':'CPU_CLK_UNHALTED.THREAD','c':'MEM_LOAD_L3_HIT_RETIRED.XSNP_MISS'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......Contested_Accesses(%)':'100*(60 * ( a + c ) / b) '\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......Data_Sharing(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'MEM_LOAD_L3_HIT_RETIRED.XSNP_HIT','b':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......Data_Sharing(%)':'100*(43 * a /b) '\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......L3_Latency(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'b':'OFFCORE_REQUESTS_OUTSTANDING.DEMAND_DATA_RD_GE_6','c':'OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6','d':'CPU_CLK_UNHALTED.THREAD','e':'OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_DEMAND_DATA_RD','f':'OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_L3_MISS_DEMAND_DATA_RD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......L3_Latency(%)':'100*((((case when e > d then d else e end)-(case when f > d then d else f end))/d)-(((case when b > d then d else b end)-(case when d > d then d else c end))/d))'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......L3_Bandwidth(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'b':'OFFCORE_REQUESTS_OUTSTANDING.DEMAND_DATA_RD_GE_6','c':'OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6','d':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......L3_Bandwidth(%)':'100*((case when b > d then d else b end)-(case when c > d then d else c end))/d'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....MEM_Bound(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'CYCLE_ACTIVITY.STALLS_L3_MISS','d':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....MEM_Bound(%)':'100*(a/d)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......MEM_Bandwidth(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6','b':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......MEM_Bandwidth(%)':'100*((case when a > b then b else a end))/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_......MEM_Latency(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'OFFCORE_REQUESTS_OUTSTANDING.L3_MISS_DEMAND_DATA_RD_GE_6','b':'OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_L3_MISS_DEMAND_DATA_RD','c':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_......MEM_Latency(%)':'100*(((case when b > c then c else b end))-((case when a > c then c else a end)))/c'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....Stores_Bound(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'EXE_ACTIVITY.BOUND_ON_STORES','b':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....Stores_Bound(%)':'100*(a/b)'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....Divider(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'ARITH.DIVIDER_ACTIVE','b':'CPU_CLK_UNHALTED.THREAD'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....Divider(%)':'100*a/b'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....Ports_Utilization(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'd':'EXE_ACTIVITY.EXE_BOUND_0_PORTS','f':'EXE_ACTIVITY.1_PORTS_UTIL','g':'CYCLE_ACTIVITY.STALLS_MEM_ANY','h':'EXE_ACTIVITY.BOUND_ON_STORES','j':'EXE_ACTIVITY.2_PORTS_UTIL','p':'CPU_CLK_UNHALTED.THREAD','q':'INST_RETIRED.ANY'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....Ports_Utilization(%)':'100*((d+f+(case when ((q/p)>1.8) then j else 0 end)+g+h)-g-h)/p'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....FP_Arith(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'INST_RETIRED.ANY','b':'UOPS_EXECUTED.X87','c':'UOPS_RETIRED.RETIRE_SLOTS','d':'FP_ARITH_INST_RETIRED.SCALAR_SINGLE','e':'FP_ARITH_INST_RETIRED.SCALAR_DOUBLE','f':'FP_ARITH_INST_RETIRED.128B_PACKED_DOUBLE','g':'FP_ARITH_INST_RETIRED.128B_PACKED_SINGLE','h':'FP_ARITH_INST_RETIRED.256B_PACKED_SINGLE','j':'FP_ARITH_INST_RETIRED.256B_PACKED_DOUBLE','k':'UOPS_EXECUTED.THREAD','m':'FP_ARITH_INST_RETIRED.512B_PACKED_SINGLE','n':'FP_ARITH_INST_RETIRED.512B_PACKED_DOUBLE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....FP_Arith(%)':'100*((b/k)+((d+e+f+g+h+j+m+n)/c))'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            },\n",
    "             'metric_TMAM_....Other(%)':{\n",
    "                                'sum_func':self.cores_sum,   \n",
    "                                'events':{\n",
    "                                      'a':'INST_RETIRED.ANY','b':'UOPS_EXECUTED.X87','c':'UOPS_RETIRED.RETIRE_SLOTS','d':'FP_ARITH_INST_RETIRED.SCALAR_SINGLE','e':'FP_ARITH_INST_RETIRED.SCALAR_DOUBLE','f':'FP_ARITH_INST_RETIRED.128B_PACKED_DOUBLE','g':'FP_ARITH_INST_RETIRED.128B_PACKED_SINGLE','h':'FP_ARITH_INST_RETIRED.256B_PACKED_SINGLE','j':'FP_ARITH_INST_RETIRED.256B_PACKED_DOUBLE','k':'UOPS_EXECUTED.THREAD','m':'FP_ARITH_INST_RETIRED.512B_PACKED_SINGLE','n':'FP_ARITH_INST_RETIRED.512B_PACKED_DOUBLE'\n",
    "                                },\n",
    "                                'formula':{\n",
    "                                       'metric_TMAM_....Other(%)':'100*(1-((b/k)+((d+e+f+g+h+j+m+n)/c)))'\n",
    "                                },\n",
    "                                'fmt':lambda l: F.round(l, 3)\n",
    "                            }\n",
    "        })\n",
    "        self.effective_metric=None\n",
    "        self.appclients=[] # there is no appid and client column\n",
    "\n",
    "    def count_sum(self,collected_cores):\n",
    "        return F.expr('+'.join(['_{:d}/_2*{:d}'.format(c+3,self.tsc) for c in collected_cores]))\n",
    "\n",
    "    def cores_sum(self,collected_cores):\n",
    "        return self.count_sum(collected_cores)\n",
    "\n",
    "    def mem_sum(self,collected_cores):\n",
    "        return self.count_sum(list(range(40)))\n",
    "\n",
    "    def pcie_sum(self,collected_cores):\n",
    "        return self.count_sum([2,3,7,8])\n",
    "        \n",
    "    def list_metric(self):\n",
    "        if self.effective_metric is None:\n",
    "            self.get_effective_metric()\n",
    "        for k in self.effective_metric:\n",
    "            m=self.emon_metrics[k]\n",
    "            print(k)\n",
    "            for fk,fm in m['formula'].items():\n",
    "                print(\"    \",fk)\n",
    "            \n",
    "    def load_data(self):\n",
    "        paths=os.path.split(self.file)\n",
    "        if fs.exists(paths[0]+\"/emon.parquet/_SUCCESS\"):\n",
    "            self.df=spark.read.parquet(paths[0]+\"/emon.parquet\")\n",
    "            self.df.cache()\n",
    "            return\n",
    "        \n",
    "        emondata=sc.textFile(self.file)\n",
    "        emondf=emondata.mapPartitions(splits_fill0).toDF()\n",
    "        emondf=emondf.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "        giddf=emondf.where(emondf._1.rlike(\"======\")).selectExpr(\"id as g_id\")\n",
    "        \n",
    "        iddf=emondf.where(emondf._1.rlike(\"\\d\\d/\")).selectExpr(\"_1 as r_1\",\"_2 as r_2\",\"id as r_id\")\n",
    "        jfid=emondf.where(emondf._1.rlike(\"[A-Z]{3}\")).join(iddf,on=[emondf.id>iddf.r_id]).groupBy('id').agg(F.max('r_id').alias('r_id'))\n",
    "        iddf=iddf.join(jfid,on='r_id',how='left')\n",
    "        emondf=emondf.where(emondf._1.rlike(\"[A-Z]{3}\")).join(iddf,on='id',how='left')\n",
    "        \n",
    "        jfid=emondf.join(giddf,on=[emondf.id>giddf.g_id]).groupBy('id').agg(F.max('g_id').alias('g_id'))\n",
    "        giddf=giddf.join(jfid,on='g_id',how='left')\n",
    "        emondf=emondf.join(giddf,on='id',how='inner')\n",
    "        \n",
    "        df=emondf\n",
    "        for i in (range(2,256)):\n",
    "            df=df.withColumn(f\"_{i}\",F.col(f\"_{i}\").astype(LongType()))\n",
    "        df=df.withColumn(\"timestamp\",F.unix_timestamp(F.concat_ws(' ','r_1','r_2'),'MM/dd/yyyy HH:mm:ss')*F.lit(1000)+(F.split(F.col('r_2'),'\\.')[1]).astype(IntegerType()))\n",
    "        df=df.drop(\"r_1\")\n",
    "        df=df.drop(\"r_2\")\n",
    "        cores=list(range(0,self.totalcores))\n",
    "        df=df.withColumn('sum',\n",
    "                         F.when(F.col(\"_1\").startswith(\"UNC_IIO\"),self.pcie_sum(cores))\n",
    "                         .otherwise(self.cores_sum(cores)))\n",
    "        if self.begin_clk>0 and self.end_clk>0:\n",
    "            df=df.withColumn('valid',((F.col(\"timestamp\")>F.lit(self.begin_clk)) & (F.col(\"timestamp\")<F.lit(self.end_clk))))\n",
    "        else:\n",
    "            df=df.withColumn('valid',F.lit(True))\n",
    "        \n",
    "        df.repartition(3).write.mode(\"overwrite\").parquet(paths[0]+\"/emon.parquet\")\n",
    "        self.df=df\n",
    "        df.cache()\n",
    "        \n",
    "    def get_effective_metric(self):\n",
    "        if self.df==None:\n",
    "            self.load_emon()\n",
    "\n",
    "        emondf=self.df\n",
    "        gid=emondf.agg(F.min('g_id')).collect()[0]['min(g_id)']\n",
    "        emondf=emondf.where(F.col(\"g_id\")==gid)\n",
    "        emondf=emondf.cache()\n",
    "\n",
    "        effective_metric=[]\n",
    "\n",
    "        progress = IntProgress(layout=Layout(width='80%', height='40px'))\n",
    "        progress.max = len(self.emon_metrics)\n",
    "        progress.description = 'Calculate Effective Metrics'\n",
    "        display(progress)\n",
    "        progress.value=0\n",
    "\n",
    "        for k,m in self.emon_metrics.items():\n",
    "            join_df=None\n",
    "            progress.value=progress.value+1\n",
    "            for alias,event in m['events'].items():\n",
    "                if join_df is None:\n",
    "                    join_df=emondf.where(\"_1='{:s}'\".format(event)).select('r_id','g_id')\n",
    "                else:\n",
    "                    tdf=emondf.where(\"_1='{:s}'\".format(event)).select('r_id','g_id')\n",
    "                    join_dft=join_df.join(tdf.drop('g_id'),on='r_id',how='inner')\n",
    "                    if join_dft.count()==0:\n",
    "                        join_df=join_df.join(tdf.drop('r_id'),on='g_id',how='inner')\n",
    "                    else:\n",
    "                        join_df=join_dft\n",
    "            if join_df.count()>0:\n",
    "                effective_metric.append(k)\n",
    "        progress.value=progress.value+1\n",
    "        self.effective_metric=effective_metric\n",
    "        emondf.unpersist()\n",
    "    \n",
    "    def gen_metric(self,emondf, m):\n",
    "        join_df=None\n",
    "        for alias,event in m['events'].items():\n",
    "            if join_df is None:\n",
    "                join_df=emondf.where(\"_1='{:s}'\".format(event)).select('timestamp','_1','_2','r_id','g_id',*self.appclients,F.col('sum').alias(alias))\n",
    "            else:\n",
    "                tdf=emondf.where(\"_1='{:s}'\".format(event)).select('_1','_2','r_id','g_id',*self.appclients,F.col('sum').alias(alias))\n",
    "                join_dft=join_df.join(tdf.drop('g_id'),on=['r_id',*self.appclients],how='inner')\n",
    "                if join_dft.count()==0:\n",
    "                    join_df=join_df.join(tdf.drop('r_id'),on=['g_id',*self.appclients],how='inner')\n",
    "                else:\n",
    "                    join_df=join_dft\n",
    "        return join_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        trace_events=Analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        cores=list(range(0,self.totalcores))\n",
    "        \n",
    "        emondf=self.df\n",
    "        if 'collected_cores' in kwargs:\n",
    "            cores=kwargs.get(\"collected_cores\",None)\n",
    "            emondf=emondf.withColumn('sum',\n",
    "                     F.when(F.col(\"_1\").startswith(\"UNC_IIO\"),self.pcie_sum(cores))\n",
    "                     .otherwise(self.cores_sum(cores)))\n",
    "        show_metric= kwargs.get('show_metric', None)\n",
    "            \n",
    "        if show_metric is None and self.effective_metric is None:\n",
    "            self.get_effective_metric()\n",
    "\n",
    "        self.effective_metric=show_metric if show_metric is not None else self.effective_metric\n",
    "        \n",
    "        emondf=self.df\n",
    "        \n",
    "        tid=0\n",
    "        for k in self.effective_metric:\n",
    "            m=self.emon_metrics[k]\n",
    "            join_df=self.gen_metric(emondf,m)\n",
    "            rstdf=join_df.select(\n",
    "                            F.lit(tid).alias('tid'),\n",
    "                            F.lit(id).alias('pid'),\n",
    "                            F.lit('C').alias('ph'),\n",
    "                            F.lit(k).alias('name'),\n",
    "                            (F.col('timestamp')-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                            F.struct(*[m['fmt'](F.expr(formula)).alias(col_name) for col_name,formula in m['formula'].items() ]).alias('args')\n",
    "            ).where(F.col(\"ts\").isNotNull()).orderBy('ts')\n",
    "            trace_events.extend(rstdf.toJSON().collect())\n",
    "            print(trace_events[-1],\"pid is \", id)\n",
    "            trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":tid,\"args\":{\"sort_index \":tid}}))\n",
    "            tid=tid+1\n",
    "            \n",
    "#        if self.begin_clk>0:\n",
    "#            trace_events.append(json.dumps({\"name\":\"begin\",\n",
    "#                         \"ph\":\"i\",\n",
    "#                         \"ts\":self.begin_clk-self.starttime,\n",
    "#                         \"pid\":id,\n",
    "#                        \"tid\":tid,\n",
    "#                         \"s\":\"p\"\n",
    "#                        }))\n",
    " #       if self.end_clk>0:\n",
    " #           trace_events.append(json.dumps({\"name\":\"end\",\n",
    " #                        \"ph\":\"i\",\n",
    " #                        \"ts\":self.end_clk-self.starttime,\n",
    " #                        \"pid\":id,\n",
    " #                        \"tid\":tid,\n",
    " #                        \"s\":\"p\"\n",
    " #                       }))            \n",
    "\n",
    "        return trace_events\n",
    "    \n",
    "    def show_emon_metric(self,metric,sub_metric,core,draw=True,metric_define=None, **kwargs):\n",
    "        if self.df==None:\n",
    "            self.load_data()\n",
    "        emondf=self.df\n",
    "        \n",
    "        showalltime=kwargs.get(\"showalltime\",True)\n",
    "        \n",
    "        if not showalltime:\n",
    "            emondf=emondf.filter(F.col(\"valid\")==F.lit(True))\n",
    "        \n",
    "        if metric is None or metric=='':\n",
    "            for k in self.effective_metric:\n",
    "                m=self.emon_metrics[k]\n",
    "                if sub_metric in m['formula']:\n",
    "                    break\n",
    "            else:\n",
    "                print(\"can't find metric\",sub_metric)\n",
    "                return        \n",
    "        else:\n",
    "            k=metric\n",
    "        if metric_define is None:\n",
    "            m= self.emon_metrics[k]\n",
    "        else:\n",
    "            m= metric_define[k]\n",
    "\n",
    "        if type(core)==int:\n",
    "            core=[core,]\n",
    "        emondf=emondf.withColumn('sum',\n",
    "                 F.when(F.col(\"_1\").startswith(\"UNC_IIO\"),self.pcie_sum(core))\n",
    "                 .otherwise(self.count_sum(core)))\n",
    "            \n",
    "        join_df=self.gen_metric(emondf,m)\n",
    "        \n",
    "        rstdf=join_df.select(\n",
    "                    F.col('timestamp').alias('ts'),\n",
    "                    m['fmt'](F.expr(m['formula'][sub_metric])).alias(sub_metric),\n",
    "                    'r_id'\n",
    "        ).where(F.col(\"timestamp\").isNotNull()).orderBy('timestamp')\n",
    "        \n",
    "        metric_sum=rstdf.select(sub_metric).summary().toPandas()\n",
    "        display(metric_sum)\n",
    "        \n",
    "        if draw:\n",
    "            pddf=rstdf.toPandas()\n",
    "            pddf['ts']=(pddf['ts']-pddf.loc[0,'ts'])/1000\n",
    "            fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 5]})\n",
    "            plt.subplots_adjust(wspace=0.01)\n",
    "            sns.violinplot(y=sub_metric, data=pddf, ax=axs[0],palette=['g'])\n",
    "            axs[0].yaxis.grid(True, which='major')\n",
    "            ax=axs[1]\n",
    "            ax.stackplot(pddf['ts'], pddf[sub_metric],colors=['bisque'])\n",
    "            #ymin, ymax = ax.get_ylim()\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.set_ylim(ax.get_ylim())\n",
    "            ax2.axhline(y=float(metric_sum.loc[4,sub_metric]), linewidth=2, color='r')\n",
    "            ax2.axhline(y=float(metric_sum.loc[5,sub_metric]), linewidth=2, color='r')\n",
    "            ax2.axhline(y=float(metric_sum.loc[6,sub_metric]), linewidth=2, color='r')\n",
    "            ax2.axhline(y=float(metric_sum.loc[7,sub_metric]), linewidth=2, color='r')\n",
    "            ax.set_xlabel('time (s)')\n",
    "            ax.yaxis.grid(True, which='major')\n",
    "            plt.show()\n",
    "            \n",
    "            hist_elapsedtime=rstdf.select('`{:s}`'.format(sub_metric)).rdd.flatMap(lambda x: x).histogram(15)\n",
    "            fig, axs = plt.subplots(figsize=(30, 5))\n",
    "            ax=axs\n",
    "            binSides, binCounts = hist_elapsedtime\n",
    "            binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "            N = len(binCounts)\n",
    "            ind = numpy.arange(N)\n",
    "            width = 0.5\n",
    "\n",
    "            rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "            ax.set_ylabel('Frequencies')\n",
    "            ax.set_title(sub_metric)\n",
    "            ax.set_xticks(numpy.arange(N+1))\n",
    "            ax.set_xticklabels(binSides)\n",
    "        return rstdf\n",
    "        \n",
    "\n",
    "    def gen_reduce_metric(self,metric,core,sub_metric,agg_func):\n",
    "        if self.df==None:\n",
    "            self.load_data()\n",
    "        emondf=self.df\n",
    "        \n",
    "        emondf=emondf.where(F.col(\"valid\")==F.lit(True))\n",
    "        \n",
    "        k=metric\n",
    "        m= self.emon_metrics[k]\n",
    "\n",
    "        if type(core)==int:\n",
    "            core=[core,]\n",
    "        \n",
    "        if len(core)<self.totalcores:\n",
    "            emondf=emondf.withColumn('sum',\n",
    "                     F.when(F.col(\"_1\").startswith(\"UNC_IIO\"),self.pcie_sum(core))\n",
    "                     .otherwise(self.count_sum(core)))\n",
    "\n",
    "        join_df=self.gen_metric(emondf,m)\n",
    "        \n",
    "        rstdf=join_df.select(\n",
    "                *self.appclients,\n",
    "                m['fmt'](F.expr(m['formula'][sub_metric])).alias(sub_metric)\n",
    "        ).where(F.col(\"timestamp\").isNotNull())\n",
    "        return rstdf\n",
    "    \n",
    "    def get_reduce_metric(self,metric,core,sub_metric,agg_func):\n",
    "        rstdf=self.gen_reduce_metric(metric,core,sub_metric,agg_func)\n",
    "        return rstdf.agg(*[l(\"`{:s}`\".format(sub_metric)).alias(get_alias_name(sub_metric,l)) for l in agg_func]).toPandas()\n",
    "   \n",
    "    def get_reduce_metrics(self,core=None,agg_func=[F.max,F.mean,F.min,F.sum]):\n",
    "        coldf=None\n",
    "        if self.effective_metric is None:\n",
    "            self.get_effective_metric()\n",
    "\n",
    "        if core is None:\n",
    "            core=list(range(0,self.totalcores))\n",
    "        progress = IntProgress(layout=Layout(width='80%', height='40px'))\n",
    "        progress.max = len(self.effective_metric)\n",
    "        progress.description = 'Calculate Effective Metrics'\n",
    "        display(progress)\n",
    "        progress.value=0\n",
    "        \n",
    "        columns=[f.__name__ for f in agg_func]\n",
    "            \n",
    "        for k in self.effective_metric:\n",
    "            progress.value=progress.value+1\n",
    "            m=self.emon_metrics[k]\n",
    "            for fk,fm in m['formula'].items():\n",
    "                df=self.get_reduce_metric(k,core,fk,agg_func)\n",
    "                df.columns=columns\n",
    "                df.index=[fk]\n",
    "                if coldf is None:\n",
    "                    coldf=df\n",
    "                else:\n",
    "                    coldf=coldf.append(df)\n",
    "        progress.value=progress.value+1\n",
    "        return coldf\n",
    "    \n",
    "\n",
    "class Emon_Analysis_All(Emon_Analysis):\n",
    "    def __init__(self,emon_files):\n",
    "        Emon_Analysis.__init__(self,emon_files[0])\n",
    "        self.emon_files=emon_files\n",
    "        self.appclients=['appid','client']\n",
    "        \n",
    "    def load_data(self):\n",
    "        spark.clearCache()\n",
    "        emondf=spark.read.format(\"parquet\").load(self.emon_files)\n",
    "        emondf=emondf.withColumn(\"file\",F.input_file_name())\n",
    "        filepath=emondf.select(F.col(\"file\")).limit(1).collect()[0]['file']\n",
    "        length=len(filepath.split(\"/\"))\n",
    "        emondf=emondf.withColumn(\"appid\",F.split(\"file\",\"/\")[length-4])\n",
    "        emondf=emondf.withColumn(\"client\",F.split(\"file\",\"/\")[length-3]).drop(\"file\")\n",
    "        emondf=emondf.cache()\n",
    "        self.df=emondf\n",
    "        \n",
    "    def get_reduce_metric(self,metric,core=None,sub_metric=None,agg_func=[F.max,F.mean,F.min,F.sum]):\n",
    "        \n",
    "        if core is None:\n",
    "            core=list(range(0,self.totalcores))\n",
    "        if sub_metric is None:\n",
    "            m=self.emon_metrics[metric]\n",
    "            sub_metric = list(m['formula'].keys())[0]\n",
    "        \n",
    "        rstdf=self.gen_reduce_metric(metric,core,sub_metric,agg_func)\n",
    "        return rstdf.groupBy(\"appid\").agg(*[l(\"`{:s}`\".format(sub_metric)).alias(get_alias_name(sub_metric,l)) for l in agg_func]).toPandas()\n",
    "    \n",
    "    def get_reduce_metrics(self,core=None,agg_func=[F.max,F.mean,F.min,F.sum]):\n",
    "        return None\n",
    "    \n",
    "    def generate_trace_view_list(self, id , **kwargs):\n",
    "        Analysis.generate_trace_view_list(self,0)\n",
    "        \n",
    "        cores=list(range(0,self.totalcores))\n",
    "        \n",
    "        pidmap=kwargs.get(\"pidmap\",None)\n",
    "        if pidmap is None:\n",
    "            print(\"multiple emon process needs pidmap in {'client':pid,} format\")\n",
    "            return []\n",
    "        else:\n",
    "            display(pidmap)\n",
    "            \n",
    "        emondf=self.df\n",
    "        if 'collected_cores' in kwargs:\n",
    "            cores=kwargs.get(\"collected_cores\",None)\n",
    "            emondf=emondf.withColumn('sum',\n",
    "                     F.when(F.col(\"_1\").startswith(\"UNC_IIO\"),self.pcie_sum(cores))\n",
    "                     .otherwise(self.cores_sum(cores)))\n",
    "        show_metric= kwargs.get('show_metric', None)\n",
    "            \n",
    "        if show_metric is None and self.effective_metric is None:\n",
    "            self.get_effective_metric()\n",
    "\n",
    "        self.effective_metric=show_metric if show_metric is not None else self.effective_metric\n",
    "        \n",
    "        mapexpr=F.create_map([F.lit(x) for x in chain(*pidmap.items())])\n",
    "        \n",
    "        trace_events=[]\n",
    "        for c,id in pidmap.items():\n",
    "            trace_events.append(json.dumps({\"name\": \"process_name\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"name\":\" \"+c}}))\n",
    "        tid=0\n",
    "        for k in self.effective_metric:\n",
    "            m=self.emon_metrics[k]\n",
    "            join_df=self.gen_metric(emondf,m)\n",
    "            join_df=join_df.withColumn('pid',mapexpr.getItem(F.col(\"client\")))\n",
    "            rstdf=join_df.select(\n",
    "                            F.lit(tid).alias('tid'),\n",
    "                            F.col('pid').alias('pid'),\n",
    "                            F.lit('C').alias('ph'),\n",
    "                            F.lit(k).alias('name'),\n",
    "                            (F.col('timestamp')-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                            F.struct(*[m['fmt'](F.expr(formula)).alias(col_name) for col_name,formula in m['formula'].items() ]).alias('args')\n",
    "            ).where(F.col(\"ts\").isNotNull()).orderBy('ts')\n",
    "            trace_events.extend(rstdf.toJSON().collect())\n",
    "            for id in pidmap.values():\n",
    "                trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":tid,\"args\":{\"sort_index \":tid}}))\n",
    "            tid=tid+1\n",
    "            \n",
    "#        infodf=emondf.where(F.col(\"valid\")==F.lit(True)).groupBy(\"client\").agg(F.min('timestamp'),F.max('timestamp')).cache()\n",
    "#        infodf=infodf.withColumn('pid',mapexpr.getItem(F.col(\"client\")))\n",
    "#        trace_events.extend(infodf.select(\n",
    "#            F.lit('begin').alias(\"name\"),\n",
    "#            F.lit(\"i\").alias(\"ph\"),\n",
    "#            (F.col(\"min(timestamp)\")-F.lit(self.starttime)).alias('ts'),\n",
    "#             F.col(\"pid\"),\n",
    "#             F.lit(0).alias(\"tid\"),\n",
    "#             F.lit(\"p\").alias(\"s\")\n",
    "#        ).toJSON().collect())\n",
    "#        trace_events.extend(infodf.select(\n",
    "#            F.lit('begin').alias(\"name\"),\n",
    "#            F.lit(\"i\").alias(\"ph\"),\n",
    "#            (F.col(\"max(timestamp)\")-F.lit(self.starttime)).alias('ts'),\n",
    "#             F.col(\"pid\"),\n",
    "#             F.lit(0).alias(\"tid\"),\n",
    "#             F.lit(\"p\").alias(\"s\")\n",
    "#        ).toJSON().collect())\n",
    "#        infodf.unpersist()\n",
    "        return trace_events    \n",
    "    \n",
    "    \n",
    "def get_emon_parquets(apps,basedir):\n",
    "\n",
    "    emondfunion=None\n",
    "    emondfs=[]\n",
    "    for appid in apps:\n",
    "        slaves=fs.list_status(\"/\"+basedir+\"/\"+appid)\n",
    "        slaves=[f['pathSuffix'] for f in slaves if f['type']=='DIRECTORY']\n",
    "        for client in slaves:\n",
    "            if not fs.exists(f\"/{basedir}/{appid}/{client}/emon.parquet\"):\n",
    "                print(f\"/{basedir}/{appid}/{client}/emon.parquet is not found, trying to load data ...\")\n",
    "                emonals=Emon_Analysis(f\"/{basedir}/{appid}/{client}/emon.rst\")\n",
    "                emonals.load_data()\n",
    "            emondfs.append(f\"/{basedir}/{appid}/{client}/emon.parquet\")\n",
    "    return emondfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_gradient(s, m, M, cmap='PuBu', low=0, high=0):\n",
    "    from matplotlib import colors\n",
    "    rng = M - m\n",
    "    norm = colors.Normalize(m - (rng * low),\n",
    "                            M + (rng * high))\n",
    "    normed = norm(s.values)\n",
    "    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n",
    "    return ['background-color: {:s}'.format(color) for color in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app log analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf(\"long\")\n",
    "def isfinish_udf(s):\n",
    "    def isfinish(root):\n",
    "        if \"isFinalPlan=false\" in root['simpleString']:\n",
    "            return 0\n",
    "        if root['children'] is not None:\n",
    "            for c in root[\"children\"]:\n",
    "                if isfinish(c)==0:\n",
    "                    return 0\n",
    "        return 1\n",
    "    if len(s)>0:\n",
    "        return isfinish(s[0])\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "@pandas_udf(\"taskid long, start long, dur long, name string\", PandasUDFType.GROUPED_MAP)\n",
    "def time_breakdown(pdf):\n",
    "    ltime=pdf['Launch Time'][0]+2\n",
    "    pdf['start']=0\n",
    "    pdf['dur']=0\n",
    "    outpdf=[]\n",
    "    ratio=(pdf[\"Finish Time\"][0]-pdf[\"Launch Time\"][0])/pdf[\"Update\"].sum()\n",
    "    ratio=1 if ratio>1 else ratio\n",
    "    for idx,l in pdf.iterrows():\n",
    "        if(l[\"Update\"]*ratio>1):\n",
    "            outpdf.append([l[\"Task ID\"],ltime,int(l[\"Update\"]*ratio),l[\"mname\"]])\n",
    "            ltime=ltime+int(l[\"Update\"]*ratio)\n",
    "    if len(outpdf)>0:\n",
    "        return pandas.DataFrame(outpdf)\n",
    "    else:\n",
    "        return pandas.DataFrame({'taskid': pandas.Series([], dtype='long'),\n",
    "                   'start': pandas.Series([], dtype='long'),\n",
    "                   'dur': pandas.Series([], dtype='long'),\n",
    "                   'name': pandas.Series([], dtype='str'),\n",
    "                                })\n",
    "    \n",
    "class App_Log_Analysis(Analysis):\n",
    "    def __init__(self, file, jobids):\n",
    "        Analysis.__init__(self,file)\n",
    "        self.jobids=[] if jobids is None else [str(l) for l in jobids]\n",
    "        self.df=None\n",
    "        self.pids=[]\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"load data \", self.file)\n",
    "        jobids=self.jobids\n",
    "        df=spark.read.json(self.file)\n",
    "        if 'App ID' in df.columns:\n",
    "            self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        else:\n",
    "            self.appid=\"Application-00000000\"\n",
    "        \n",
    "        if df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").count()>0:\n",
    "            self.dfacc=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").select(F.col(\"executionId\").alias(\"queryid\"),F.explode(\"accumUpdates\"))\n",
    "        else:\n",
    "            self.dfacc = None\n",
    "            \n",
    "        if \"sparkPlanInfo\" in df.columns:\n",
    "            self.queryplans=df.where(\"(Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart' or Event='org.apache.spark.sql.execution.ui.SparkListenerSQLAdaptiveExecutionUpdate') \\\n",
    "                                  and (sparkPlanInfo.nodeName!='AdaptiveSparkPlan' or sparkPlanInfo.simpleString='AdaptiveSparkPlan isFinalPlan=true') \").select(F.col(\"executionId\").alias(\"queryid\"),\"sparkPlanInfo.*\")\n",
    "        else:\n",
    "            self.queryplans=None\n",
    "        \n",
    "        seen = set()\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.queryplans=self.queryplans.where(isfinish_udf(\"children\")==1)\n",
    "        \n",
    "            self.allmetrics=[]\n",
    "            if self.queryplans.count() > 0:\n",
    "                metrics=self.queryplans.collect()\n",
    "                def get_metric(root):\n",
    "                    for l in root[\"metrics\"]:\n",
    "                        if l['accumulatorId'] not in seen:\n",
    "                            seen.add(l['accumulatorId'])\n",
    "                            self.allmetrics.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"]])\n",
    "                    if root['children'] is not None:\n",
    "                        for c in root[\"children\"]:\n",
    "                            get_metric(c)\n",
    "                for c in metrics:\n",
    "                    get_metric(c)\n",
    "        \n",
    "            amsdf=spark.createDataFrame(self.allmetrics)\n",
    "            amsdf=amsdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"type\").withColumnRenamed(\"_3\",\"Name\").withColumnRenamed(\"_4\",\"nodeName\")\n",
    "        \n",
    "        \n",
    "        if self.dfacc is not None:\n",
    "            self.dfacc=self.dfacc.select(\"queryid\",(F.col(\"col\")[0]).alias(\"ID\"),(F.col(\"col\")[1]).alias(\"Update\")).join(amsdf,on=[\"ID\"])\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.metricscollect=[l for l in self.allmetrics if l[1] in ['nsTiming','timing'] and (l[2].startswith(\"totaltime_\") or l[2].startswith(\"scan time\") or l[2].startswith(\"shuffle write time\")) and l[2] not in(\"totaltime_collectbatch\") ]\n",
    "        \n",
    "        #config=df.where(\"event='SparkListenerJobStart' and Properties.`spark.executor.cores` is not null\").select(\"Properties.*\").limit(1).collect()\n",
    "        config=df.select(\"`Spark Properties`.*\").where(\"`spark.app.id` is not null\").limit(1).collect()\n",
    "    \n",
    "        configdic=config[0].asDict()\n",
    "        self.parallelism=int(configdic['spark.sql.shuffle.partitions']) if 'spark.sql.shuffle.partitions' in configdic else 1\n",
    "        self.executor_cores=int(configdic['spark.executor.cores']) if 'spark.executor.cores' in configdic else 1\n",
    "        self.executor_instances=int(configdic['spark.executor.instances']) if 'spark.executor.instances' in configdic else 1\n",
    "        self.taskcpus= int(configdic['spark.task.cpus'])if 'spark.task.cpus' in configdic else 1\n",
    "        self.batchsize= int(configdic['spark.sql.execution.arrow.maxRecordsPerBatch'])if 'spark.sql.execution.arrow.maxRecordsPerBatch' in configdic else 1\n",
    "        \n",
    "        self.realexecutors = df.select(\"Executor ID\").distinct().count()\n",
    "\n",
    "        if \"spark.sql.execution.id\" in df.where(\"Event='SparkListenerJobStart'\").select(\"Properties.*\").columns:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.col(\"Properties.`spark.sql.execution.id`\").alias(\"queryid\"),\"Stage IDs\")\n",
    "        else:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.lit(0).alias(\"queryid\"),\"Stage IDs\")\n",
    "        df_jobend=df.where(\"Event='SparkListenerJobEnd'\").select(\"`Job ID`\",\"Completion Time\")\n",
    "        df_job=df_jobstart.join(df_jobend,\"Job ID\")\n",
    "        df_job=df_job.withColumnRenamed(\"Submission Time\",\"job_start_time\")\n",
    "        df_job=df_job.withColumnRenamed(\"Completion Time\",\"job_stop_time\")\n",
    "        self.df_job=df_job\n",
    "        \n",
    "        jobstage=df_job.select(\"*\",F.explode(\"Stage IDs\").alias(\"Stage ID\"))\n",
    "        task=df.where(\"(Event='SparkListenerTaskEnd' or Event='SparkListenerTaskStart') \").select(\"Event\",\"Stage ID\",\"task info.*\",\"task metrics.*\")\n",
    "        \n",
    "        self.failed_stages = [str(l['Stage ID']) for l in task.where(\"Failed='true'\").select(\"Stage ID\").distinct().collect()]\n",
    "        \n",
    "        \n",
    "        taskjob=task.\\\n",
    "            select(\"Host\",\"`Event`\",\"`Launch Time`\",\"`Executor ID`\",\"`Task ID`\",\"`Finish Time`\",\n",
    "                    \"`Stage ID`\",\"`Input Metrics`.`Bytes Read`\",\"`Disk Bytes Spilled`\",\"`Memory Bytes Spilled`\",\"`Shuffle Read Metrics`.`Local Bytes Read`\",\"`Shuffle Read Metrics`.`Remote Bytes Read`\",\n",
    "                   \"`Shuffle Write Metrics`.`Shuffle Bytes Written`\",\"`Executor Deserialize Time`\",\"`Shuffle Read Metrics`.`Fetch Wait Time`\",\"`Executor Run Time`\",\"`Shuffle Write Metrics`.`Shuffle Write Time`\",\n",
    "                   \"`Result Serialization Time`\",\"`Getting Result Time`\",\"`JVM GC Time`\",\"`Executor CPU Time`\",\"Accumulables\",\n",
    "                    F.when(task['Finish Time']==0,task['Launch Time']).otherwise(task['Finish Time']).alias('eventtime')\n",
    "        ).join(jobstage,\"Stage ID\")\n",
    "        \n",
    "        self.df=taskjob\n",
    "        \n",
    "        if len(jobids)>0:\n",
    "            self.df=self.df.where('`Job ID` in ({:s})'.format(','.join(jobids)))\n",
    "        \n",
    "        queryids=self.df.select(F.col(\"queryid\").astype(IntegerType())).distinct().where(\"queryid is not null\").orderBy(\"queryid\").toPandas()\n",
    "        \n",
    "        self.query_num=len(queryids)\n",
    "        if self.query_num>0:\n",
    "            queryidx=queryids.reset_index()\n",
    "            queryidx['index']=queryidx['index']+1\n",
    "            #tpcds query\n",
    "            if self.query_num==103:\n",
    "                queryidx['index']=queryidx['index'].map(tpcds_query_map)\n",
    "            qidx=spark.createDataFrame(queryidx)\n",
    "            qidx=qidx.withColumnRenamed(\"index\",\"real_queryid\")\n",
    "            self.df=self.df.join(qidx,on=\"queryid\",how=\"left\")\n",
    "            if self.dfacc is not None:\n",
    "                self.dfacc=self.dfacc.join(qidx,on=\"queryid\",how='left')\n",
    "\n",
    "            if self.queryplans:\n",
    "                self.queryplans=self.queryplans.join(qidx,\"queryid\",how=\"right\")\n",
    "        \n",
    "        self.df=self.df.fillna(0)\n",
    "        self.df.cache()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##############################\n",
    "        \n",
    "        dfx=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"Stage ID\",\"Launch Time\",\"Finish Time\",\"Task ID\")\n",
    "        dfxpds=dfx.toPandas()\n",
    "        dfxpds.columns=[l.replace(\" \",\"_\") for l in dfxpds.columns]\n",
    "        dfxpds_ods=sqldf('''select * from dfxpds order by finish_time desc''')\n",
    "        criticaltasks=[]\n",
    "        idx=0\n",
    "        prefinish=0\n",
    "        launchtime=dfxpds_ods[\"Launch_Time\"][0]\n",
    "        criticaltasks.append([dfxpds_ods[\"Task_ID\"][0],launchtime,dfxpds_ods[\"Finish_Time\"][0]])\n",
    "        total_row=len(dfxpds_ods)\n",
    "\n",
    "        while True:\n",
    "            while idx<total_row:\n",
    "                if dfxpds_ods[\"Finish_Time\"][idx]-2<launchtime:\n",
    "                    break\n",
    "                idx=idx+1\n",
    "            else:\n",
    "                break\n",
    "            cur_finish=dfxpds_ods[\"Finish_Time\"][idx]\n",
    "            cur_finish=launchtime-1 if cur_finish>=launchtime else cur_finish\n",
    "            launchtime=dfxpds_ods[\"Launch_Time\"][idx]\n",
    "            criticaltasks.append([dfxpds_ods[\"Task_ID\"][idx],launchtime,cur_finish])\n",
    "        self.criticaltasks=criticaltasks\n",
    "\n",
    "    def get_basic_state(appals):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        display(HTML(\"<a href=http://sr525:18080/history/\"+appals.appid+\">http://sr525:18080/history/\"+appals.appid+\"</a>\"))\n",
    "        \n",
    "        errorcolor=\"#000000\" if appals.executor_instances == appals.realexecutors else \"#c0392b\"\n",
    "        \n",
    "        qtime=appals.get_query_time(plot=False)\n",
    "        sums=qtime.sum()\n",
    "        if len(appals.failed_stages)>0:\n",
    "            failure=\"<br>\".join([\"query: \" + str(l[\"real_queryid\"])+\"|stage: \" + str(l[\"Stage ID\"]) for l in appals.df.where(\"`Stage ID` in (\"+\",\".join(appals.failed_stages)+\")\").select(\"real_queryid\",\"Stage ID\").distinct().collect()])\n",
    "        else:\n",
    "            failure=\"\"\n",
    "        display(HTML(f'''\n",
    "        <table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">appid</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.appid}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.instances</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_instances}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.cores</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_cores}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle.partitions</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.parallelism}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle.partitions</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.batchsize}</strong></span></td>\n",
    "                </tr>                \n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">real executors</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{appals.realexecutors}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Failed Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{failure}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">runtime</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['runtime'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">disk spilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['disk spilled'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">memspilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['memspilled'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">local_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['local_read'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">remote_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['remote_read'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle_write</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['shuffle_write'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">task run time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['run_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">ser_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['ser_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">f_wait_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['f_wait_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">gc_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['gc_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">input read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['input read'],2)}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">acc_task_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['acc_task_time'],2)}</strong></td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "\n",
    "        '''))        \n",
    "        \n",
    "    def generate_trace_view_list_exec(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,**kwargs)\n",
    "        showcpu=kwargs.get('showcpu',False)\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        \n",
    "        showdf=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        \n",
    "        events=showdf.toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=self.starttime\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "        \n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                self.pids.append(pid)\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(\"task {:d} tid is {:s}.{:d}\".format(tsk,pid,t))\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "\n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "                    print(f'applog start time: {starttime}')\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['job id'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "\n",
    "                des_time=l['Executor Deserialize Time']\n",
    "                read_time=l['Fetch Wait Time']\n",
    "                exec_time=l['Executor Run Time']\n",
    "                write_time=math.floor(l['Shuffle Write Time']/1000000)\n",
    "                ser_time=l['Result Serialization Time']\n",
    "                getrst_time=l['Getting Result Time']\n",
    "                durtime=fintime-sstime-starttime;\n",
    "\n",
    "                times=[0,des_time,read_time,exec_time,write_time,ser_time,getrst_time]\n",
    "                time_names=['sched delay','deserialize time','read time','executor time','write time','serialize time','result time']\n",
    "                evttime=reduce((lambda x, y: x + y),times)\n",
    "                if evttime>durtime:\n",
    "                    times=[math.floor(l*1.0*durtime/evttime) for l in times]\n",
    "                else:\n",
    "                    times[0]=durtime-evttime\n",
    "\n",
    "                esstime=sstime\n",
    "                for idx in range(0,len(times)):\n",
    "                    if times[idx]>0:\n",
    "                        trace_events.append({\n",
    "                             'tid':pid+int(t),\n",
    "                             'ts':esstime,\n",
    "                             'dur':times[idx],                \n",
    "                             'pid':pid,\n",
    "                             'ph':'X',\n",
    "                             'name':time_names[idx]})\n",
    "                        if idx==3:\n",
    "                            trace_events.append({\n",
    "                                 'tid':pid+int(t),\n",
    "                                 'ts':esstime,\n",
    "                                 'dur':l['JVM GC Time'],\n",
    "                                 'pid':pid,\n",
    "                                 'ph':'X',\n",
    "                                 'name':'GC Time'})\n",
    "                            if showcpu:\n",
    "                                trace_events.append({\n",
    "                                     'tid':pid+int(t),\n",
    "                                     'ts':esstime,\n",
    "                                     'pid':pid,\n",
    "                                     'ph':'C',\n",
    "                                     'name':'cpu% {:d}'.format(pid+int(t)),\n",
    "                                     'args':{'value':l['Executor CPU Time']/1000000.0/times[idx]}})\n",
    "                                trace_events.append({\n",
    "                                     'tid':pid+int(t),\n",
    "                                     'ts':esstime+times[idx],\n",
    "                                     'pid':pid,\n",
    "                                     'ph':'C',\n",
    "                                     'name':'cpu% {:d}'.format(pid+int(t)),\n",
    "                                     'args':{'value':0}})\n",
    "                        esstime=esstime+times[idx]\n",
    "        self.starttime=starttime\n",
    "        return [json.dumps(l) for l in trace_events]\n",
    "\n",
    "    def generate_trace_view_list(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,**kwargs)\n",
    "        showcpu=kwargs.get('showcpu',False)\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        \n",
    "        showdf=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        \n",
    "        showdf=showdf.orderBy([\"eventtime\", \"Finish Time\"], ascending=[1, 0])\n",
    "        \n",
    "        events=showdf.drop(\"Accumulables\").toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=self.starttime\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "        \n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        tskmap={}\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                self.pids.append(pid)\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(\"task {:d} tid is {:s}.{:d}\".format(tsk,pid,t))\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "                \n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "                    print(f'applog start time: {starttime}')\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['Job ID'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "                tskmap[tsk]={'pid':pid,'tid':pid+int(t)}\n",
    "\n",
    "        self.starttime=starttime\n",
    "        self.tskmap=tskmap\n",
    "        output=[json.dumps(l) for l in trace_events]\n",
    "        \n",
    "        df=self.df\n",
    "        \n",
    "        if showcpu and len(self.metricscollect)>0:\n",
    "            metricscollect=self.metricscollect\n",
    "            metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "            m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "            metric_name_df = spark.createDataFrame(metricscollect)\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "\n",
    "            met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "            met_df=met_df.where(\"Update>1\")\n",
    "\n",
    "            metdfx=met_df.groupBy(\"Task ID\",\"elapsedtime\").agg(F.sum(\"Update\").alias(\"totalCnt\"))\n",
    "            taskratio=metdfx.withColumn(\"ratio\",F.when(F.col(\"totalCnt\")<F.col(\"elapsedtime\"),1).otherwise(F.col(\"elapsedtime\")/F.col(\"totalCnt\"))).select(\"Task ID\",\"ratio\")\n",
    "            met_df=met_df.join(taskratio,on=\"Task ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.col(\"Update\")*F.col(\"ratio\"))\n",
    "\n",
    "            w = (Window.partitionBy('Task ID').orderBy(F.desc(\"Update\")).rangeBetween(Window.unboundedPreceding, 0))\n",
    "            met_df=met_df.withColumn('cum_sum', F.sum('Update').over(w))\n",
    "\n",
    "            met_df=met_df.withColumn(\"starttime\",F.col(\"Launch Time\")+F.col(\"cum_sum\")-F.col(\"Update\"))\n",
    "\n",
    "            tskmapdf = spark.createDataFrame(pandas.DataFrame(self.tskmap).T.reset_index())\n",
    "            met_df=met_df.join(tskmapdf,on=[met_df[\"Task ID\"]==tskmapdf[\"index\"]])\n",
    "\n",
    "            rstdf=met_df.select(\n",
    "                F.col(\"tid\"),\n",
    "                F.round(F.col(\"starttime\")-self.starttime,0).alias(\"ts\"),\n",
    "                F.round(F.col(\"Update\"),0).alias(\"dur\"),\n",
    "                F.col(\"pid\"),\n",
    "                F.lit(\"X\").alias(\"ph\"),\n",
    "                F.col(\"mname\").alias(\"name\")\n",
    "            ).where(F.col(\"ts\").isNotNull()).orderBy('ts')\n",
    "\n",
    "            output.extend(rstdf.toJSON().collect())\n",
    "\n",
    "            qtime=df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"real_queryid\").agg(F.min(\"Finish Time\").alias(\"time\"))\n",
    "            output.extend(qtime.select(\n",
    "                F.lit(\"i\").alias(\"ph\"),\n",
    "                (F.col(\"time\")-starttime).alias('ts'),\n",
    "                F.lit(0).alias(\"pid\"),\n",
    "                F.lit(0).alias(\"tid\"),\n",
    "                F.lit(\"p\").alias(\"s\")\n",
    "            ).toJSON().collect())\n",
    "        \n",
    "        self.starttime=starttime\n",
    "        \n",
    "        if kwargs.get(\"show_criticalshow_time_metric_path\",True):\n",
    "            output.extend(self.generate_critical_patch_traceview(hostid-1))\n",
    "        \n",
    "        return output        \n",
    "\n",
    "    def generate_critical_patch_traceview(self,pid):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        traces=[]\n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd' and real_queryid is not null\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        traces.extend(df_ctsk.select(F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)+1).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")-1).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"stg\"),F.col(\"Stage ID\")).alias(\"name\"),\n",
    "                      F.struct(\n",
    "                          F.col(\"Task ID\").alias('taskid'),\n",
    "                          F.col(\"Executor ID\").astype(IntegerType()).alias('exec_id'),\n",
    "                          F.col(\"Host\").alias(\"host\"),\n",
    "                          ).alias(\"args\")\n",
    "                        ).toJSON().collect())\n",
    "        traces.extend(df.groupBy(\"real_queryid\").agg(F.max(\"Finish Time\").alias(\"finish\"),F.min(\"Launch Time\").alias(\"launch\")).select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"qry\"),F.col(\"real_queryid\")).alias(\"name\")).toJSON().collect())\n",
    "\n",
    "\n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df_ctsk.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        metric_name_df=metric_name_df.where(\"mname <> 'totaltime_collectbatch'\")\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        \n",
    "        #pandas UDF doesn't work. hang\n",
    "        #tmbk=met_df.groupBy('Task ID').apply(time_breakdown)\n",
    "        \n",
    "        w=Window.partitionBy('Task ID')\n",
    "        met_df1=met_df.withColumn(\"sum_update\",F.sum(\"Update\").over(w))\n",
    "        met_df2=met_df1.withColumn(\"ratio\",(F.col(\"Finish Time\")-F.col(\"Launch Time\")-2)/F.col(\"sum_update\"))\n",
    "        met_df3=met_df2.withColumn(\"ratio\",F.when(F.col(\"ratio\")>1,1).otherwise(F.col(\"ratio\")))\n",
    "        met_df4=met_df3.withColumn(\"update_ratio\",F.floor(F.col(\"ratio\")*F.col(\"Update\")))\n",
    "        met_df5=met_df4.where(F.col(\"update_ratio\")>2)\n",
    "        w = (Window.partitionBy('Task ID').orderBy(F.desc(\"update_ratio\")).rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "        met_df6=met_df5.withColumn('ltime_dur', F.sum('update_ratio').over(w))\n",
    "        met_df8=met_df6.withColumn(\"ltime\",F.col(\"ltime_dur\")+F.col(\"Launch Time\")-F.col(\"update_ratio\"))\n",
    "\n",
    "        tmbk=met_df8.withColumn(\"taskid\",F.col(\"Task ID\")).withColumn(\"start\",F.col(\"ltime\")+F.lit(1)).withColumn(\"dur\",F.col(\"update_ratio\")-F.lit(1)).withColumn(\"name\",F.col(\"mname\"))\n",
    "        \n",
    "        \n",
    "        traces.extend(tmbk.select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"start\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"dur\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.col(\"name\").alias(\"name\")).toJSON().collect())\n",
    "        traces.append(json.dumps({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"critical path\"}\n",
    "                      }))\n",
    "        return traces    \n",
    "    \n",
    "    def show_Stage_histogram(apps,stageid,bincount):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        inputsize = apps.df.where(\"`Stage ID`={:d}\".format(stageid)).select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Task ID\") \\\n",
    "                      .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "\n",
    "\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) )\\\n",
    "                        .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                        .fillna(0) \\\n",
    "                        .select(F.col('Host'), \n",
    "                                F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                F.round((F.col('`input read`')+F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input'))\n",
    "        stage37=stage37.cache()\n",
    "        hist_elapsedtime=stage37.select('elapsedtime').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        hist_input=stage37.select('input').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        fig, axs = plt.subplots(figsize=(30, 5),nrows=1, ncols=2)\n",
    "        ax=axs[0]\n",
    "        binSides, binCounts = hist_elapsedtime\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} elapsed time breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        ax=axs[1]\n",
    "        binSides, binCounts = hist_input\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} input data breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        out=stage37\n",
    "        outpds=out.toPandas()\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=3, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "        groups= outpds.groupby('Host')\n",
    "        for name, group in groups:\n",
    "            axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "        axs[0].set_xlabel('input size (MB)')\n",
    "        axs[0].set_ylabel('elapsed time (s)')\n",
    "\n",
    "        axs[0].legend()\n",
    "\n",
    "        axs[0].get_shared_y_axes().join(axs[0], axs[1])\n",
    "\n",
    "        sns.violinplot(y='elapsedtime', x='Host', data=outpds,palette=['g'],ax=axs[1])\n",
    "\n",
    "        sns.violinplot(y='input', x='Host', data=outpds,palette=['g'],ax=axs[2])\n",
    "\n",
    "        #ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "        #ax.yaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "\n",
    "        if False:\n",
    "            out=stage37\n",
    "            vecAssembler = VectorAssembler(inputCols=[\"input\",'elapsedtime'], outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "            new_df = vecAssembler.transform(out)\n",
    "            kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "            model = kmeans.fit(new_df.select('features'))\n",
    "            transformed = model.transform(new_df)\n",
    "\n",
    "\n",
    "            outpds=transformed.select('Host','elapsedtime','input','prediction').toPandas()\n",
    "\n",
    "            fig, axs = plt.subplots(nrows=1, ncols=2, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "            plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "            groups= outpds.groupby('prediction')\n",
    "            for name, group in groups:\n",
    "                axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "            axs[0].legend()\n",
    "\n",
    "            bars=transformed.where('prediction=1').groupBy(\"Host\").count().toPandas()\n",
    "\n",
    "            axs[1].bar(bars['Host'], bars['count'], 0.4, color='coral')\n",
    "            axs[1].set_title('cluster=1')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def show_Stages_hist(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df\n",
    "        \n",
    "        totaltime=df.where(\"event='SparkListenerTaskEnd'\" ).agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).collect()[0]['total_time']\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`').agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).orderBy('total_time', ascending=False).toPandas()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "        fig, ax = plt.subplots(figsize=(30, 5))\n",
    "\n",
    "        rects1 = ax.plot(stage_time['index'],stage_time['acc_total'],'b.-')\n",
    "        ax.set_xticks(stage_time['index'])\n",
    "        ax.set_xticklabels(stage_time['Stage ID'])\n",
    "        ax.set_xlabel('stage')\n",
    "        ax.grid(which='major', axis='x')\n",
    "        plt.show()\n",
    "        shownstage=[]\n",
    "        for x in stage_time.index:\n",
    "            if stage_time['acc_total'][x]<=threshold:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "            else:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "                break\n",
    "        for row in shownstage:\n",
    "            apps.show_Stage_histogram(row,bincount) \n",
    "            \n",
    "    def get_hottest_stages(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df.where(\"queryid is not NULL\")\n",
    "\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`','Job ID','real_queryid').agg(\n",
    "            F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time'),\n",
    "            F.stddev(F.col('Finish Time')/1000-F.col('Launch Time')/1000).alias('stdev_time'),\n",
    "            F.count(\"*\").alias(\"cnt\"),\n",
    "            F.first('queryid').astype(IntegerType()).alias('queryid')\n",
    "            )\\\n",
    "            .select('`Stage ID`','Job ID','real_queryid','queryid',\n",
    "                    (F.col(\"total_time\")/1000/(F.when(F.col(\"cnt\")>F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus),F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus)).otherwise(F.col(\"cnt\")))).alias(\"total_time\"),\n",
    "                    F.col(\"stdev_time\")\n",
    "                   ).orderBy('total_time', ascending=False).toPandas()\n",
    "\n",
    "        totaltime=stage_time['total_time'].sum()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time['total'] = stage_time['total_time']/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "\n",
    "        shownstage=stage_time.loc[stage_time['acc_total'] <=threshold]\n",
    "        shownstage['stg']=shownstage['real_queryid'].astype(str)+'_'+shownstage['Job ID'].astype(str)+'_'+shownstage['Stage ID'].astype(str)\n",
    "        if plot:\n",
    "            shownstage.plot.bar(x=\"stg\",y=\"total\",figsize=(30,8))\n",
    "\n",
    "\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=max(stage_time.queryid))\n",
    "        cmap = matplotlib.cm.get_cmap('brg')\n",
    "        def setbkcolor(x):\n",
    "            rgba=cmap(norm(x['queryid']))\n",
    "            return ['background-color:rgba({:d},{:d},{:d},1); color:white'.format(int(rgba[0]*255),int(rgba[1]*255),int(rgba[2]*255))]*9\n",
    "\n",
    "        if plot:\n",
    "            display(stage_time.style.apply(setbkcolor,axis=1).format({\"total_time\":lambda x: '{:,.2f}'.format(x),\"acc_total\":lambda x: '{:,.2%}'.format(x),\"total\":lambda x: '{:,.2%}'.format(x)}))\n",
    "        \n",
    "        return stage_time\n",
    "\n",
    "    def scatter_elapsetime_input(apps,stageid):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) ).select(F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),F.round((F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input')).toPandas()\n",
    "        stage37.plot.scatter('input','elapsedtime',figsize=(30, 5))\n",
    "\n",
    "    def get_critical_path_stages(self):     \n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        df_ctsk=df_ctsk.withColumn(\"elapsed\",(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000)\n",
    "        return df_ctsk.where(\"elapsed>10\").orderBy(F.desc(\"elapsed\")).select(\"real_queryid\",F.round(\"elapsed\",2).alias(\"elapsed\"),\"Host\",\"executor ID\",\"Stage ID\",\"Task ID\",F.round(F.col(\"Bytes Read\")/1000000,0).alias(\"file read\"),F.round((F.col(\"Local Bytes Read\")+F.col(\"Remote Bytes Read\"))/1000000,0).alias(\"shuffle read\")).toPandas()\n",
    "        \n",
    "    def show_time_metric(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        taskids=kwargs.get(\"taskids\",None)\n",
    "        \n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        \n",
    "        showexecutor=kwargs.get(\"showexecutor\",True) if not taskids else False\n",
    "        queryid = query[0] if query else 0\n",
    "        \n",
    "        df=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        df=df.where(F.col(\"real_queryid\").isin(query)) if query else df.where(\"queryid is not NULL\")\n",
    "\n",
    "        df=df.where(F.col(\"Task ID\").isin(taskids)) if taskids else df\n",
    "\n",
    "        exec_cores=1 if taskids else self.executor_cores\n",
    "        execs=1 if taskids else self.executor_instances\n",
    "\n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        runtime=metrics_explode.agg(F.round(F.max(\"Finish Time\")/1000-F.min(\"Launch Time\")/1000,2).alias(\"runtime\")).collect()[0][\"runtime\"]\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        outpdf=met_df.groupBy(\"`Executor ID`\",\"mname\").sum(\"Update\").orderBy(\"Executor ID\").toPandas()\n",
    "\n",
    "        met_time_cnt=df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        exectime=met_time_cnt.groupBy(\"Executor ID\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\"),F.sum(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"tasktime\"))\n",
    "\n",
    "        totaltime_query=met_time_cnt.groupBy(\"real_queryid\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\")).agg(F.sum(\"totaltime\").alias(\"totaltime\")).collect()\n",
    "        totaltime_query=totaltime_query[0][\"totaltime\"]\n",
    "        \n",
    "        pdf=exectime.toPandas()\n",
    "        exeids=set(outpdf['Executor ID'])\n",
    "        outpdfs=[outpdf[outpdf[\"Executor ID\"]==l] for l in exeids]\n",
    "        tasktime=pdf.set_index(\"Executor ID\").to_dict()['tasktime']\n",
    "\n",
    "        def comb(l,r):\n",
    "            execid=list(r['Executor ID'])[0]\n",
    "            lp=r[['mname','sum(Update)']]\n",
    "            lp.columns=[\"mname\",\"val_\"+execid]\n",
    "            idle=totaltime_query*exec_cores-tasktime[execid]\n",
    "            nocount=tasktime[execid]-sum(lp[\"val_\"+execid])\n",
    "            if idle<0:\n",
    "                idle=0\n",
    "            if nocount<0:\n",
    "                nocount=0\n",
    "            lp=lp.append([{\"mname\":\"idle\",\"val_\"+execid:idle}])\n",
    "            lp=lp.append([{\"mname\":\"not_counted\",\"val_\"+execid:nocount}])\n",
    "            if l is not None:\n",
    "                return pandas.merge(lp, l,on=[\"mname\"],how='outer')\n",
    "            else:\n",
    "                return lp\n",
    "\n",
    "        rstpdf=None\n",
    "        for l in outpdfs[0:]:\n",
    "            rstpdf=comb(rstpdf,l)\n",
    "            \n",
    "        for l in [l for l in rstpdf.columns if l!=\"mname\"]:\n",
    "            rstpdf[l]=rstpdf[l]/1000/exec_cores\n",
    "    \n",
    "        rstpdf=rstpdf.sort_values(by=\"val_\"+list(exeids)[0],axis=0,ascending=False)\n",
    "        if showexecutor and plot:\n",
    "            rstpdf.set_index(\"mname\").T.plot.bar(stacked=True,figsize=(30,8))\n",
    "        pdf_sum=pandas.DataFrame(rstpdf.set_index(\"mname\").T.sum())\n",
    "        totaltime=totaltime_query/1000\n",
    "        pdf_sum[0]=pdf_sum[0]/(execs)\n",
    "        pdf_sum[0][\"idle\"]=(totaltime_query-sum(tasktime.values())/execs/exec_cores)/1000\n",
    "        pdf_sum=pdf_sum.sort_values(by=0,axis=0,ascending=False)\n",
    "        pdf_sum=pdf_sum.T\n",
    "        pdf_sum.columns=[\"{:>2.0f}%_{:s}\".format(pdf_sum[l][0]/totaltime*100,l) for l in pdf_sum.columns]\n",
    "        matplotlib.rcParams['font.sans-serif'] = \"monospace\"\n",
    "        matplotlib.rcParams['font.family'] = \"monospace\"\n",
    "        import matplotlib.font_manager as font_manager\n",
    "        if plot:\n",
    "            ax=pdf_sum.plot.bar(stacked=True,figsize=(30,8))\n",
    "            font = font_manager.FontProperties(family='monospace',\n",
    "                                               style='normal', size=14)\n",
    "            ax.legend(prop=font,loc=4)\n",
    "            plt.title(\"{:s} q{:d} executors={:d} cores_per_executor={:d} parallelism={:d} sumtime={:.0f} runtime={:.0f}\".format(self.file.split(\"/\")[2],queryid,self.executor_instances,self.executor_cores,self.parallelism,totaltime,runtime),fontdict={'fontsize':24})\n",
    "        return pdf_sum\n",
    "\n",
    "    def show_critical_path_time_breakdown(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        return self.show_time_metric(taskids=[l[0].item() for l in self.criticaltasks])\n",
    "    \n",
    "    def get_spark_config(self):\n",
    "        df=spark.read.json(self.file)\n",
    "        self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        pandas.set_option('display.max_rows', None)\n",
    "        pandas.set_option('display.max_columns', None)\n",
    "        pandas.set_option('display.max_colwidth', 100000)\n",
    "        return df.select(\"Properties.*\").where(\"`spark.app.id` is not null\").limit(1).toPandas().T\n",
    "    \n",
    "    def get_app_name(self):\n",
    "        cfg=self.get_spark_config()\n",
    "        display(HTML(\"<font size=5 color=red>\" + cfg.loc[cfg.index=='spark.app.name'][0][0]+\"</font>\"))\n",
    "        \n",
    "        \n",
    "    def get_query_time(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        showtable=kwargs.get(\"showtable\",True)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "           \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)) if queryid else self.df.where(\"queryid is not NULL\")\n",
    "        \n",
    "            \n",
    "        stages=df.select(\"real_queryid\",\"Stage ID\").distinct().orderBy(\"Stage ID\").groupBy(\"real_queryid\").agg(F.collect_list(\"Stage ID\").alias(\"stages\")).orderBy(\"real_queryid\")\n",
    "        runtimeacc=df.where(\"Event='SparkListenerTaskEnd'\") \\\n",
    "                      .groupBy(\"real_queryid\") \\\n",
    "                      .agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"acc_task_time\"))\n",
    "        inputsize = df.select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"real_queryid\") \\\n",
    "                      .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read\")).orderBy(\"real_queryid\")\n",
    "        if self.dfacc is not None:\n",
    "            inputsizev1 = self.dfacc.where(\"Name='size of files read'\").groupBy(\"real_queryid\").agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read v1\")).orderBy(\"real_queryid\")\n",
    "            inputsize=inputsize.join(inputsizev1,on=\"real_queryid\",how=\"outer\")\n",
    "            inputsize=inputsize.withColumn(\"input read\",F.coalesce(F.col(\"input read\"),F.col(\"input read v1\"))).drop(\"input read v1\")\n",
    "        \n",
    "        outputrows = df.select(\"real_queryid\",\"Stage ID\",\"Stage ID\",F.explode(\"Accumulables\"))\\\n",
    "                        .select(\"real_queryid\",\"Stage ID\",\"Stage ID\",\"col.*\")\\\n",
    "                        .where(\"Name='number of output rows'\")\\\n",
    "                        .groupBy(\"real_queryid\")\\\n",
    "                        .agg(F.round(F.sum(\"Update\")/1000000000,2).alias(\"output rows\"))\n",
    "        \n",
    "        stages=runtimeacc.join(stages,on=\"real_queryid\",how=\"left\")\n",
    "        stages=inputsize.join(stages,on=\"real_queryid\",how=\"left\")\n",
    "        stages=stages.join(outputrows,on='real_queryid',how=\"left\")\n",
    "        \n",
    "        out=df.groupBy(\"real_queryid\").agg(\n",
    "            F.round(F.max(\"job_stop_time\")/1000-F.min(\"job_start_time\")/1000,2).alias(\"runtime\"),\n",
    "            F.round(F.sum(\"Disk Bytes Spilled\")/1024/1024/1024,2).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(\"Memory Bytes Spilled\")/1024/1024/1024,2).alias(\"memspilled\"),\n",
    "            F.round(F.sum(\"Local Bytes Read\")/1024/1024/1024,2).alias(\"local_read\"),\n",
    "            F.round(F.sum(\"Remote Bytes Read\")/1024/1024/1024,2).alias(\"remote_read\"),\n",
    "            F.round(F.sum(\"Shuffle Bytes Written\")/1024/1024/1024,2).alias(\"shuffle_write\"),\n",
    "            F.round(F.sum(\"Executor Deserialize Time\")/1000/self.parallelism,2).alias(\"deser_time\"),\n",
    "            F.round(F.sum(\"Executor Run Time\")/1000/self.parallelism,2).alias(\"run_time\"),\n",
    "            F.round(F.sum(\"Result Serialization Time\")/1000/self.parallelism,2).alias(\"ser_time\"),\n",
    "            F.round(F.sum(\"Fetch Wait Time\")/1000/self.parallelism,2).alias(\"f_wait_time\"),\n",
    "            F.round(F.sum(\"JVM GC Time\")/1000/self.parallelism,2).alias(\"gc_time\"),\n",
    "            F.max(\"queryid\").alias(\"queryid\")\n",
    "            ).join(stages,\"real_queryid\",how=\"left\").orderBy(\"real_queryid\").toPandas().set_index(\"real_queryid\")\n",
    "        out[\"executors\"]=self.executor_instances\n",
    "        out[\"core/exec\"]=self.executor_cores\n",
    "        out[\"task.cpus\"]=self.taskcpus\n",
    "        out['parallelism']=self.parallelism\n",
    "        \n",
    "        if not showtable:\n",
    "            return out\n",
    "\n",
    "        def highlight_greater(x):\n",
    "            m1 = x['acc_task_time'] / x['runtime'] * 100\n",
    "            m2 = x['run_time'] / x['runtime'] * 100\n",
    "            m3 = x['f_wait_time'] / x['runtime'] * 100\n",
    "            \n",
    "\n",
    "            df1 = pandas.DataFrame('', index=x.index, columns=x.columns)\n",
    "\n",
    "            df1['acc_task_time'] = m1.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['run_time'] = m2.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['f_wait_time'] = m3.apply(lambda x: 'background-image: linear-gradient(to right,#d65f5f {:f}%,white {:f}%)'.format(x,x))\n",
    "            return df1\n",
    "\n",
    "\n",
    "        cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "        if plot:\n",
    "            display(out.style.apply(highlight_greater, axis=None).background_gradient(cmap=cm,subset=['input read', 'shuffle_write']))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_query_time_metric(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        querids=self.df.select(\"queryid\").distinct().collect()\n",
    "        for idx,q in enumerate([l[\"queryid\"] for l in querids]):\n",
    "            self.show_time_metric(query=[q,],showexecutor=False)\n",
    "            \n",
    "    def getOperatorCount(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        df=spark.read.json(self.file)\n",
    "        queryids=self.df.select(F.col(\"queryid\").astype(LongType()),F.col(\"real_queryid\")).distinct()\n",
    "        queryplans=self.queryplans.collect()\n",
    "        list_queryid=[l.real_queryid for l in queryids.collect()]\n",
    "\n",
    "        def get_child(execid,node):\n",
    "            if node[\"nodeName\"] not in qps:\n",
    "                qps[node[\"nodeName\"]]={l:0 for l in list_queryid}\n",
    "            qps[node[\"nodeName\"]][execid]=qps[node[\"nodeName\"]][execid]+1\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "\n",
    "        qps={}\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        return pandas.DataFrame(qps).T.sort_index(axis=0)        \n",
    "    \n",
    "    def get_query_plan(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        stageid=kwargs.get(\"stageid\",None)\n",
    "        \n",
    "        show_plan_only=kwargs.get(\"show_plan_only\",False)\n",
    "        show_simple_string=kwargs.get(\"show_simple_string\",False)\n",
    "\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        colors=[\"#{:02x}{:02x}{:02x}\".format(int(l[0]*255),int(l[1]*255),int(l[2]*255)) for l in matplotlib.cm.get_cmap('tab20').colors]\n",
    "\n",
    "        if queryid is not None and (type(queryid)==int or type(queryid)==str):\n",
    "            queryid = [queryid,]\n",
    "            shown_stageid = [l[\"Stage ID\"] for l in self.df.where(F.col(\"real_queryid\").isin(queryid)).select(\"Stage ID\").distinct().collect()]\n",
    "        if stageid is not None and type(stageid)==int:\n",
    "            shown_stageid = [stageid,]\n",
    "            queryid = [l[\"real_queryid\"] for l in self.df.where(F.col(\"`Stage ID`\").isin(shown_stageid)).select(\"real_queryid\").limit(1).collect()]\n",
    "\n",
    "\n",
    "        queryplans=[]\n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").select(\"Stage ID\",\"ID\",\"Update\").groupBy(\"ID\",\"Stage ID\").agg(F.round(F.sum(\"Update\"),1).alias(\"value\"),F.round(F.stddev(\"Update\"),1).alias(\"stdev\")).collect()\n",
    "        accid2stageid={l.ID:(l[\"Stage ID\"],l[\"value\"],l[\"stdev\"]) for l in dfmetric}\n",
    "\n",
    "        stagetime=self.df.where((F.col(\"real_queryid\").isin(queryid))).where(F.col(\"Event\")=='SparkListenerTaskEnd').groupBy(\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.stddev(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000,1).alias(\"time stdev\"),\n",
    "            F.count(F.col(\"Task ID\")).alias(\"partitions\")\n",
    "            ).orderBy(F.desc(\"elapsed time\")).collect()\n",
    "\n",
    "        apptotaltime=reduce(lambda x,y: x+y['elapsed time'], stagetime,0)\n",
    "        if apptotaltime==0:\n",
    "            display(HTML(\"<font size=4 color=red>Error, totaltime is 0 </font>\"))\n",
    "            apptotaltime=1\n",
    "            return \"\"\n",
    "\n",
    "        stagemap={l[\"Stage ID\"]:l[\"elapsed time\"] for l in stagetime}\n",
    "        stage_time_stdev_map={l[\"Stage ID\"]:l[\"time stdev\"] for l in stagetime}\n",
    "        stagepartmap={l[\"Stage ID\"]:l[\"partitions\"] for l in stagetime}\n",
    "\n",
    "        keystage=[]\n",
    "        keystagetime=[]\n",
    "        subtotal=0\n",
    "        for s in stagetime:\n",
    "            subtotal=subtotal+s['elapsed time']\n",
    "            keystage.append(s['Stage ID'])\n",
    "            keystagetime.append(s['elapsed time'])\n",
    "            if subtotal/apptotaltime>0.9:\n",
    "                break\n",
    "        keystagetime=[\"{:02x}{:02x}\".format(int(255*l/keystagetime[0]),255-int(255*l/keystagetime[0])) for l in keystagetime if keystagetime[0]>0]\n",
    "        keystagemap=dict(zip(keystage,keystagetime))\n",
    "        outstr=[]\n",
    "        def print_plan(level,node,parent_stageid):\n",
    "            stageid = accid2stageid[int(node[\"metrics\"][0][\"accumulatorId\"])][0]  if len(node[\"metrics\"])>0 and node[\"metrics\"][0][\"accumulatorId\"] in accid2stageid else parent_stageid\n",
    "\n",
    "            if stageid in shown_stageid:\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystage else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stageParts=0 if stageid not in stagepartmap else stagepartmap[stageid]\n",
    "\n",
    "                input_rowcntstr=\"\"\n",
    "                output_rowcntstr=\"\"\n",
    "                timestr=\"\"\n",
    "                timename=\"\"\n",
    "                input_columnarbatch=\"\"\n",
    "                output_columnarbatch=\"\"\n",
    "                output_row_batch=\"\"\n",
    "                other_metric_name_str=\"\"\n",
    "                other_metric_str=\"\"\n",
    "\n",
    "                outputrows=0\n",
    "                outputbatches=0\n",
    "                for m in node[\"metrics\"]:\n",
    "\n",
    "                    if m[\"accumulatorId\"] not in accid2stageid:\n",
    "                        continue\n",
    "\n",
    "                    value=accid2stageid[m[\"accumulatorId\"]][1]\n",
    "                    stdev_value=accid2stageid[m[\"accumulatorId\"]][2]\n",
    "                    if m[\"metricType\"] in ['nsTiming','timing']:\n",
    "                        totaltime=value/1000 if  m[\"metricType\"] == 'timing' else value/1000000000\n",
    "                        stdev_value=stdev_value/1000 if  m[\"metricType\"] == 'timing' else stdev_value/1000000000\n",
    "                        timename=timename+m[\"name\"]+\"<br>\"\n",
    "                        timeratio= 0  if stagetime==0 else totaltime/self.executor_instances/self.executor_cores*self.taskcpus/stagetime*100\n",
    "                        timestr=timestr+\"{:.2f}s ({:.1f}%, {:.1f}%, {:.2f})<br>\".format(totaltime,timeratio, totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100,stdev_value)\n",
    "                    elif m[\"name\"]==\"number of output rows\":\n",
    "                        output_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                        outputrows=value\n",
    "                    elif m[\"name\"] in [\"number of output columnar batches\",\"number of output batches\",\"output_batches\"]:\n",
    "                        output_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                        outputbatches=value\n",
    "                    elif m[\"name\"]==\"number of input rows\":\n",
    "                        input_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                    elif m[\"name\"] in [\"number of input batches\",\"number of Input batches\",\"input_batches\"]:\n",
    "                        input_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                    else:\n",
    "                        other_metric_name_str=other_metric_name_str+m[\"name\"]+\"<br>\"\n",
    "                        if value>1000000000:\n",
    "                            other_metric_str=other_metric_str+\"{:,.1f} G (stdev: {:,.1f})<br>\".format(value/1000000000,stdev_value/1000000000)\n",
    "                        elif value>1000000:\n",
    "                            other_metric_str=other_metric_str+\"{:,.1f} M (stdev: {:,.1f})<br>\".format(value/1000000,stdev_value/1000000)\n",
    "                        elif value>1000:\n",
    "                            other_metric_str=other_metric_str+\"{:,.1f} K (stdev: {:,.1f})<br>\".format(value/1000,stdev_value/1000)\n",
    "                        else:\n",
    "                            other_metric_str=other_metric_str+\"{:,d} (stdev: {:,.1f})<br>\".format(int(value),stdev_value)\n",
    "\n",
    "\n",
    "                if outputrows>0 and outputbatches>0:\n",
    "                    output_row_batch=\"{:,d}\".format(int(outputrows/outputbatches))\n",
    "\n",
    "\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystage else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stage_time_stdev=0 if stageid not in stage_time_stdev_map else stage_time_stdev_map[stageid]\n",
    "                \n",
    "                if not show_plan_only:\n",
    "                    nodestr= \" \".join([\"|_\" for l in range(0,level)]) + \" \" + node[\"nodeName\"]\n",
    "                    if show_simple_string :\n",
    "                        nodestr = nodestr +  \"<br>\" + node['simpleString']\n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime}({stage_time_stdev}) </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + nodestr + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_row_batch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {timename} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'>{timestr}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {other_metric_name_str} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'>{other_metric_str}</td></tr>\")\n",
    "                else:\n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + \" \".join([\"|_\" for l in range(0,level)]) + \" \" + node[\"nodeName\"] + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td></tr>\")\n",
    "                    \n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    print_plan(level+1,c,stageid)\n",
    "\n",
    "        for c in queryplans:\n",
    "            outstr.append(\"<font color=red size=4>\"+str(c['real_queryid'])+\"</font><table>\")\n",
    "            if not show_plan_only:\n",
    "                outstr.append('''<tr>\n",
    "                                    <td>stage id</td>\n",
    "                                    <td>stage time</td>\n",
    "                                    <td>partions</td>\n",
    "                                    <td>operator</td>\n",
    "                                    <td>input rows</td>\n",
    "                                    <td>input batches</td>\n",
    "                                    <td>output rows</td>\n",
    "                                    <td>output batches</td>\n",
    "                                    <td>output rows/batch</td>\n",
    "                                    <td>time metric name</td>\n",
    "                                    <td>time(%stage,%total,stdev)</td>\n",
    "                                    <td>other metric name</td>\n",
    "                                    <td>value</td>\n",
    "                                </tr>''')\n",
    "            else:\n",
    "                outstr.append('''<tr>\n",
    "                                    <td>stage id</td>\n",
    "                                    <td>stage time</td>\n",
    "                                    <td>partions</td>\n",
    "                                    <td>operator</td>\n",
    "                                    <td>output rows</td>\n",
    "                                </tr>''')\n",
    "\n",
    "            print_plan(0,c,0)\n",
    "            outstr.append(\"</table>\")\n",
    "        if plot:\n",
    "            display(HTML(\" \".join(outstr)))\n",
    "        return \" \".join(outstr)\n",
    "\n",
    "    def get_metric_output_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of output rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_input_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of input rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_rowcnt(self,rowname, **kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "\n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "        queryplans=[]\n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "\n",
    "\n",
    "        qps=[]\n",
    "\n",
    "        rownames=rowname if type(rowname)==list else [rowname,]\n",
    "        def get_child(execid,node):\n",
    "            if node['metrics'] is not None:\n",
    "                outputrows=[x for x in node[\"metrics\"] if \"name\" in x and x[\"name\"] in rownames]\n",
    "                if len(outputrows)>0:\n",
    "                    qps.append([node[\"nodeName\"],execid,outputrows[0]['accumulatorId']])\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        if len(qps)==0:\n",
    "            print(\"Metric \",rowname,\" is not found. \")\n",
    "            return None\n",
    "        stagetime=self.df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"Stage ID\").agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"stage time\"))\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").drop(\"metric\")\n",
    "        numrowmetric=spark.createDataFrame(qps)\n",
    "        numrowmetric=numrowmetric.withColumnRenamed(\"_1\",\"metric\").withColumnRenamed(\"_2\",\"real_queryid\").withColumnRenamed(\"_3\",\"metricid\")\n",
    "        dfmetric_rowcnt=dfmetric.join(numrowmetric.drop(\"real_queryid\"),on=[F.col(\"metricid\")==F.col(\"ID\")],how=\"right\")\n",
    "        stagemetric=dfmetric_rowcnt.groupBy(\"queryid\",\"real_queryid\",\"Job ID\",\"Stage ID\",\"metricid\").agg(F.round(F.sum(\"Update\")/1000000,2).alias(\"total_row\"),F.max(\"metric\").alias(\"nodename\")).join(stagetime,\"Stage ID\")\n",
    "\n",
    "        if queryid:\n",
    "            return stagemetric.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"Stage ID\").toPandas()\n",
    "        else:\n",
    "            noderow=stagemetric.groupBy(\"real_queryid\",\"nodename\").agg(F.round(F.sum(\"total_row\"),2).alias(\"total_row\")).orderBy(\"nodename\").collect()\n",
    "            out={}\n",
    "            qids=set([r.real_queryid for r in noderow])\n",
    "            for r in noderow:\n",
    "                if r.nodename not in out:\n",
    "                    out[r.nodename]={c:0 for c in qids}\n",
    "                out[r.nodename][r.real_queryid]=r.total_row\n",
    "            return pandas.DataFrame(out).T\n",
    "    \n",
    "    def get_query_info(self,queryid):\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time stat info </b></font>\",))\n",
    "        tmp=self.get_query_time(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage stat info </b></font>\",))\n",
    "        display(self.get_stage_stat(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query plan </b></font>\",))\n",
    "        self.get_query_plan(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage hist info </b></font>\",))\n",
    "        self.show_Stages_hist(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time info </b></font>\",))\n",
    "        display(self.show_time_metric(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator and rowcount </b></font>\",))\n",
    "        display(self.get_metric_input_rowcnt(queryid=queryid))\n",
    "        display(self.get_metric_output_rowcnt(queryid=queryid))\n",
    "        \n",
    "    def get_app_info(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        display(HTML(f\"<font color=red size=7 face='Courier New'><b> {self.appid} </b></font>\",))\n",
    "        print(f\"http://sr525:18080/history/{self.appid}\")\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query time </b></font>\",))\n",
    "        tmp=self.get_query_time(**kwargs)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator count </b></font>\",))\n",
    "        pdf=self.getOperatorCount()\n",
    "        display(pdf.style.apply(background_gradient,\n",
    "               cmap='OrRd',\n",
    "               m=pdf.min().min(),\n",
    "               M=pdf.max().max(),\n",
    "               low=0,\n",
    "               high=1))\n",
    "        \n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator input row count </b></font>\",))\n",
    "        pdf=self.get_metric_input_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator output row count </b></font>\",))\n",
    "        pdf=self.get_metric_output_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        self.show_time_metric(**kwargs)\n",
    "        \n",
    "    def get_stage_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "\n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "            \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)).where(F.col(\"Event\")=='SparkListenerTaskEnd')\n",
    "        \n",
    "        inputsize = df.select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Stage ID\") \\\n",
    "                      .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read\"))\n",
    "        \n",
    "        return df.groupBy(\"Job ID\",\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.sum(F.col(\"Disk Bytes Spilled\"))/1024/1024/1024,1).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(F.col(\"Memory Bytes Spilled\"))/1024/1024/1024,1).alias(\"mem spilled\"),\n",
    "            F.round(F.sum(F.col(\"Local Bytes Read\"))/1024/1024/1024,1).alias(\"local read\"),\n",
    "            F.round(F.sum(F.col(\"Remote Bytes Read\"))/1024/1024/1024,1).alias(\"remote read\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Bytes Written\"))/1024/1024/1024,1).alias(\"shuffle write\"),\n",
    "            F.round(F.sum(F.col(\"Executor Deserialize Time\"))/1000,1).alias(\"deseri time\"),\n",
    "            F.round(F.sum(F.col(\"Fetch Wait Time\"))/1000,1).alias(\"fetch wait time\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Write Time\"))/1000000000,1).alias(\"shuffle write time\"),\n",
    "            F.round(F.sum(F.col(\"Result Serialization Time\"))/1000,1).alias(\"seri time\"),\n",
    "            F.round(F.sum(F.col(\"Getting Result Time\"))/1000,1).alias(\"get result time\"),\n",
    "            F.round(F.sum(F.col(\"JVM GC Time\"))/1000,1).alias(\"gc time\"),\n",
    "            F.round(F.sum(F.col(\"Executor CPU Time\"))/1000000000,1).alias(\"exe cpu time\")    \n",
    "            ).join(inputsize,on=[\"Stage ID\"],how=\"left\").orderBy(\"Stage ID\").toPandas()\n",
    "    \n",
    "    def get_metrics_by_node(self,node_name):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        metrics=self.queryplans.collect()\n",
    "        coalesce=[]\n",
    "        metricsid=[0]\n",
    "        def get_metric(root):\n",
    "            if root['nodeName']==node_name:\n",
    "                metricsid[0]=metricsid[0]+1\n",
    "                for l in root[\"metrics\"]:\n",
    "                    coalesce.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"],metricsid[0]])\n",
    "            for c in root[\"children\"]:\n",
    "                get_metric(c)\n",
    "        for c in metrics:\n",
    "            get_metric(c)\n",
    "\n",
    "        df=self.df.select(\"queryid\",\"real_queryid\",'Stage ID','Task ID','Job ID',F.explode(\"Accumulables\"))\n",
    "        df=df.select(\"*\",\"col.*\")\n",
    "        metricdf=spark.createDataFrame(coalesce)\n",
    "        metricdf=metricdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"Unit\").withColumnRenamed(\"_3\",\"metricName\").withColumnRenamed(\"_4\",\"nodeName\").withColumnRenamed(\"_5\",\"nodeID\")\n",
    "        df=df.join(metricdf,on=[\"ID\"],how=\"right\")\n",
    "        shufflemetric=set(l[2] for l in coalesce)\n",
    "        metricdfs=[df.where(F.col(\"Name\")==l).groupBy(\"real_queryid\",\"nodeID\").agg(F.min(\"Stage ID\").alias(\"Stage ID\"),F.mean(\"Update\").alias(l) if l.startswith(\"avg\") else F.sum(\"Update\").alias(l)) for l in shufflemetric]\n",
    "        return reduce(lambda x,y: x.join(y, on=['nodeID',\"Stage ID\",\"real_queryid\"],how=\"full\"),metricdfs)\n",
    "    \n",
    "    \n",
    "    def get_coalesce_batch_row_cnt(self,**kwargs):\n",
    "        stagesum=self.get_metrics_by_node(\"CoalesceBatches\")\n",
    "        \n",
    "        pandas.options.display.float_format = '{:,}'.format\n",
    "        \n",
    "        stagesum=stagesum.withColumnRenamed(\"number of output rows\",\"rows\")\n",
    "        \n",
    "        coalescedf = stagesum.orderBy(\"real_queryid\",'Stage ID').where(\"rows>4000\").toPandas()\n",
    "        \n",
    "        coalescedf[\"row/input_batch\"] = coalescedf[\"rows\"]/coalescedf[\"input_batches\"]\n",
    "        coalescedf[\"row/out_batch\"] = coalescedf[\"rows\"]/coalescedf[\"output_batches\"]\n",
    "        coalescedf['stage']=coalescedf[\"real_queryid\"].astype(str)+\"_\"+coalescedf['Stage ID'].astype(str)\n",
    "        \n",
    "        ax=coalescedf.plot(y=[\"row/input_batch\",\"row/out_batch\"],figsize=(30,8),style=\"-*\")\n",
    "        coalescedf.plot(ax=ax,y=['rows'],secondary_y=['rows'],style=\"k_\")\n",
    "        self.print_real_queryid(ax,coalescedf)\n",
    "        \n",
    "        return coalescedf\n",
    "    \n",
    "    def print_real_queryid(self,ax,dataset):\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "\n",
    "        real_queryid=list(dataset['real_queryid'])\n",
    "        s=real_queryid[0]\n",
    "        lastx=0\n",
    "        for idx,v in enumerate(real_queryid):\n",
    "            if v!=s:\n",
    "                xmin = xmax = idx-1+0.5\n",
    "                l = mlines.Line2D([xmin,xmax], [ymin,ymax],color=\"green\")\n",
    "                ax.add_line(l)\n",
    "                ax.text(lastx+(xmin-lastx)/2-0.25,ymin-(ymax-ymin)/20,f\"{s}\",size=20)\n",
    "                s=v\n",
    "                lastx=xmin\n",
    "\n",
    "    def get_shuffle_stat(self,**kwargs):\n",
    "        exchangedf=self.get_metrics_by_node(\"ColumnarExchange\")\n",
    "        exchangedf.cache()\n",
    "        exchangedf.count()\n",
    "        mapdf=exchangedf.where(\"totaltime_split is not null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"map_stageid\"),\"real_queryid\",\"totaltime_split\",\"shuffle write time\",'shuffle records written','data size','shuffle bytes written','shuffle bytes spilled','number of input rows')\n",
    "        reducerdf=exchangedf.where(\"totaltime_split is null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"reducer_stageid\"),\"real_queryid\",'local blocks read','local bytes read','avg read batch num rows','remote bytes read','records read','remote blocks read')\n",
    "        shuffledf=mapdf.join(reducerdf,on=[\"nodeID\",\"real_queryid\"],how=\"full\")\n",
    "        shuffle_pdf=shuffledf.where(\"`shuffle bytes written`>1000000000\").orderBy(\"real_queryid\",\"map_stageid\").toPandas()\n",
    "        shuffle_pdf[\"shuffle bytes written\"]=shuffle_pdf[\"shuffle bytes written\"]/1000000000\n",
    "        shuffle_pdf[\"data size\"]=shuffle_pdf[\"data size\"]/1000000000\n",
    "\n",
    "        ax=shuffle_pdf.plot(y=\"avg read batch num rows\",figsize=(30,8),style=\"b-*\",title=\"average batch size after split\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"split_ratio\"]=shuffle_pdf[\"records read\"]/shuffle_pdf['shuffle records written']\n",
    "        ax=shuffle_pdf.plot(y=[\"split_ratio\",\"records read\"],secondary_y=[\"records read\"],figsize=(30,8),style=\"-*\",title=\"Split Ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"compress_ratio\"]=shuffle_pdf[\"data size\"]/shuffle_pdf['shuffle bytes written']\n",
    "        ax=shuffle_pdf.plot(y=[\"shuffle bytes written\",\"compress_ratio\"],secondary_y=[\"compress_ratio\"],figsize=(30,8),style=\"-*\",title=\"compress ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shufflewritepdf=shuffle_pdf\n",
    "        ax=shufflewritepdf.plot.bar(y=[\"shuffle write time\",\"totaltime_split\"],stacked=True,figsize=(30,8),title=\"split time + shuffle write time vs. shuffle bytes written\")\n",
    "        ax=shufflewritepdf.plot(ax=ax,y=[\"shuffle bytes written\"],secondary_y=[\"shuffle bytes written\"],style=\"-*\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        shufflewritepdf.plot(x=\"shuffle bytes written\",y=[\"shuffle write time\",\"totaltime_split\"],figsize=(30,8),style=\"*\")\n",
    "        shufflewritepdf[\"avg batch size after split\"]=shufflewritepdf[\"shuffle bytes written\"]*1000/shufflewritepdf['records read']\n",
    "        ax=shufflewritepdf.plot(y=[\"shuffle bytes written\",\"avg batch size after split\"],secondary_y=[\"shuffle bytes written\"],figsize=(30,8),style=\"-*\",title=\"avg batch MB after split\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        shufflewritepdf[\"avg batch# per splitted partition\"]=shufflewritepdf['records read']/(shufflewritepdf['local blocks read']+shufflewritepdf['remote blocks read'])\n",
    "        ax=shufflewritepdf.plot(y=[\"avg batch# per splitted partition\",'records read'],secondary_y=['records read'],figsize=(30,8),style=\"-*\",title=\"avg batch# per splitted partition\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        return shufflewritepdf\n",
    "    \n",
    "    def get_stages_w_odd_partitions(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        return appals.df.where(\"Event='SparkListenerTaskEnd'\")\\\n",
    "                    .groupBy(\"Stage ID\",\"real_queryid\")\\\n",
    "                    .agg((F.sum(F.col('Finish Time')-F.col('Launch Time'))/1000).alias(\"elapsed time\"),\n",
    "                         F.count('*').alias('partitions'))\\\n",
    "                    .where(F.col(\"partitions\")%(appals.executor_cores*appals.executor_instances/appals.taskcpus)!=0)\\\n",
    "                    .orderBy(F.desc(\"elapsed time\")).toPandas()\n",
    "   \n",
    "    def get_scaned_column_v1(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"Scan arrow\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\":\")[0] for l in re.split(r'[<>]',s['metadata']['ReadSchema'])[1].split(\",\")])])\n",
    "        return alltable\n",
    "    \n",
    "    def get_scaned_column_v2(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"ColumnarBatchScan\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\"#\")[0] for l in re.split(r\"[\\[\\]]\",s['simpleString'])[1].split(\",\")])])\n",
    "        return alltable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notlist=['resource.executor.cores',\n",
    " 'spark.app.id',\n",
    " 'spark.app.initial.file.urls',\n",
    " 'spark.app.name',\n",
    " 'spark.app.startTime',\n",
    " 'spark.driver.port',\n",
    " 'spark.job.description',\n",
    " 'spark.jobGroup.id',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
    " 'spark.rdd.scope',\n",
    " 'spark.sql.execution.id',\n",
    " '__fetch_continuous_blocks_in_batch_enabled',\n",
    " 'spark.driver.appUIAddress'\n",
    " 'spark.driver.appUIAddress',\n",
    " 'spark.driver.host',\n",
    " 'spark.driver.appUIAddress',\n",
    " 'spark.driver.extraClassPath',\n",
    " 'spark.eventLog.dir',\n",
    " 'spark.executorEnv.CC',\n",
    " 'spark.executorEnv.LD_LIBRARY_PATH',\n",
    " 'spark.executorEnv.LD_PRELOAD',\n",
    " 'spark.executorEnv.LIBARROW_DIR',\n",
    " 'spark.files',\n",
    " 'spark.history.fs.logDirectory',\n",
    " 'spark.sql.warehouse.dir',\n",
    " 'spark.yarn.appMasterEnv.LD_PRELOAD',\n",
    " 'spark.yarn.dist.files'\n",
    "]\n",
    "def comp_spark_conf(app0,app1):   \n",
    "    pdf_sparkconf_0=app0.get_spark_config()\n",
    "    pdf_sparkconf_1=app1.get_spark_config()\n",
    "    pdfc=pdf_sparkconf_0.join(pdf_sparkconf_1,lsuffix=app0.appid[-8:],rsuffix=app1.appid[-8:])\n",
    "    pdfc[\"0\"+app0.appid[-8:]]=pdfc[\"0\"+app0.appid[-8:]].str.lower()\n",
    "    pdfc[\"0\"+app1.appid[-8:]]=pdfc[\"0\"+app1.appid[-8:]].str.lower()\n",
    "    \n",
    "    pdfc['comp']=(pdfc[\"0\"+app0.appid[-8:]]==pdfc[\"0\"+app1.appid[-8:]])\n",
    "    return pdfc.loc[(pdfc['comp']==False) & (~pdfc.index.isin(notlist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node log analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"host string, id string,taskid int, time double\", PandasUDFType.GROUPED_MAP)\n",
    "def collect_udf_time(pdf):\n",
    "    proxy_handler = request.ProxyHandler({})\n",
    "    opener = request.build_opener(proxy_handler)\n",
    "\n",
    "    rst=[]\n",
    "    for idx,l in pdf.iterrows():\n",
    "        ip=\"10.1.2.19\"+l['Host'][-1:]\n",
    "        execid=\"{:06d}\".format(int(l['Executor ID'])+1)\n",
    "        appid=l['appid']\n",
    "        url = f'http://{ip}:8042/node/containerlogs/container_{appid}_01_{execid}/yuzhou/stderr/?start=0'\n",
    "        # open the website with the opener\n",
    "        req = opener.open(url)\n",
    "        data = req.read().decode('utf8')\n",
    "        cnt=data.split(\"\\n\")\n",
    "        cnt_udf=[l.split(\" \") for l in cnt if l.startswith('start UDF') or l.startswith('stop UDF')]\n",
    "        unf_pdf=pandas.DataFrame(cnt_udf)\n",
    "        srst=unf_pdf.loc[:,[0,4,6]]\n",
    "        srst.columns=['id','taskid','time']\n",
    "        srst['host']=l['Host']\n",
    "        srst['taskid']=srst['taskid'].astype(int)\n",
    "        srst['time']=srst['time'].apply(lambda f: float(re.search('\\d+\\.\\d+',f).group(0)))\n",
    "        rst.append(srst)\n",
    "    return pandas.concat(rst)\n",
    "\n",
    "\n",
    "class App_Log_Analysis_Node_log(App_Log_Analysis):\n",
    "    def __init__(self, appid,jobids):\n",
    "        App_Log_Analysis.__init__(self, appid,jobids)\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        showcpu=kwargs['showcpu'] if 'showcpu' in kwargs else False\n",
    "        \n",
    "        appid=self.appid\n",
    "        events=self.df.toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=0\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=self.df.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "\n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        tskmap={}\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(\"task {:d} tid is {:s}.{:d}\".format(tsk,pid,t))\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "\n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['job id'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "                tskmap[tsk]={'pid':pid,'tid':pid+int(t)}\n",
    "\n",
    "        self.starttime=starttime\n",
    "        self.tskmap=tskmap\n",
    "\n",
    "        hostdf=self.df.select('Host','Executor ID',F.lit(appid[len('application_'):]).alias('appid')).distinct().orderBy('Host')\n",
    "        rst=hostdf.groupBy('Host').apply(collect_udf_time)\n",
    "        rst.cache()\n",
    "        start_df=rst.where(\"id='start'\").select(F.col('taskid').alias('start_taskid'),F.col('time').alias(\"starttime\"))\n",
    "        stop_df=rst.where(\"id='stop'\").select('taskid',F.col('time').alias(\"stop_time\"))\n",
    "        df=start_df.join(stop_df, on=[start_df.start_taskid==stop_df.taskid,stop_df['stop_time']>=start_df['starttime']],how='left').groupBy('taskid','starttime').agg(F.min('stop_time').alias('stop_time'))\n",
    "        pdf=df.toPandas() \n",
    "        for idx,l in pdf.iterrows():\n",
    "                trace_events.append({\n",
    "                     'tid':self.tskmap[l['taskid']]['tid'],\n",
    "                     'ts':l['starttime']*1000-self.starttime,\n",
    "                     'dur':(l['stop_time']-l['starttime'])*1000,                \n",
    "                     'pid':self.tskmap[l['taskid']]['pid'],\n",
    "                     'ph':'X',\n",
    "                     'name':'udf'})\n",
    "        \n",
    "        return [json.dumps(l) for l in trace_events]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class App_Log_Analysis_Node_log(App_Log_Analysis):\n",
    "    def __init__(self, appid,jobids):\n",
    "        App_Log_Analysis.__init__(self, appid,jobids)\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        showcpu=kwargs['showcpu'] if 'showcpu' in kwargs else False\n",
    "        \n",
    "        appid=self.appid\n",
    "        events=self.df.toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=0\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=self.df.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "\n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        tskmap={}\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(\"task {:d} tid is {:s}.{:d}\".format(tsk,pid,t))\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "\n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['job id'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "                tskmap[tsk]={'pid':pid,'tid':pid+int(t)}\n",
    "\n",
    "        self.starttime=starttime\n",
    "        self.tskmap=tskmap\n",
    "\n",
    "        hostdf=self.df.select('Host','Executor ID',F.lit(appid[len('application_'):]).alias('appid')).distinct().orderBy('Host')\n",
    "        rst=hostdf.groupBy('Host').apply(collect_udf_time)\n",
    "        rst.cache()\n",
    "        start_df=rst.where(\"id='start'\").select(F.col('taskid').alias('start_taskid'),F.col('time').alias(\"starttime\"))\n",
    "        stop_df=rst.where(\"id='stop'\").select('taskid',F.col('time').alias(\"stop_time\"))\n",
    "        df=start_df.join(stop_df, on=[start_df.start_taskid==stop_df.taskid,stop_df['stop_time']>=start_df['starttime']],how='left').groupBy('taskid','starttime').agg(F.min('stop_time').alias('stop_time'))\n",
    "        pdf=df.toPandas() \n",
    "        for idx,l in pdf.iterrows():\n",
    "                trace_events.append({\n",
    "                     'tid':self.tskmap[l['taskid']]['tid'],\n",
    "                     'ts':l['starttime']*1000-self.starttime,\n",
    "                     'dur':(l['stop_time']-l['starttime'])*1000,                \n",
    "                     'pid':self.tskmap[l['taskid']]['pid'],\n",
    "                     'ph':'X',\n",
    "                     'name':'udf'})\n",
    "        \n",
    "        return [json.dumps(l) for l in trace_events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class App_Log_Analysis_Node_Log_Uni(App_Log_Analysis):\n",
    "    def __init__(self, file,jobids):\n",
    "        App_Log_Analysis.__init__(self, file,jobids)\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        showcpu=False\n",
    "        \n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "\n",
    "        showdf=self.df #self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "\n",
    "        events=showdf.drop(\"Accumulables\",\"Stage IDs\").orderBy(\"Launch Time\",\"Finish Time\").toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=0\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "\n",
    "        applog=os.path.splitext(self.file)[0]+\".stdout\"\n",
    "        logdfs=[]\n",
    "        if fs.exists(applog):\n",
    "            logdata=sc.textFile(os.path.splitext(self.file)[0]+\".stdout\",84)\n",
    "            logdf=logdata.mapPartitions(splits).toDF()\n",
    "            logdfs.append(logdf)\n",
    "\n",
    "        p=os.path.split(self.file)\n",
    "        for c in shownodes:\n",
    "            f=p[0]+\"/\"+c+\"/xgbtck.txt\"\n",
    "            if fs.exists(f):\n",
    "                logdata=sc.textFile(f,84)\n",
    "                logdf=logdata.mapPartitions(splits).toDF()\n",
    "                logdfs.append(logdf)\n",
    "        logdf=reduce(lambda l,r: l.concat(r),logdfs)\n",
    "        logdf=logdf.cache()\n",
    "        logdf.count()\n",
    "\n",
    "        firstrow=logdf.limit(1).collect()\n",
    "\n",
    "        for c in logdf.columns:\n",
    "            if firstrow[0][c]!=\"xgbtck\":\n",
    "                logdf=logdf.drop(c)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        usefulc=[\"xgbtck\",\"event\",\"ts\",\"elapsed\",\"threadid\",\"taskid\"]\n",
    "        for i in range(0,len(usefulc)):\n",
    "            logdf=logdf.withColumnRenamed(logdf.columns[i],usefulc[i])\n",
    "\n",
    "        logdf=logdf.where(F.col(\"event\").isin(['load_library','data_load','data_convert']))\n",
    "        \n",
    "        task_thread=logdf.where(\"event='data_convert'\").select(F.col(\"taskid\").astype(IntegerType()),F.col(\"threadid\").astype(IntegerType())).distinct().toPandas().set_index('taskid').to_dict('index')\n",
    "        #task_thread={}\n",
    "\n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        tskmap={}\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "\n",
    "                tidarr=[tsk,stime]\n",
    "\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=tidarr\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=tidarr\n",
    "                #print(\"task {:d} tid is {:s}.{:d}\".format(tsk,pid,t))\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "\n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]):\n",
    "                    if (tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2) or \\\n",
    "                        (tsk in task_thread and tids[ps][0] in task_thread and task_thread[tsk][\"threadid\"]==task_thread[tids[ps][0]][\"threadid\"]):\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['Job ID'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "                tskmap[tsk]={'pid':pid,'tid':pid+int(t)}\n",
    "\n",
    "        self.starttime=starttime\n",
    "        self.tskmap=tskmap\n",
    "\n",
    "        tskmapdf = spark.createDataFrame(pandas.DataFrame(self.tskmap).T.reset_index())\n",
    "        logdf=logdf.withColumn(\"ts\",F.col(\"ts\").astype(LongType()))\n",
    "        logdf=logdf.withColumn(\"taskid\",F.col(\"taskid\").astype(LongType()))\n",
    "        logdf=logdf.withColumnRenamed(\"event\",'type')\n",
    "        mgd=logdf.join(tskmapdf,on=(F.col('taskid')==F.col(\"index\")),how=\"right\")\n",
    "        rstdf=mgd.select(F.col('tid').alias(\"tid\"),\n",
    "          (F.round(F.col('ts')-F.lit(self.starttime),3)).alias(\"ts\"),\n",
    "          F.round(F.col(\"elapsed\"),3).alias(\"dur\"),\n",
    "           F.lit(F.col('pid')).alias(\"pid\"),\n",
    "           F.lit(\"X\").alias(\"ph\"),\n",
    "           F.col(\"type\").alias(\"name\")\n",
    "           ).where(F.col(\"ts\").isNotNull()).orderBy('ts')\n",
    "\n",
    "        #        logdf=logdf.withColumn(\"type\",F.substring_index(\"event\",\"_\",1))\n",
    "        #        window= Window.partitionBy(logdf['taskid']).orderBy(\"type\",\"ts\")\n",
    "        #        logdfx=logdf.select(\"taskid\",\"event\",\"type\",\"ts\",F.lag('ts',1).over(window).alias(\"last\"),F.lag('rownum',1).over(window).alias(\"rownum\")).orderBy(\"taskid\",\"ts\").where(\"event like '%end'\")\n",
    "\n",
    "\n",
    "        output=[json.dumps(l) for l in trace_events]\n",
    "        output.extend(rstdf.toJSON().collect())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perf trace analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b4df375ac06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPerf_trace_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msar_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mAnalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msar_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Analysis' is not defined"
     ]
    }
   ],
   "source": [
    "def split_trace(x):\n",
    "    fi=[]\n",
    "    for l in x:\n",
    "        rst1=re.search(r\"^(\\d+\\.\\d+).*sched:(sched_switch):.+:(\\d+) \\[\\d+\\] (\\S+) ==> .+:(\\d+) \"\"\",l)\n",
    "        rst2=re.search(r\"(\\d+\\.\\d+) \\( +(\\d+\\.\\d+) +ms\\):[^/]+/(\\d+) (recvfrom|sendto)\\(fd: \\d+<\\S+:\\[\\d+\\]>, \\S+: 0x[a-f0-9]+, \\S+: (\\d+)\",l)\n",
    "        rst3=re.search(r\"(\\d+\\.\\d+) \\( +\\): [^/]+/(\\d+) (recvfrom|sendto)\\(fd: \\d+<\\S+:\\[\\d+\\]>, \\S+: 0x[a-f0-9]+, \\S+: (\\d+)\",l)\n",
    "        rst4=re.search(r\"(\\d+\\.\\d+) \\( *(\\d+\\.\\d+) ms\\): [^/]+/(\\d+)  ... \\[continued\\]: (sendto|recvfrom|poll)\",l)\n",
    "        rst5=re.search(r\"(\\d+\\.\\d+) \\( +(\\d+\\.\\d+) +ms\\): [^/]+/(\\d+) (poll)\",l)\n",
    "        rst6=re.search(r\"(\\d+\\.\\d+) \\( +\\): [^/]+/(\\d+) (poll)\",l)\n",
    "\n",
    "        rstx=re.search(r\"(\\d+\\.\\d+)*sched:(sched_switch):.*prev_pid=(\\d+).*prev_state=(\\S+) ==> .*next_pid=(\\d+)\"\"\",l)\n",
    "        if not rst1:\n",
    "            rst1=rstx\n",
    "        \n",
    "        if rst1:\n",
    "            fi.append((rst1.group(1),rst1.group(2),rst1.group(3),rst1.group(4),rst1.group(5))) #time, switch, src, status, dst\n",
    "        elif rst2:\n",
    "            fi.append((rst2.group(1),rst2.group(4),rst2.group(3),rst2.group(2),rst2.group(5))) #time, sed/rcv, pid, ms, size \n",
    "        elif rst3:\n",
    "            fi.append((rst3.group(1),rst3.group(3),rst3.group(2),0, rst3.group(4)))             #time, sed/rcv, pid, 0, size\n",
    "        elif rst4:\n",
    "            fi.append((rst4.group(1),rst4.group(4),rst4.group(3),rst4.group(2), 0))              #time, sed/rcv, pid, ms, 0\n",
    "        elif rst5:\n",
    "            fi.append((rst5.group(1),rst5.group(4),rst5.group(3),rst5.group(2), 0))              #time, sed/rcv, pid, ms, 0\n",
    "        elif rst6:\n",
    "            fi.append((rst6.group(1),rst6.group(3),rst6.group(2),0, 0))              #time, sed/rcv, pid, ms0, 0\n",
    "        elif not re.match(r\"^ +?\",l):\n",
    "            fi.append((0,l,'','',''))\n",
    "    return iter(fi)\n",
    "                  \n",
    "\n",
    "\n",
    "class Perf_trace_analysis(Analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Analysis.__init__(self,sar_file)\n",
    "        self.starttime=None\n",
    "        \n",
    "    def load_data(self):\n",
    "        sardata=sc.textFile(self.file)\n",
    "        sardf=sardata.mapPartitions(split_trace).toDF()\n",
    "        display(sardf.where(\"_1=0\").limit(5).collect())\n",
    "        sardf=sardf.withColumn(\"_1\",F.col(\"_1\").astype(DoubleType()))\n",
    "        sardf=sardf.where(\"_1>0\")\n",
    "        starttime=sardf.agg(F.min(\"_1\")).collect()[0][0]\n",
    "        if self.starttime is None:\n",
    "            self.starttime=(float(starttime))\n",
    "        else:\n",
    "            paths=os.path.split(self.file)\n",
    "            if fs.exists(paths[0]+\"/uptime.txt\"):\n",
    "                with fs.open(paths[0]+\"/uptime.txt\") as f:\n",
    "                    strf=f.read().decode('ascii')\n",
    "                    print(\"input starttime:\",self.starttime,\"uptime:\",float(strf)*1000,\"record starttime:\",starttime)\n",
    "                    self.starttime=self.starttime-float(strf)*1000\n",
    "            else:\n",
    "                print(\"uptime.txt isn't found, wrong\")\n",
    "                return\n",
    "            \n",
    "        self.df=sardf\n",
    "        return sardf\n",
    "\n",
    "    def generate_sched_view_list(self,id=0,**kwargs):\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        starttime=starttime+kwargs.get(\"sched_time_offset\",0)\n",
    "        print(\"offset time\",starttime)\n",
    "        \n",
    "        swdf=sardf.where(\"_2='sched_switch'\")\n",
    "        \n",
    "        cputhreshold=kwargs.get(\"cpu_threshold\",0.1)\n",
    "        sched_cnt = kwargs.get(\"sched_cnt\",10)\n",
    "        \n",
    "        pidstat_tids=kwargs.get(\"pidstat_tids\",None)\n",
    "        pidstat_tids_txt=kwargs.get(\"pidstat_tids_txt\",\"sched_threads.txt\")\n",
    "        \n",
    "        if pidstat_tids:\n",
    "            if type(pidstat_tids) is list:\n",
    "                tids=pidstat_tids\n",
    "            else:\n",
    "                tids=[re.split(r'\\s+',t) for t in pidstat_tids.split(\"\\n\")]\n",
    "                tids=[t[3] for t in tids if len(t)>4]\n",
    "        else:\n",
    "            paths=os.path.split(self.file)\n",
    "            if fs.exists(paths[0]+\"/\"+pidstat_tids_txt):\n",
    "                with fs.open(paths[0]+\"/\"+pidstat_tids_txt) as f:\n",
    "                    tids=[l.strip() for l in f.read().decode('ascii').split(\"\\n\") if len(l)>0] \n",
    "            else:\n",
    "                print(\"Wrong, no pidstat_tids args and no sched_threads.txt file\")\n",
    "                return []\n",
    "        tidcnt=swdf.where(F.col(\"_5\").isin(tids)).groupBy(\"_5\").count()\n",
    "        tidm10=tidcnt.where(\"count>{:d}\".format(sched_cnt)).select(\"_5\").collect()\n",
    "        rtids=[t[0] for t in tidm10]\n",
    "        rtiddf=swdf.where(F.col(\"_5\").isin(rtids) | F.col(\"_3\").isin(rtids))\n",
    "        rtiddf=rtiddf.withColumn(\"_1\",F.col(\"_1\").astype(DoubleType())-starttime)\n",
    "        rtiddf=rtiddf.withColumn(\"_3\",F.col(\"_3\").astype(IntegerType()))\n",
    "        rtiddf=rtiddf.withColumn(\"_5\",F.col(\"_5\").astype(IntegerType()))\n",
    "        rtiddf=rtiddf.withColumn(\"_1\",F.round(F.col(\"_1\"),3))\n",
    "        rtidcol=rtiddf.collect()\n",
    "        tidmap={}\n",
    "        tidtotal={}\n",
    "        for t in rtids:\n",
    "            tidmap[int(t)]=0\n",
    "            tidtotal[int(t)]=0\n",
    "        trace_events=[]\n",
    "        mintime=rtidcol[0][\"_1\"]\n",
    "        maxtime=0\n",
    "        for r in rtidcol:\n",
    "            if r[\"_3\"] in tidtotal:\n",
    "                tidtotal[r[\"_3\"]]=tidtotal[r[\"_3\"]]+r[\"_1\"]-tidmap[r[\"_3\"]]\n",
    "                tidmap[r[\"_3\"]]=r[\"_1\"]\n",
    "                maxtime=r[\"_1\"]\n",
    "            if r[\"_5\"] in tidmap:\n",
    "                tidmap[r[\"_5\"]]=r[\"_1\"]\n",
    "        for r in rtidcol:\n",
    "            if r[\"_3\"] in tidmap and tidtotal[r[\"_3\"]]/(maxtime-mintime)>cputhreshold:\n",
    "                trace_events.append({\n",
    "                    'tid':r[\"_3\"],\n",
    "                     'ts':tidmap[r[\"_3\"]],\n",
    "                     'pid':id,\n",
    "                     'ph':'X',\n",
    "                     'dur':round(r[\"_1\"]-tidmap[r[\"_3\"]],3),\n",
    "                     'name':r[\"_4\"]\n",
    "                })\n",
    "\n",
    "                tidmap[r[\"_3\"]]=r[\"_1\"]\n",
    "            if r[\"_5\"] in tidmap:\n",
    "                tidmap[r[\"_5\"]]=r[\"_1\"]\n",
    "        return [json.dumps(l) for l in trace_events]\n",
    "\n",
    "    def generate_nic_view_list(self,id=0,**kwargs):\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        starttime=starttime+kwargs.get(\"sched_time_offset\",0)\n",
    "        print(\"offset time\",starttime)\n",
    "        \n",
    "        nicdf=sardf.where(\"_2<>'sched_switch'\")\n",
    "        cntdf=nicdf.where(\"_2='continued'\")\n",
    "        cntdf=cntdf.select(\"_1\",\"_3\",\"_4\").withColumnRenamed(\"_4\",\"cnt_4\")\n",
    "        nicdf=nicdf.join(cntdf,on=[\"_1\",\"_3\"],how=\"leftouter\")\n",
    "        nicdf=nicdf.where(\"_2<>'continued'\")\n",
    "        nicdf=nicdf.select(F.col(\"_1\"),F.col(\"_2\"),F.col(\"_3\"),F.when(F.col(\"cnt_4\").isNull(), F.col(\"_4\")).otherwise(F.col(\"cnt_4\")).alias(\"_4\"),F.col(\"_5\"))\n",
    "        nicdf=nicdf.withColumn(\"_1\",F.col(\"_1\").astype(DoubleType())-starttime)\n",
    "        nicdf=nicdf.withColumn(\"_3\",F.col(\"_3\").astype(IntegerType()))\n",
    "        nicdf=nicdf.withColumn(\"_5\",F.col(\"_5\").astype(IntegerType()))\n",
    "        nicdf=nicdf.withColumn(\"_1\",F.col(\"_1\").astype(IntegerType()))\n",
    "        nicdf=nicdf.withColumn(\"_4\",F.col(\"_4\").astype(DoubleType()))\n",
    "        nicdf=nicdf.withColumn(\"_4\",F.col(\"_4\").astype(LongType()))\n",
    "        return nicdf.select(\n",
    "                F.col(\"_3\").alias('tid'),\n",
    "                (F.col(\"_1\")).alias('ts'),\n",
    "                F.lit(0).alias('pid'),\n",
    "                F.lit('X').alias('ph'),\n",
    "                F.col(\"_4\").alias('dur'),\n",
    "                F.col(\"_2\").alias('name'),\n",
    "                F.struct(\n",
    "                    F.col(\"_5\").alias(\"size\")\n",
    "                ).alias('args')\n",
    "            ).toJSON().collect()\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0,**kwargs):\n",
    "        trace_events=Analysis.generate_trace_view_list(self,id,**kwargs)\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        \n",
    "        events=self.generate_sched_view_list(id,**kwargs)\n",
    "        events.extend(self.generate_nic_view_list(id,**kwargs))\n",
    "        events.extend(trace_events)\n",
    "        \n",
    "#        events.extend(nicdf.where(\"_5>1000 and _2='sendto'\").select(\n",
    "#                 F.lit(0).alias('tid'),\n",
    "#                F.col(\"_1\").alias('ts'),\n",
    "#                F.lit(0).alias('pid'),\n",
    "#                F.lit('i').alias('ph'),\n",
    "#                F.col(\"_2\").alias('name'),\n",
    "#                F.lit(\"g\").alias(\"s\")\n",
    "#            ).toJSON().collect())\n",
    "\n",
    "\n",
    "        return events\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sar analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     167
    ]
   },
   "outputs": [],
   "source": [
    "def splits(x):\n",
    "    fi=[]\n",
    "    for l in x:\n",
    "        li=re.split(r'\\s+',l)\n",
    "        for j in range(len(li),118):\n",
    "            li.append('')\n",
    "        fi.append(li)\n",
    "    return iter(fi)\n",
    "\n",
    "class Sar_analysis(Analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        sardata=sc.textFile(self.file)\n",
    "        sardf=sardata.mapPartitions(splits).toDF()\n",
    "        sardf=sardf.where(\"_1<>'Average:'\")\n",
    "        \n",
    "        colstart=1;\n",
    "        ampm=sardf.where(\"_2='AM' or _2='PM'\").count()\n",
    "        if ampm==0:\n",
    "            for i in range(len(sardf.columns),1,-1):\n",
    "                sardf=sardf.withColumnRenamed(f'_{i}',f'_{i+1}')\n",
    "            self.timeformat='yyyy-MM-dd HH:mm:ss '\n",
    "            sardf=sardf.withColumn('_2',F.lit(''))\n",
    "            #print('no PM/AM')\n",
    "            colstart=1\n",
    "        else:\n",
    "            self.timeformat='yyyy-MM-dd hh:mm:ss a'\n",
    "            colstart=2\n",
    "            #print('with PM/AM')\n",
    "        \n",
    "        f=fs.open(self.file)\n",
    "        t=f.readline()\n",
    "        t=f.readline()\n",
    "        while len(t)==1:\n",
    "            t=f.readline()\n",
    "        cols=t.decode('ascii')\n",
    "        li=re.split(r'\\s+',cols)\n",
    "        ci=3;\n",
    "        for c in li[colstart:]:\n",
    "            sardf=sardf.withColumnRenamed(f\"_{ci}\",c)\n",
    "            ci=ci+1\n",
    "            \n",
    "        sardf=sardf.where(F.col(li[-2])!=li[-2]).where(F.col(\"_1\")!=F.lit(\"Linux\"))        \n",
    "        \n",
    "        sardf.cache()\n",
    "        self.df=sardf\n",
    "        \n",
    "        self.sarversion=\"\"\n",
    "        paths=os.path.split(self.file)\n",
    "        if fs.exists(paths[0]+\"/sarv.txt\"):\n",
    "            with fs.open(paths[0]+\"/sarv.txt\") as f:\n",
    "                allcnt = f.read().decode('ascii')\n",
    "                #print(allcnt)\n",
    "                self.sarversion=allcnt.split(\"\\n\")[0].split(\" \")[2]\n",
    "        \n",
    "        return sardf\n",
    "\n",
    "    def col_df(self,cond,colname,args,slaver_id=0, thread_id=0):\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        cpudf=sardf.where(cond)\n",
    "        #cpudf.select(F.date_format(F.from_unixtime(F.lit(starttime/1000)), 'yyyy-MM-dd HH:mm:ss').alias('starttime'),'_1').show(1)\n",
    "\n",
    "        cpudf=cpudf.withColumn('time',F.unix_timestamp(F.concat_ws(' ',F.date_format(F.from_unixtime(F.lit(starttime/1000)), 'yyyy-MM-dd'),F.col('_1'),F.col('_2')),self.timeformat))\n",
    "\n",
    "        cols=cpudf.columns\n",
    "                \n",
    "        cpudf=cpudf.groupBy('time').agg(\n",
    "            F.sum(F.when(F.col(cols[1]).rlike('^\\d+(\\.\\d+)*$'),F.col(cols[1]).astype(FloatType())).otherwise(0)).alias(cols[1]),\n",
    "            F.sum(F.when(F.col(cols[2]).rlike('^\\d+(\\.\\d+)*$'),F.col(cols[2]).astype(FloatType())).otherwise(0)).alias(cols[2]),\n",
    "            *[F.sum(F.col(c)).alias(c) for c in cols[3:] if not c.startswith(\"_\") and c!=\"\" and c!=\"time\"]\n",
    "        )\n",
    "        \n",
    "        traces=cpudf.orderBy(F.col(\"time\")).select(\n",
    "                F.lit(thread_id).alias('tid'),\n",
    "                (F.expr(\"time*1000\")-F.lit(self.starttime)).astype(IntegerType()).alias('ts'),\n",
    "                F.lit(slaver_id).alias('pid'),\n",
    "                F.lit('C').alias('ph'),\n",
    "                F.lit(colname).alias('name'),\n",
    "                args(cpudf).alias('args')\n",
    "            ).toJSON().collect()\n",
    "        return traces\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        return trace_events\n",
    "\n",
    "    def get_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "class Sar_cpu_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        self.df=self.df.withColumn(\"%iowait\",F.when(F.col(\"%iowait\")>100,F.lit(100)).otherwise(F.col(\"%iowait\")))\n",
    "        \n",
    "        trace_events.extend(self.col_df(\"CPU='all'\",             \"all cpu%\",    lambda l: F.struct(\n",
    "                                                                                                    F.floor(F.col('%user').astype(FloatType())).alias('user'),\n",
    "                                                                                                    F.floor(F.col('%system').astype(FloatType())).alias('system'),\n",
    "                                                                                                    F.floor(F.col('%iowait').astype(FloatType())).alias('iowait')\n",
    "                                                                                                    ),                            id, 0))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"sort_index \":0}}))\n",
    "        \n",
    "        return trace_events    \n",
    "    def get_stat(sar_cpu,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_cpu)\n",
    "        \n",
    "        cpuutil=sar_cpu.df.where(\"CPU='all'\").groupBy(\"_1\").agg(*[F.mean(F.col(l).astype(FloatType())).alias(l) for l in [\"%user\",\"%system\",\"%iowait\"]]).orderBy(\"_1\")\n",
    "        cnt=cpuutil.count()\n",
    "        user_morethan_90=cpuutil.where(\"`%user`>0.9\").count()\n",
    "        kernel_morethan_10=cpuutil.where(\"`%system`>0.1\").count()\n",
    "        iowait_morethan_10=cpuutil.where(\"`%iowait`>0.1\").count()\n",
    "        out=[['%user>90%',user_morethan_90/cnt],['%kernel>10%',kernel_morethan_10/cnt],[\"%iowait>10%\",iowait_morethan_10/cnt]]\n",
    "        avgutil=cpuutil.agg(*[F.mean(l).alias(l) for l in [\"%user\",\"%system\",\"%iowait\"]]).collect()\n",
    "        out.extend([[\"avg \" + l,avgutil[0][l]] for l in [\"%user\",\"%system\",\"%iowait\"]])\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_cpu.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "class Sar_mem_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        Sar_analysis.load_data(self)\n",
    "        if self.sarversion in (\"12.2.0\",\"12.5.4\"):\n",
    "            self.df=self.df.withColumn(\"kbrealused\",F.col(\"kbmemused\"))\n",
    "        else:\n",
    "            # sar 10.1.5, sar 11.6.1\n",
    "            self.df=self.df.withColumn(\"kbrealused\",F.col(\"kbmemused\")-F.col(\"kbcached\")-F.col(\"kbbuffers\"))\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        \n",
    "        trace_events.extend(self.col_df(F.col('kbmemfree').rlike('^\\d+$'),\"mem % \",      lambda l: F.struct(F.floor(l['kbcached']*l['%memused']/l['kbmemused']).alias('cached'),  # kbcached / (kbmemfree+kbmemused)\n",
    "                                                                                                       F.floor(l['kbbuffers']*l['%memused']/l['kbmemused']).alias('buffered'),# kbbuffers / (kbmemfree+kbmemused)\n",
    "                                                                                                       F.floor(l['kbrealused']*l['%memused']/l['kbmemused']).alias('used')), # (%memused- kbcached-kbbuffers )/  (kbmemfree+kbmemused)\n",
    "                                          id,1))\n",
    "        #trace_events.extend(self.col_df(self.df._3.rlike('^\\d+$'),\"mem cmt % \",  lambda l: F.struct(F.floor(l._8*F.lit(100)/(l._3+l._4)).alias('commit/phy'),\n",
    "        #                                                                                                   F.floor(l._10-l._8*F.lit(100)/(l._3+l._4)).alias('commit/all')),                                                             id))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":1,\"args\":{\"sort_index \":1}}))\n",
    "        return trace_events    \n",
    "    def get_stat(sar_mem,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_mem)\n",
    "        \n",
    "        memutil=sar_mem.df.where(F.col('kbmemfree').rlike('^\\d+$')).select(F.floor(F.col('kbcached').astype(FloatType())*F.lit(100)*F.col('%memused')/F.col('kbmemused')).alias('cached'),  \n",
    "                                                                                   F.floor(F.col('kbbuffers').astype(FloatType())*F.lit(100)*F.col('%memused')/F.col('kbmemused')).alias('buffered'),\n",
    "                                                                                   F.floor(F.col('kbrealused').astype(FloatType())*F.lit(100)*F.col('%memused')/F.col('kbmemused')).alias('used'))\n",
    "        memsum=memutil.summary().toPandas()\n",
    "        memsum=memsum.set_index(\"summary\")\n",
    "        out=[\n",
    "            [[l + ' mean',float(memsum[l][\"mean\"])],\n",
    "            [l + ' 75%',float(memsum[l][\"75%\"])],\n",
    "            [l + ' max',float(memsum[l][\"max\"])]] for l in [\"cached\",\"used\"]]\n",
    "        out=[*out[0],*out[1]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_mem.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "\n",
    "class Sar_disk_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "        \n",
    "    def load_data(self):\n",
    "        Sar_analysis.load_data(self)\n",
    "        \n",
    "        self.df=self.df.withColumn(\"%util\",F.col(\"%util\").astype(IntegerType()))\n",
    "        used_disk=self.df.groupBy(\"DEV\").agg(F.max(F.col(\"%util\")).alias(\"max_util\"),F.mean(\"%util\").alias(\"avg_util\")).where(F.col(\"max_util\")>10).collect()\n",
    "        self.df=self.df.where(F.col(\"DEV\").isin([l['DEV'] for l in used_disk]))\n",
    "        #print(\"used disks with its max util% and avg util% are: \")\n",
    "        #display([(l['DEV'],l[\"max_util\"],l[\"avg_util\"]) for l in used_disk])\n",
    "        \n",
    "        if \"rd_sec/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"rkB/s\",F.expr('cast(`rd_sec/s` as float)*512/1024'))\n",
    "        if \"wr_sec/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"wkB/s\",F.expr('cast(`wr_sec/s` as float)*512/1024'))\n",
    "        \n",
    "        if \"areq-sz\" in self.df.columns:\n",
    "            self.df=self.df.withColumnRenamed(\"areq-sz\",\"avgrq-sz\")\n",
    "        if \"aqu-sz\" in self.df.columns:\n",
    "            self.df=self.df.withColumnRenamed(\"aqu-sz\",\"avgqu-sz\")\n",
    "            \n",
    "        if \"rkB/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"rkB/s\",F.expr('cast(`rkB/s` as float)/1024'))\n",
    "        if \"wkB/s\" in self.df.columns:\n",
    "            self.df=self.df.withColumn(\"wkB/s\",F.expr('cast(`wkB/s` as float)/1024'))\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "\n",
    "        disk_prefix=kwargs.get('disk_prefix',\"\")\n",
    "        \n",
    "        \n",
    "        devcnt=self.df.where(\"DEV like '\"+disk_prefix+\"%'\").select(\"DEV\").distinct().count()\n",
    "        \n",
    "        trace_events.extend(self.col_df(\"DEV like '\"+disk_prefix+\"%'\",      \"disk b/w\",       lambda l: F.struct(\n",
    "                                                                                                            F.floor(F.col(\"rKB/s\")).alias('read'),\n",
    "                                                                                                            F.floor(F.col(\"wKB/s\")).alias('write')),id, 2))\n",
    "        trace_events.extend(self.col_df(\"DEV like '\"+disk_prefix+\"%'\",      \"disk%\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"%util\")/F.lit(devcnt)).alias('%util')),id, 3))\n",
    "        trace_events.extend(self.col_df(\"DEV like '\"+disk_prefix+\"%'\",      \"req size\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"avgrq-sz\")/F.lit(devcnt)).alias('avgrq-sz')),id, 4))\n",
    "        trace_events.extend(self.col_df(\"DEV like '\"+disk_prefix+\"%'\",      \"queue size\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"avgqu-sz\")/F.lit(512*devcnt/1024)).alias('avgqu-sz')),id, 5))\n",
    "        trace_events.extend(self.col_df(\"DEV like '\"+disk_prefix+\"%'\",      \"await\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"await\")/F.lit(devcnt)).alias('await')),id, 6))\n",
    "        \n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":2,\"args\":{\"sort_index \":2}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":3,\"args\":{\"sort_index \":3}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":4,\"args\":{\"sort_index \":4}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":5,\"args\":{\"sort_index \":5}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":6,\"args\":{\"sort_index \":6}}))\n",
    "        return trace_events    \n",
    "\n",
    "    def get_stat(sar_disk,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_disk)\n",
    "        disk_prefix=kwargs.get('disk_prefix',\"\")\n",
    "\n",
    "        diskutil=sar_disk.df.where(f\"DEV like '{disk_prefix}%'\").groupBy(\"_1\").agg(F.mean(F.col(\"%util\").astype(FloatType())).alias(\"%util\")).orderBy(\"_1\")\n",
    "        totalcnt=diskutil.count()\n",
    "        time_morethan_90=diskutil.where(F.col(\"%util\")>90).count()/totalcnt\n",
    "        avgutil=diskutil.agg(F.mean(\"%util\")).collect()\n",
    "        out=[[\"avg disk util\",avgutil[0][\"avg(%util)\"]],\n",
    "            [\"time more than 90%\", time_morethan_90]]\n",
    "        diskbw=sar_disk.df.where(f\"DEV like '{disk_prefix}%'\").groupBy(\"_1\").agg(F.sum(F.col(\"rKB/s\")).alias(\"rd_bw\"),F.sum(F.col(\"wKB/s\")).alias(\"wr_bw\"))\n",
    "        bw=diskbw.agg(F.sum(\"rd_bw\").alias(\"total read\"),F.sum(\"wr_bw\").alias(\"total write\"),F.mean(\"rd_bw\").alias(\"read bw\"),F.mean(\"wr_bw\").alias(\"write bw\"),F.max(\"rd_bw\").alias(\"max read\"),F.max(\"wr_bw\").alias(\"max write\")).collect()\n",
    "        maxread=bw[0][\"max read\"]\n",
    "        maxwrite=bw[0][\"max write\"]\n",
    "        rdstat, wrstat = diskbw.stat.approxQuantile(['rd_bw','wr_bw'],[0.75,0.95,0.99],0.0)\n",
    "        time_rd_morethan_95 = diskbw.where(F.col(\"rd_bw\")>rdstat[1]).count()/totalcnt\n",
    "        time_wr_morethan_95 = diskbw.where(F.col(\"wr_bw\")>rdstat[1]).count()/totalcnt\n",
    "        out.append(['total read (G)' , bw[0][\"total read\"]/1024])\n",
    "        out.append(['total write (G)', bw[0][\"total write\"]/1024])\n",
    "        out.append(['avg read bw (MB/s)', bw[0][\"read bw\"]])\n",
    "        out.append(['avg write bw (MB/s)', bw[0][\"write bw\"]])\n",
    "        out.append(['read bw %75', rdstat[0]])\n",
    "        out.append(['read bw %95', rdstat[1]])\n",
    "        out.append(['read bw max', rdstat[2]])\n",
    "        out.append(['time_rd_morethan_95', time_rd_morethan_95])\n",
    "        out.append(['write bw %75', wrstat[0]])\n",
    "        out.append(['write bw %95', wrstat[1]])\n",
    "        out.append(['write bw max', wrstat[2]])\n",
    "        out.append(['time_wr_morethan_95', time_wr_morethan_95])\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_disk.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "class Sar_nic_analysis(Sar_analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Sar_analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Sar_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        nicfilter=\"\"\n",
    "        if 'nic_prefix' in kwargs.keys():\n",
    "            nicfilter= \"IFACE in (\" + \",\".join(kwargs.get('nic_prefix',[\"'eth3'\",\"'enp24s0f1'\"])) + \")\"\n",
    "        else:\n",
    "            nicfilter= \"IFACE != 'lo'\"\n",
    "        \n",
    "        trace_events.extend(self.col_df(nicfilter,       \"eth \",        lambda l: F.struct(F.floor(F.expr('cast(`rxkB/s` as float)/1024')).alias('rxmb/s'),F.floor(F.expr('cast(`txkB/s` as float)/1024')).alias('txmb/s')),                id, 7))\n",
    "        trace_events.extend(self.col_df(\"_3 like 'ib%'\",        \"ib \",        lambda l: F.struct(F.floor(F.expr('cast(`rxkB/s` as float)/1024')).alias('rxmb/s'),F.floor(F.expr('cast(`txkB/s` as float)/1024')).alias('txmb/s')),                id, 8))\n",
    "        trace_events.extend(self.col_df(\"_3 = 'lo'\",            \"lo \",         lambda l: F.struct(F.floor(F.expr('cast (`rxkB/s` as float)/1024')).alias('rxmb/s'),F.floor(F.expr('cast (`txkB/s` as float)/1024')).alias('txmb/s')),              id, 9))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":7,\"args\":{\"sort_index \":7}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":8,\"args\":{\"sort_index \":8}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":9,\"args\":{\"sort_index \":9}}))\n",
    "        return trace_events  \n",
    "    \n",
    "    def get_stat(sar_nic,**kwargs):\n",
    "        Sar_analysis.get_stat(sar_nic)\n",
    "        nicfilter=\"\"\n",
    "        \n",
    "        if 'nic_prefix' in kwargs.keys():\n",
    "            nicfilter= \"IFACE in (\" + \",\".join(kwargs.get('nic_prefix',[\"'eth3'\",\"'enp24s0f1'\"])) + \")\"\n",
    "        else:\n",
    "            nicfilter= \"IFACE != 'lo'\"\n",
    "            \n",
    "        nicbw=sar_nic.df.where(nicfilter).groupBy(\"_1\").agg(F.sum(F.col(\"rxkB/s\").astype(FloatType())/1024).alias(\"rx MB/s\")).orderBy(\"_1\")\n",
    "        out=nicbw.stat.approxQuantile(['rx MB/s'],[0.75,0.95,0.99],0.0)[0]\n",
    "        out=[[\"rx MB/s 75%\",out[0]],[\"rx MB/s 95%\",out[1]],[\"rx MB/s 99%\",out[2]]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_nic.file.split(\"/\")[-2]]\n",
    "        return pdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PID State analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Pidstat_analysis(Analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        sardata=sc.textFile(self.file)\n",
    "        sardf=sardata.mapPartitions(splits).toDF()\n",
    "        sardf=sardf.where(\"_1<>'Average:'\")\n",
    "        \n",
    "        headers=sardf.where(\"_4='TID' or _5='TID'\").limit(1).collect()\n",
    "        r=headers[0].asDict()\n",
    "        findtime=False\n",
    "        for i,v in r.items():\n",
    "            if(v==\"Time\"):\n",
    "                findtime=True\n",
    "        if not findtime:\n",
    "            r[\"_1\"]=\"Time\"\n",
    "        for i,v in r.items():\n",
    "            if(v!=\"\"):\n",
    "                sardf=sardf.withColumnRenamed(i,v)\n",
    "        sardf=sardf.where(\"TGID='0' or TGID='-'\") \n",
    "\n",
    "        self.df=sardf\n",
    "        return sardf\n",
    "\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_list=Analysis.generate_trace_view_list(self,id,**kwargs)\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        \n",
    "        sardf=sardf.withColumn(\"%CPU\",F.col(\"%CPU\").astype(FloatType()))\n",
    "        sardf=sardf.withColumn(\"Time\",F.col(\"Time\").astype(LongType()))\n",
    "        sardf=sardf.withColumn(\"TID\",F.col(\"TID\").astype(LongType()))\n",
    "        hotthreads=sardf.where(\"`%CPU`>30\").groupBy(\"TID\").count().collect()\n",
    "        hts=[(r[0],r[1]) for r in hotthreads]\n",
    "        htc=[r[1] for r in hotthreads]\n",
    "        if len(htc)==0:\n",
    "            return trace_list\n",
    "        maxcnt=max(htc)\n",
    "        hts=[r[0] for r in hts if r[1]>maxcnt/2]\n",
    "        tdfs=list(map(lambda x: sardf.withColumnRenamed(\"TID\",\"TID_\"+str(x)).withColumnRenamed(\"%CPU\",\"CPU_\"+str(x)).where(F.col(\"TID\")==x).select(\"Time\",\"TID_\"+str(x),\"CPU_\"+str(x)),hts))\n",
    "        finaldf=reduce(lambda x,y: x.join(y,on=[\"Time\"]),tdfs)\n",
    "        othersdf=sardf.where(\"TID not in (\"+\",\".join(map(lambda x: str(x),hts))+\")\").groupBy(\"Time\").agg(F.sum(\"%CPU\").alias(\"CPU_Other\"))\n",
    "        finaldf=finaldf.join(othersdf,on=[\"Time\"])\n",
    "        finaldf=finaldf.orderBy(\"Time\")\n",
    "        hts.append(\"Other\")\n",
    "        stt=[F.col(\"CPU_\"+str(x)).alias(str(x)) for x in hts]\n",
    "        args=F.struct(*stt)\n",
    "        \n",
    "        trace_list.extend(finaldf.select(\n",
    "                F.lit(6).alias('tid'),\n",
    "                (F.expr(\"Time*1000\")-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "                F.lit(id).alias('pid'),\n",
    "                F.lit('C').alias('ph'),\n",
    "                F.lit(\"pidstat\").alias('name'),\n",
    "                args.alias('args')\n",
    "            ).toJSON().collect())\n",
    "        return trace_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Perf Trace Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-50642c8e680c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPerfstat_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnalysis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msar_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mAnalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msar_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Analysis' is not defined"
     ]
    }
   ],
   "source": [
    "class Perfstat_analysis(Analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        sardata=sc.textFile(self.file)\n",
    "        sardf=sardata.mapPartitions(splits).toDF()\n",
    "        \n",
    "        paths=os.path.split(self.file)\n",
    "        if fs.exists(paths[0]+\"/perfstarttime\"):\n",
    "            with fs.open(paths[0]+\"/perfstarttime\") as f:\n",
    "                strf=f.read().decode('ascii')\n",
    "        else:\n",
    "            print(\"error, perfstarttime not found\")\n",
    "            return\n",
    "        \n",
    "        strf=strf[len(\"# started on \"):].strip()\n",
    "        starttime=datetime.strptime(strf, \"%a %b %d %H:%M:%S %Y\").timestamp()*1000\n",
    "        sardf=sardf.where(\"_1<>'#'\")\n",
    "        sardf=sardf.withColumn(\"ts\",F.col(\"_2\").astype(DoubleType())*1000+F.lit(starttime)).where(\"ts is not null\").select(\"ts\",\"_3\",\"_4\")\n",
    "        sardf=sardf.withColumn('_3', F.regexp_replace('_3', ',', '').astype(LongType()))\n",
    "        sardf=sardf.cache()\n",
    "        self.df=sardf\n",
    "        return sardf\n",
    "\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_list=Analysis.generate_trace_view_list(self,id,**kwargs)\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        \n",
    "        stringIndexer = StringIndexer(inputCol=\"_4\", outputCol=\"syscall_idx\")\n",
    "        model = stringIndexer.fit(sardf)\n",
    "        sardf=model.transform(sardf)\n",
    "        \n",
    "#        cnts=sardf.select(\"_4\").distinct().collect()\n",
    "#        cnts=[l['_4'] for l in cnts]\n",
    "#        cntmap={ cnts[i]:i  for i in range(0, len(cnts) ) }\n",
    "#        mapexpr=F.create_map([F.lit(x) for x in chain(*cntmap.items())])\n",
    "#        sardf.select(mapexpr.getItem(F.col(\"_4\")))\n",
    "        \n",
    "        sardf=sardf.withColumn(\"syscall_idx\",F.col(\"syscall_idx\").astype(IntegerType()))\n",
    "        \n",
    "        trace_list.extend(sardf.select(\n",
    "            (F.lit(100)+F.col(\"syscall_idx\")).alias('tid'),\n",
    "            (F.col(\"ts\")-F.lit(starttime)).astype(LongType()).alias('ts'),\n",
    "            F.lit(id).alias('pid'),\n",
    "            F.lit('C').alias('ph'),\n",
    "            F.col(\"_4\").alias('name'),\n",
    "            F.struct(F.col(\"_3\").alias(\"cnt\")).alias('args')\n",
    "        ).toJSON().collect())\n",
    "        return trace_list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class gpu_analysis(Analysis):\n",
    "    def __init__(self,gpu_file):\n",
    "        Analysis.__init__(self,gpu_file)\n",
    "        \n",
    "    def load_data(self):\n",
    "        df_pf=spark.read.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").option(\"mode\", \"DROPMALFORMED\").option(\"delimiter\", \",\").load(self.file)\n",
    "        df_pf2=df_pf.withColumn('timestamp',F.unix_timestamp(F.col('timestamp'),'yyyy/MM/dd HH:mm:ss')*1000+(F.split(F.col('timestamp'),'\\.')[1]).astype(IntegerType()))\n",
    "        df_pf2=df_pf2.withColumnRenamed(' utilization.gpu [%]','gpu_util')\n",
    "        df_pf2=df_pf2.withColumnRenamed(' utilization.memory [%]','mem_util')\n",
    "        df_pf2=df_pf2.withColumnRenamed(' memory.used [MiB]','mem_used')\n",
    "        df_pf2=df_pf2.withColumnRenamed(' index','index')\n",
    "        df_pf2=df_pf2.withColumn('gpu_util', (F.split('gpu_util',' ')[1]).astype(IntegerType()))\n",
    "        df_pf2=df_pf2.withColumn('mem_util', (F.split('mem_util',' ')[1]).astype(IntegerType()))\n",
    "        df_pf2=df_pf2.withColumn('mem_used', (F.split('mem_used',' ')[1]).astype(IntegerType()))\n",
    "        df_pf.cache()\n",
    "        self.df=df_pf2\n",
    "        return df_pf2\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,id)\n",
    "            \n",
    "        df_pf2=self.df\n",
    "        starttime=self.starttime\n",
    "        trace_events=[]\n",
    "        \n",
    "        trace_events.extend(df_pf2.orderBy(df_pf2['timestamp']).select(\n",
    "            F.col('index').alias('tid'),\n",
    "            (F.expr(\"timestamp\")-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.lit(id).alias('pid'),\n",
    "            F.lit('C').alias('ph'),\n",
    "            F.concat(F.lit('gpu_util_'),F.col('index')).alias('name'),\n",
    "            F.struct(F.col('gpu_util').alias('gpu')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        trace_events.extend(df_pf2.orderBy(df_pf2['timestamp']).select(\n",
    "            F.col('index').alias('tid'),\n",
    "            (F.expr(\"timestamp\")-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.lit(int(id)+1).alias('pid'),\n",
    "            F.lit('C').alias('ph'),\n",
    "            F.concat(F.lit('mem_util_'),F.col('index')).alias('name'),\n",
    "            F.struct((F.col('mem_used')/F.lit(32768)).alias('mem')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        return trace_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "def splits_dmon(x):\n",
    "    fi=[]\n",
    "    for l in x:\n",
    "        l=l.strip()\n",
    "        if l.startswith('20'):\n",
    "            li=re.split(r'\\s+',l)\n",
    "            if len(li)==11:\n",
    "                fi.append(li)\n",
    "    return iter(fi)\n",
    "\n",
    "class gpu_dmon_analysis(Analysis):\n",
    "    def __init__(self,gpu_file):\n",
    "        Analysis.__init__(self,gpu_file)\n",
    "        \n",
    "    def load_data(self):\n",
    "        df_pf=sc.textFile(self.file)\n",
    "        df_pf=df_pf.mapPartitions(splits_dmon).toDF()\n",
    "        \n",
    "        df_pf2=df_pf.withColumn('_1',F.unix_timestamp(F.concat_ws(' ',F.col('_1'),F.col('_2')),'yyyyMMdd HH:mm:ss')*1000)\n",
    "        for c in range(3,12):\n",
    "            df_pf2=df_pf2.withColumn(f'_{c}',F.col(f'_{c}').astype(IntegerType()))\n",
    "\n",
    "        df_pf.cache()\n",
    "        self.df=df_pf2\n",
    "        return df_pf2\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,id)\n",
    "\n",
    "        df_pf2=self.df\n",
    "        starttime=self.starttime\n",
    "        trace_events=[]\n",
    "        \n",
    "        trace_events.extend(df_pf2.orderBy(df_pf2['_1']).select(\n",
    "            F.col('_3').alias('tid'),\n",
    "            (F.expr(\"_1\")-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.lit(id).alias('pid'),\n",
    "            F.lit('C').alias('ph'),\n",
    "            F.concat(F.lit('gpu_util_'),F.col('_3')).alias('name'),\n",
    "            F.struct(F.col('_4').alias('gpu')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        trace_events.extend(df_pf2.orderBy(df_pf2['_1']).select(\n",
    "            F.col('_3').alias('tid'),\n",
    "            (F.expr(\"_1\")-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.lit(id+1).alias('pid'),\n",
    "            F.lit('C').alias('ph'),\n",
    "            F.concat(F.lit('mem_util_'),F.col('_3')).alias('name'),\n",
    "            F.struct(F.col('_5').alias('mem')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        trace_events.extend(df_pf2.orderBy(df_pf2['_1']).select(\n",
    "            F.col('_3').alias('tid'),\n",
    "            (F.expr(\"_1\")-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.lit(id+2).alias('pid'),\n",
    "            F.lit('C').alias('ph'),\n",
    "            F.concat(F.lit('gpu_freq_'),F.col('_3')).alias('name'),\n",
    "            F.struct(F.col('_9').alias('gpu_freq')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        trace_events.extend(df_pf2.orderBy(df_pf2['_1']).select(\n",
    "            F.col('_3').alias('tid'),\n",
    "            (F.expr(\"_1\")-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.lit(id+3).alias('pid'),\n",
    "            F.lit('C').alias('ph'),\n",
    "            F.concat(F.lit('pcie_'),F.col('_3')).alias('name'),\n",
    "            F.struct(F.col('_10').alias('tx'),F.col('_11').alias('rx')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        return trace_events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# DASK analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_dask(x):\n",
    "    fi=[]\n",
    "    for l in x:\n",
    "        print(l)\n",
    "        li=[]\n",
    "        if l.startswith('('):\n",
    "            lx=re.split(r'[()]',l)\n",
    "            lv=lx[1]\n",
    "            p=re.search(r\"'(.*)-([0-9a-f]+)', *(\\d+)\",lv)\n",
    "            if not p:\n",
    "                print(\"dask log first field doesn't match (.*)-[0-9a-f]+', *(\\d+)\")\n",
    "                return\n",
    "            li.append(p.group(1))\n",
    "            li.extend(lx[2].split(\",\")[1:])\n",
    "            li.append(p.group(3))\n",
    "        else:\n",
    "            li=l.split(',')\n",
    "            p=re.search(r\"(.*)-([0-9a-f]+-[0-9a-f]+-[0-9a-f]+-[0-9a-f]+-[0-9a-f]+)$\",li[0])\n",
    "            if not p:\n",
    "                p=re.search(r\"(.*)-([0-9a-f]+)$\",li[0])\n",
    "            \n",
    "            li[0]=p.group(1)\n",
    "            li.append(p.group(2))\n",
    "        fi.append(li)\n",
    "    return iter(fi)\n",
    "\n",
    "class dask_analysis(Analysis):\n",
    "    def __init__(self,dask_file):\n",
    "        Analysis.__init__(self,dask_file)\n",
    "\n",
    "    def load_data(self):\n",
    "        rdds=sc.textFile(self.file)\n",
    "        df_pf=rdds.mapPartitions(split_dask).toDF()\n",
    "        df_pf=df_pf.withColumnRenamed('_1','_c0')\n",
    "        df_pf=df_pf.withColumnRenamed('_2','_c1')\n",
    "        df_pf=df_pf.withColumnRenamed('_3','_c2')\n",
    "        df_pf=df_pf.withColumnRenamed('_4','_c3')\n",
    "        df_pf=df_pf.withColumnRenamed('_5','_id')\n",
    "        \n",
    "        df_pf=df_pf.withColumn('_c1',F.split(F.col('_c1'),\":\")[2])\n",
    "        df_pf=df_pf.withColumn('_c3',df_pf._c3.astype(DoubleType())*1000) \n",
    "        df_pf=df_pf.withColumn('_c2',df_pf._c2.astype(DoubleType())*1000)\n",
    "        \n",
    "        df_pf.cache()\n",
    "        self.df=df_pf\n",
    "        self.starttime=df_pf.agg(F.min(\"_c2\")).collect()[0]['min(_c2)']\n",
    "        return df_pf\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,id)\n",
    "        \n",
    "        df_pf=self.df\n",
    "\n",
    "        window = Window.partitionBy(\"_c1\").orderBy(\"_c3\")\n",
    "        df_pf=df_pf.withColumn(\"last_tsk_done\", F.lag('_c3', 1, None).over(window))\n",
    "        df_pf=df_pf.withColumn('last_tsk_done',F.coalesce('last_tsk_done','_c2'))\n",
    "        df_pf=df_pf.withColumn('last_tsk_done',F.when(F.col('_c2')>F.col('last_tsk_done'),F.col('_c2')).otherwise(F.col('last_tsk_done')) )\n",
    "        \n",
    "        trace_events=[]\n",
    "        \n",
    "        trace_events.extend(df_pf.select(\n",
    "            F.col('_c1').alias('tid'),\n",
    "            (F.col('last_tsk_done')-F.lit(self.starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.expr('_c3 - last_tsk_done  ').alias('dur'),\n",
    "            F.lit(id).alias('pid'),\n",
    "            F.lit('X').alias('ph'),\n",
    "            F.col('_c0').alias('name'),\n",
    "            F.struct(F.col('_id').alias('uuid')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        return trace_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class dask_analysis_log(dask_analysis):\n",
    "    def __init__(self,dask_file,logs):\n",
    "        Analysis.__init__(self,dask_file)\n",
    "\n",
    "    def load_data(self):\n",
    "        rdds=sc.textFile(self.file)\n",
    "        df_pf=rdds.mapPartitions(split_dask).toDF()\n",
    "        df_pf=df_pf.withColumnRenamed('_1','_c0')\n",
    "        df_pf=df_pf.withColumnRenamed('_2','_c1')\n",
    "        df_pf=df_pf.withColumnRenamed('_3','_c2')\n",
    "        df_pf=df_pf.withColumnRenamed('_4','_c3')\n",
    "        df_pf=df_pf.withColumnRenamed('_5','_id')\n",
    "        \n",
    "        df_pf=df_pf.withColumn('_c1',F.split(F.col('_c1'),\":\")[2])\n",
    "        df_pf=df_pf.withColumn('_c3',df_pf._c3.astype(DoubleType())*1000) \n",
    "        df_pf=df_pf.withColumn('_c2',df_pf._c2.astype(DoubleType())*1000)\n",
    "        \n",
    "        df_pf.cache()\n",
    "        self.df=df_pf\n",
    "        self.starttime=df_pf.agg(F.min(\"_c2\")).collect()[0]['min(_c2)']\n",
    "        return df_pf\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,id)\n",
    "        \n",
    "        df_pf=self.df\n",
    "\n",
    "        window = Window.partitionBy(\"_c1\").orderBy(\"_c3\")\n",
    "        df_pf=df_pf.withColumn(\"last_tsk_done\", F.lag('_c3', 1, None).over(window))\n",
    "        df_pf=df_pf.withColumn('last_tsk_done',F.coalesce('last_tsk_done','_c2'))\n",
    "        df_pf=df_pf.withColumn('last_tsk_done',F.when(F.col('_c2')>F.col('last_tsk_done'),F.col('_c2')).otherwise(F.col('last_tsk_done')) )\n",
    "        \n",
    "        trace_events=[]\n",
    "        \n",
    "        trace_events.extend(df_pf.select(\n",
    "            F.col('_c1').alias('tid'),\n",
    "            (F.col('last_tsk_done')-F.lit(self.starttime)).astype(IntegerType()).alias('ts'),\n",
    "            F.expr('_c3 - last_tsk_done  ').alias('dur'),\n",
    "            F.lit(id).alias('pid'),\n",
    "            F.lit('X').alias('ph'),\n",
    "            F.col('_c0').alias('name'),\n",
    "            F.struct(F.col('_id').alias('uuid')).alias('args')\n",
    "        ).toJSON().collect())\n",
    "\n",
    "        return trace_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# instantevent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## format: _2 = Name; _3 = time\n",
    "\n",
    "class InstantEvent_analysis(Analysis):\n",
    "    def __init__(self,sar_file):\n",
    "        Analysis.__init__(self,sar_file)\n",
    "    \n",
    "    def load_data(self):\n",
    "        sardata=sc.textFile(self.file)\n",
    "        sardf=sardata.mapPartitions(splits).toDF()\n",
    "        self.df=sardf\n",
    "        return sardf\n",
    "\n",
    "\n",
    "    def generate_trace_view_list(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,id)\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        return sardf.select(F.lit(0).alias('tid'),\n",
    "                (F.col(\"_3\").astype(DoubleType())*1000-F.lit(starttime)).astype(IntegerType()).alias('ts'),\n",
    "                F.lit(0).alias('pid'),\n",
    "                F.lit('i').alias('ph'),\n",
    "                F.col(\"_2\").alias('name'),\n",
    "                F.lit(\"g\").alias(\"s\")\n",
    "            ).toJSON().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Run:\n",
    "    def __init__(self,samples):\n",
    "        self.samples=samples\n",
    "    \n",
    "    def generate_trace_view(self,appid,**kwargs):\n",
    "        traces=[]\n",
    "        \n",
    "        for idx, s in enumerate(self.samples):\n",
    "            traces.extend(s.generate_trace_view_list(idx,**kwargs))        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ]\n",
    "        }'''\n",
    "\n",
    "        with open('/home/yuzhou/trace_result/'+appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "\n",
    "        print(\"http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+appid+\".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dask Application Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dask_Application_Run:\n",
    "    def __init__(self, appid):\n",
    "        self.appid=appid\n",
    "        self.filedir=\"/tmp/dgx-2Log/\"+self.appid+\"/\"\n",
    "        \n",
    "        self.analysis={\n",
    "            'dask':{'als':dask_analysis(self.filedir+\"cluster.log\"),'pid':8000},\n",
    "            'sar_cpu':{'als':Sar_cpu_analysis(self.filedir + \"/\"+\"sar_cpu.sar\"),'pid':10*0+0},\n",
    "            'sar_disk':{'als':Sar_disk_analysis(self.filedir + \"/\"+\"sar_disk.sar\"),'pid':10*0+1},\n",
    "            'sar_mem':{'als':Sar_mem_analysis(self.filedir + \"/\"+\"sar_mem.sar\"),'pid':10*0+2},\n",
    "            'sar_nic':{'als':Sar_nic_analysis(self.filedir  + \"/\"+\"sar_nic.sar\"),'pid':10*0+3},\n",
    "            'emon':{'als':Emon_Analysis(self.filedir +  \"/\"+\"emon.rst\"),'pid':10*0+4},\n",
    "            'gpu':{'als':gpu_analysis(self.filedir + \"/gpu.txt\"),'pid':10*0+5},\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def generate_trace_view(self,showsar=True,showemon=False,showgpu=True,**kwargs):\n",
    "        traces=[]\n",
    "        daskals=self.analysis['dask']['als']\n",
    "        traces.extend(daskals.generate_trace_view_list(self.analysis['dask']['pid'],**kwargs))\n",
    "        if showsar:\n",
    "            sarals=self.analysis['sar_cpu']['als']\n",
    "            sarals.starttime=daskals.starttime\n",
    "            traces.extend(sarals.generate_trace_view_list(self.analysis['sar_cpu']['pid'],**kwargs))\n",
    "            sarals=self.analysis['sar_disk']['als']\n",
    "            sarals.starttime=daskals.starttime\n",
    "            traces.extend(sarals.generate_trace_view_list(self.analysis['sar_disk']['pid'],**kwargs))\n",
    "            sarals=self.analysis['sar_mem']['als']\n",
    "            sarals.starttime=daskals.starttime\n",
    "            traces.extend(sarals.generate_trace_view_list(self.analysis['sar_mem']['pid'],**kwargs))\n",
    "            sarals=self.analysis['sar_nic']['als']\n",
    "            sarals.starttime=daskals.starttime\n",
    "            traces.extend(sarals.generate_trace_view_list(self.analysis['sar_nic']['pid'],**kwargs))\n",
    "        if showemon:\n",
    "            emonals=self.analysis['emon']['als']\n",
    "            emonals.starttime=daskals.starttime\n",
    "            traces.extend(emonals.generate_trace_view_list(self.analysis['emon']['pid'],**kwargs))\n",
    "        if showgpu:\n",
    "            gpuals=self.analysis['gpu']['als']\n",
    "            gpuals.starttime=daskals.starttime\n",
    "            traces.extend(gpuals.generate_trace_view_list(self.analysis['gpu']['pid'],**kwargs))\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ]\n",
    "        }'''\n",
    "\n",
    "        with open('/home/yuzhou/trace_result/'+self.appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "\n",
    "        print(\"http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+self.appid+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.fromtimestamp(1546439400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dask_Application_Run2:\n",
    "    def __init__(self, appid):\n",
    "        self.appid=appid\n",
    "        \n",
    "        self.filedir=\"/tmp/dgx-2Log/\"+self.appid+\"/\"\n",
    "        self.dask=self.load_dask()\n",
    "        self.sar=self.load_sar()\n",
    "        self.gpu=self.load_gpu()\n",
    "        \n",
    "    \n",
    "    def load_dask(self):\n",
    "        return dask_analysis(self.filedir+\"cluster.log\")\n",
    "    \n",
    "    def load_sar(self):\n",
    "        return Sar_analysis(self.filedir+\"sar_data.sar\")\n",
    "        \n",
    "    def load_emon(self):\n",
    "        return Emon_Analysis(self.filedir+\"emon.rst\")\n",
    "    \n",
    "    def load_gpu(self):\n",
    "        return gpu_dmon_analysis(self.filedir+\"gpu_dmon.txt\")\n",
    "    \n",
    "    def generate_dask_trace_view(self):\n",
    "        return self.dask.generate_dask_trace_view(8000)\n",
    "    \n",
    "    def generate_sar_trace_view(self):\n",
    "        return self.sar.generate_sar_trace_view(0)\n",
    "    \n",
    "    def generate_gpu_trace_view(self):\n",
    "        return self.gpu.generate_gpu_trace_view(1)\n",
    "\n",
    "    def generate_emon_trace_view(self,collected_cores):\n",
    "        return self.emon.generate_emon_trace_view(5,collected_cores)\n",
    "    \n",
    "    def generate_trace_view(self,showsar=True,showemon=False,showgpu=True):\n",
    "        traces=[]\n",
    "        traces.extend(self.generate_dask_trace_view())\n",
    "        if showsar:\n",
    "            self.sar.starttime=self.dask.starttime\n",
    "            traces.extend(self.generate_sar_trace_view())\n",
    "        if showemon:\n",
    "            traces.extend(self.generate_emon_trace_view(collected_cores))\n",
    "        if showgpu:\n",
    "            self.gpu.starttime=self.dask.starttime\n",
    "            traces.extend(self.generate_gpu_trace_view())\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ]\n",
    "        }'''\n",
    "\n",
    "        with open('/home/yuzhou/trace_result/'+self.appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "\n",
    "        print(\"http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+self.appid+\".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Application RUN STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Application_Run_STD:\n",
    "    def __init__(self, appid):\n",
    "        self.appid=appid\n",
    "        self.filedir=\"/tmp/dgx-2Log/\"+self.appid+\"/\"\n",
    "        \n",
    "        self.analysis={\n",
    "            'sar':{'als':Sar_analysis(self.filedir+\"sar_data.sar\"),'pid':0},\n",
    "            'emon':{'als':Emon_Analysis(self.filedir+\"emon.rst\"),'pid':1},\n",
    "            'gpu':{'als':gpu_analysis(self.filedir+\"gpu.txt\"),'pid':100},\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def generate_trace_view(self,showsar=True,showemon=False,showgpu=True,**kwargs):\n",
    "        traces=[]\n",
    "        starttime=time.time()*1000\n",
    "        if showsar:\n",
    "            sarals=self.analysis['sar']['als']\n",
    "            sarals.starttime=starttime\n",
    "            traces.extend(sarals.generate_trace_view_list(self.analysis['sar']['pid'],**kwargs))\n",
    "        if showemon:\n",
    "            emonals=self.analysis['emon']['als']\n",
    "            emonals.starttime=starttime\n",
    "            traces.extend(emonals.generate_trace_view_list(self.analysis['emon']['pid'],**kwargs))\n",
    "        if showgpu:\n",
    "            gpuals=self.analysis['gpu']['als']\n",
    "            gpuals.starttime=starttime\n",
    "            traces.extend(gpuals.generate_trace_view_list(self.analysis['gpu']['pid'],**kwargs))\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ]\n",
    "        }'''\n",
    "\n",
    "        with open('/home/yuzhou/trace_result/'+self.appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "\n",
    "        print(\"http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+self.appid+\".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# application Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     152
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Application_Run:\n",
    "    def __init__(self, appid,**kwargs):\n",
    "        self.appid=appid\n",
    "        \n",
    "        basedir=kwargs.get(\"basedir\",\"skylake\")\n",
    "        self.filedir=\"/\"+basedir+\"/\"+self.appid+\"/\"\n",
    "        self.basedir=basedir\n",
    "        \n",
    "        slaves=fs.list_status(\"/\"+basedir+\"/\"+appid)\n",
    "        slaves=[f['pathSuffix'] for f in slaves if f['type']=='DIRECTORY']\n",
    "        \n",
    "        jobids=kwargs.get(\"jobids\",None)\n",
    "        \n",
    "        self.clients=slaves\n",
    "        \n",
    "        sarclnt={}\n",
    "        for idx,l in enumerate(self.clients):\n",
    "            sarclnt[l]={'sar_cpu':{'als':Sar_cpu_analysis(self.filedir + l + \"/\"+\"sar_cpu.sar\"),'pid':idx},\n",
    "                'sar_disk':{'als':Sar_disk_analysis(self.filedir + l + \"/\"+\"sar_disk.sar\"),'pid':idx},\n",
    "                'sar_mem':{'als':Sar_mem_analysis(self.filedir + l + \"/\"+\"sar_mem.sar\"),'pid':idx},\n",
    "                'sar_nic':{'als':Sar_nic_analysis(self.filedir + l + \"/\"+\"sar_nic.sar\"),'pid':idx}\n",
    "                \n",
    "            }\n",
    "            if fs.exists(self.filedir + l + \"/pidstat.out\"):\n",
    "                sarclnt[l]['sar_pid']={'als':Pidstat_analysis(self.filedir + l + \"/pidstat.out\"),'pid':idx}\n",
    "            if fs.exists(self.filedir + l + \"/sched.txt\"):\n",
    "                sarclnt[l]['sar_perf']={'als':Perf_trace_analysis(self.filedir + l + \"/sched.txt\"),'pid':100+idx}\n",
    "            if fs.exists(self.filedir + l + \"/emon.rst\"):\n",
    "                sarclnt[l]['emon']={'als':Emon_Analysis(self.filedir + l + \"/emon.rst\"),'pid':200+idx}\n",
    "            if fs.exists(self.filedir + l + \"/perfstat.txt\"):\n",
    "                sarclnt[l]['perfstat']={'als':Perfstat_analysis(self.filedir + l + \"/perfstat.txt\"),'pid':300+idx}\n",
    "            if fs.exists(self.filedir + l + \"/gpu.txt\"):\n",
    "                sarclnt[l]['gpu']={'als':gpu_analysis(self.filedir + l + \"/gpu.txt\"),'pid':400+idx}\n",
    "            \n",
    "                \n",
    "        self.analysis={\n",
    "            \"sar\": sarclnt\n",
    "        }\n",
    "        \n",
    "        if fs.exists(self.filedir+\"app.log\"):\n",
    "            self.analysis['app']={'als':App_Log_Analysis(self.filedir+\"app.log\",jobids)}\n",
    "        \n",
    "        if fs.exists(self.filedir+\"instevent.out\"):\n",
    "            self.analysis['instant']={'als':InstantEvent_analysis(self.filedir+\"instevent.out\")}\n",
    "        \n",
    "        self.starttime=0\n",
    "        if fs.exists(self.filedir+\"starttime\"):\n",
    "            with fs.open(self.filedir+\"starttime\") as f:\n",
    "                st = f.read().decode('ascii')\n",
    "                self.starttime=int(st)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def generate_trace_view(self,showsar=True,showemon=False,showgpu=True,**kwargs):\n",
    "        traces=[]\n",
    "        shownodes=kwargs.get(\"shownodes\",self.clients)\n",
    "        for l in shownodes:\n",
    "            if l not in self.clients:\n",
    "                print(l,\"is not in clients\",self.clients)\n",
    "                return\n",
    "        self.clients=shownodes\n",
    "        \n",
    "        xgbtcks=kwargs.get('xgbtcks',(\"calltrain\",'enter','begin','end'))\n",
    "        \n",
    "        if \"app\" in self.analysis:\n",
    "            appals=self.analysis['app']['als']\n",
    "            appals.starttime=self.starttime\n",
    "            traces.extend(appals.generate_trace_view_list(self.analysis['app'],**kwargs))\n",
    "            self.starttime=appals.starttime\n",
    "        \n",
    "        if 'instant' in self.analysis:\n",
    "            als=self.analysis['instant']['als']\n",
    "            als.starttime=self.starttime\n",
    "            traces.extend(als.generate_trace_view_list(**kwargs))\n",
    "        \n",
    "        counttime=kwargs.get(\"counttime\",False)\n",
    "        \n",
    "        pidmap={}\n",
    "        if showsar:\n",
    "            for l in self.clients:\n",
    "                for alskey, sarals in self.analysis[\"sar\"][l].items():\n",
    "                    t1 = time.time()\n",
    "                    if alskey!=\"emon\":\n",
    "                        sarals['als'].starttime=self.starttime\n",
    "                        traces.extend(sarals['als'].generate_trace_view_list(sarals['pid'],node=l, **kwargs))\n",
    "                    elif showemon:\n",
    "                        sarals['als'].load_data()\n",
    "                        pidmap[l]=sarals['pid']\n",
    "                    if counttime:\n",
    "                        print(l,alskey,\" spend time: \", time.time()-t1)\n",
    "        if showemon:\n",
    "            t1 = time.time()\n",
    "            emondfs=get_emon_parquets([self.appid,],self.basedir)\n",
    "            emons=Emon_Analysis_All(emondfs)\n",
    "            emons.starttime=self.starttime\n",
    "            traces.extend(emons.generate_trace_view_list(0,pidmap=pidmap,**kwargs))\n",
    "            if counttime:\n",
    "                print(\"emon process spend time: \", time.time()-t1)\n",
    "            self.emons=emons\n",
    "        \n",
    "        for idx,l in enumerate(self.clients):\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx,\"tid\":0,\"args\":{\"sort_index \":idx}}))\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx+100,\"tid\":0,\"args\":{\"sort_index \":idx+100}}))\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx+200,\"tid\":0,\"args\":{\"sort_index \":idx+200}}))\n",
    "        \n",
    "        if \"app\" in self.analysis:\n",
    "            for pid in self.analysis['app']['als'].pids:\n",
    "                traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":pid+200,\"tid\":0,\"args\":{\"sort_index \":pid+200}}))\n",
    "\n",
    "        allcnt=\"\"\n",
    "        for c in self.clients:\n",
    "            paths=self.filedir+c\n",
    "            if fs.exists(paths+\"/xgbtck.txt\"):\n",
    "                with fs.open(paths+\"/xgbtck.txt\") as f:\n",
    "                    tmp = f.read().decode('ascii')\n",
    "                    allcnt=allcnt+tmp\n",
    "        allcnt=allcnt.strip().split(\"\\n\")\n",
    "        if len(allcnt) > 1:\n",
    "            allcnt=[l.split(\" \") for l in allcnt]\n",
    "            cnts=pandas.DataFrame([[l[0],l[1],l[2],l[3]] for l in allcnt if len(l)>1 and l[1] in xgbtcks])\n",
    "            if len(cnts) > 0:\n",
    "                cnts.columns=['xgbtck','name','rank','time']\n",
    "                cntgs=cnts.groupby(\"name\").agg({\"time\":\"min\"})\n",
    "                cntgs=cntgs.reset_index()\n",
    "                cntgs.columns=['name','ts']\n",
    "                cntgs['ph']=\"i\"\n",
    "                cntgs['ts']=pandas.to_numeric(cntgs['ts'])-self.starttime\n",
    "                cntgs['pid']=0\n",
    "                cntgs['tid']=0\n",
    "                cntgs['s']='g'\n",
    "                traces.extend([json.dumps(l) for l in cntgs.to_dict(orient='records')])\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ],\n",
    "            \"displayTimeUnit\": \"ns\"\n",
    "        }'''\n",
    "\n",
    "        with open('/home/yuzhou/trace_result/'+self.appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "\n",
    "        print(\"http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/\"+self.appid+\".json\")\n",
    "        \n",
    "    def getemonmetric(app,emonmetric):\n",
    "        emondfs=get_emon_parquets([app.appid],app.basedir)\n",
    "        emons=Emon_Analysis_All(emondfs)\n",
    "        metric_msg_map={\n",
    "            'emon_cpuutil':F.avg,\n",
    "            'emon_cpufreq':F.avg,\n",
    "            'emon_instr_retired':F.sum,\n",
    "            'emon_ipc':F.avg,\n",
    "            'emon_L3 stall':F.avg,\n",
    "            'emon_mem stall':F.avg,\n",
    "            'emon_total stall':F.avg,\n",
    "            'emon_dtlb walk':F.avg,\n",
    "            'emon_uop issue stall':F.avg,\n",
    "            'emon_mem_bw':F.avg,\n",
    "            'emon_remote ratio':F.avg,\n",
    "            'emon_local lat':F.avg,\n",
    "            'emon_remote lat':F.avg,\n",
    "            'emon_l3 miss':F.avg,\n",
    "            'emon_l3 read miss bw':F.avg,\n",
    "            'emon_pcie_bw':F.avg\n",
    "        }\n",
    "\n",
    "        outdf=None\n",
    "        for k in emonmetric:\n",
    "            m=emons.emon_metrics[k]\n",
    "            for fk,fm in m['formula'].items():\n",
    "                df=emons.gen_reduce_metric(k,list(range(0,emons.totalcores)),fk,metric_msg_map[k])\n",
    "                tmpdf=df.groupBy(\"appid\",'client').agg(*[l(\"`{:s}`\".format(fk)).alias(get_alias_name(fk,l)) for l in [metric_msg_map[k]]]).toPandas()\n",
    "                tmpdf=tmpdf.set_index(\"client\").drop(columns=['appid']).T\n",
    "                if outdf is None:\n",
    "                    outdf=tmpdf\n",
    "                else:\n",
    "                    outdf=outdf.append(tmpdf)\n",
    "        pandas.options.display.float_format = '{:,.2f}'.format\n",
    "        return outdf\n",
    "    \n",
    "    def get_sar_stat(app_ww44,**kwargs):\n",
    "        disk_prefix=kwargs.get(\"disk_prefix\",\"dev259\")\n",
    "        nic_prefix = kwargs.get(\"nic_prefix\",[\"'eth3'\",\"'enp24s0f1'\"])\n",
    "        cpustat=[app_ww44.analysis[\"sar\"][l]['sar_cpu']['als'].get_stat() for l in app_ww44.clients]\n",
    "        cpustat=reduce(lambda l,r:l.join(r),cpustat)\n",
    "        diskstat=[app_ww44.analysis[\"sar\"][l]['sar_disk']['als'].get_stat(disk_prefix=disk_prefix) for l in app_ww44.clients]\n",
    "        diskstat=reduce(lambda l,r:l.join(r),diskstat)\n",
    "        memstat=[app_ww44.analysis[\"sar\"][l]['sar_mem']['als'].get_stat() for l in app_ww44.clients]\n",
    "        memstat=reduce(lambda l,r:l.join(r),memstat)\n",
    "        nicstat=[app_ww44.analysis[\"sar\"][l]['sar_nic']['als'].get_stat(nic_prefix=nic_prefix) for l in app_ww44.clients]\n",
    "        nicstat=reduce(lambda l,r:l.join(r),nicstat)\n",
    "        pandas.options.display.float_format = '{:,.2f}'.format\n",
    "        return pandas.concat([cpustat,diskstat,memstat,nicstat])\n",
    "    \n",
    "    def compare_app(app2,**kwargs):\n",
    "        output=[]\n",
    "        \n",
    "        lbasedir=kwargs.get(\"basedir\",app2.basedir)\n",
    "        r_appid=kwargs.get(\"r_appid\",app2.appid)\n",
    "        \n",
    "        app=kwargs.get(\"rapp\",Application_Run(r_appid,basedir=lbasedir))\n",
    "\n",
    "        show_queryplan_diff=kwargs.get(\"show_queryplan_diff\",True)\n",
    "        \n",
    "        appals=app.analysis[\"app\"][\"als\"]\n",
    "        appals2=app2.analysis[\"app\"][\"als\"]\n",
    "\n",
    "        out=appals.get_query_time(plot=False)\n",
    "        out2=appals2.get_query_time(plot=False)\n",
    "\n",
    "        lrun=app.appid\n",
    "        rrun=app2.appid\n",
    "        cmpcolumns=['runtime','shuffle_write','f_wait_time','input read','acc_task_time','output rows']\n",
    "        outcut=out[cmpcolumns]\n",
    "        out2cut=out2[cmpcolumns]\n",
    "        cmp=outcut.join(out2cut,lsuffix='_'+lrun,rsuffix='_'+rrun)\n",
    "\n",
    "        pdsout=pandas.DataFrame(outcut.sum(),columns=[lrun])\n",
    "        pdsout2=pandas.DataFrame(out2cut.sum(),columns=[rrun])\n",
    "        pdstime=pdsout.join(pdsout2)\n",
    "\n",
    "        print(\"emon metric\")\n",
    "        \n",
    "        emondf=app.getemonmetric(emonmetric=emonmetric)\n",
    "        emondf2=app2.getemonmetric(emonmetric=emonmetric)\n",
    "        #in case we comare with two clsuter\n",
    "        emondf.columns=emondf2.columns\n",
    "        def get_agg(emondf):\n",
    "            aggs=[]\n",
    "            for x in emondf.index:\n",
    "                if x.endswith(\"avg\"):\n",
    "                    aggs.append(emondf.loc[x].mean())\n",
    "                else:\n",
    "                    aggs.append(emondf.loc[x].sum())\n",
    "\n",
    "            emondf['agg']=aggs\n",
    "            return emondf\n",
    "        emondf=get_agg(emondf)\n",
    "        emondf2=get_agg(emondf2)\n",
    "\n",
    "        emoncolumns=emondf.columns\n",
    "        emoncmp=emondf.join(emondf2,lsuffix='_'+lrun,rsuffix='_'+rrun)\n",
    "        emonsum=emoncmp[[\"agg_\"+lrun,\"agg_\"+rrun]]\n",
    "\n",
    "        emonsum.columns=[lrun,rrun]\n",
    "\n",
    "        print(\"sar metric\")\n",
    "        sardf=app.get_sar_stat(**kwargs)\n",
    "        sardf2=app2.get_sar_stat(**kwargs)\n",
    "        \n",
    "        def get_sar_agg(sardf):\n",
    "            aggs=[]\n",
    "            for x in sardf.index:\n",
    "                if \"total\" in x:\n",
    "                    aggs.append(sardf.loc[x].sum())\n",
    "                elif \"max\" in x:\n",
    "                    aggs.append(sardf.loc[x].max())\n",
    "                else:\n",
    "                    aggs.append(sardf.loc[x].mean())\n",
    "\n",
    "            sardf['agg']=aggs\n",
    "            return sardf\n",
    "        sardf=get_sar_agg(sardf)\n",
    "        sardf2=get_sar_agg(sardf2)\n",
    "        #in case we compare two clusters\n",
    "        sardf2.columns=sardf.columns\n",
    "\n",
    "        sarcolumns=sardf.columns\n",
    "        sarcmp=sardf.join(sardf2,lsuffix='_'+lrun,rsuffix='_'+rrun)\n",
    "        sarsum=sarcmp[[\"agg_\"+lrun,\"agg_\"+rrun]]\n",
    "\n",
    "        sarsum.columns=[lrun,rrun]\n",
    "        \n",
    "        \n",
    "        summary=pandas.concat([pdstime,sarsum,emonsum])\n",
    "        summary[\"diff\"]=summary[lrun]/summary[rrun]-1\n",
    "        def highlight_diff(x):\n",
    "            styles=[]\n",
    "            mx=x.max()\n",
    "            mn=x.min()\n",
    "            mx=max(mx,-mn,0.2)\n",
    "            for j in x.index:\n",
    "                m1=(x[j])/mx*100 if x[j]!=None else 0\n",
    "                if m1>0:\n",
    "                    styles.append(f'width: 400px ; background-image: linear-gradient(to right, transparent 50%, #5fba7d 50%, #5fba7d {50+m1/2}%, transparent {50+m1/2}%)')\n",
    "                else:\n",
    "                    styles.append(f'width: 400px ;background-image: linear-gradient(to left, transparent 50%, #f1a863 50%, #f1a863 {50-m1/2}%, transparent {50-m1/2}%)')\n",
    "            return styles\n",
    "\n",
    "        output.append(summary.style.apply(highlight_diff,subset=['diff']).format({lrun:\"{:,.2f}\",rrun:\"{:,.2f}\",'diff':\"{:,.2%}\"}).render())\n",
    "\n",
    "        cmp_plot=cmp\n",
    "        cmp_plot['diff']=cmp_plot['runtime_'+lrun]-cmp_plot['runtime_'+rrun]\n",
    "\n",
    "        pltx=cmp_plot.sort_values(by='diff',axis=0).plot.bar(y=['runtime_'+lrun,'runtime_'+rrun],figsize=(30,8))\n",
    "        better_num=sqldf('''select count(*) from cmp_plot where diff>0''')['count(*)'][0]\n",
    "        pltx.text(0.1, 0.8,'{:d} queries are better'.format(better_num), ha='center', va='center', transform=pltx.transAxes)\n",
    "\n",
    "        df1 = pandas.DataFrame('', index=cmp.index, columns=cmpcolumns)\n",
    "        for l in cmpcolumns:\n",
    "            for j in cmp.index:\n",
    "                df1[l][j]=[cmp[l+\"_\"+lrun][j],cmp[l+\"_\"+rrun][j],cmp[l+\"_\"+lrun][j]/cmp[l+\"_\"+rrun][j]-1]\n",
    "\n",
    "        def highlight_greater(x,columns):\n",
    "            df1 = pandas.DataFrame('', index=x.index, columns=x.columns)\n",
    "            for l in columns:\n",
    "                m={}\n",
    "                for j in x.index:\n",
    "                    m[j] = (x[l][j][1] / x[l][j][0])*100 if x[l][j][0]!=0 else 100\n",
    "                mx=max(m.values())-100\n",
    "                mn=100-min(m.values())\n",
    "                mx=max(mx,mn)\n",
    "                for j in x.index:\n",
    "                    m1=-(100-m[j])/mx*100 if x[l][j][0]!=0 else 0\n",
    "                    if m1>0:\n",
    "                        df1[l][j] = f'background-image: linear-gradient(to right, transparent 50%, #5fba7d 50%, #5fba7d {50+m1/2}%, transparent {50+m1/2}%)'\n",
    "                    else:\n",
    "                        df1[l][j] = f'background-image: linear-gradient(to left, transparent 50%, #f1a863 50%, #f1a863 {50-m1/2}%, transparent {50-m1/2}%)'\n",
    "\n",
    "            return df1\n",
    "\n",
    "        def display_compare(df,columns):\n",
    "            output.append(df.style.set_properties(**{'width': '300px','border-style':'solid','border-width':'1px'}).apply(lambda x: highlight_greater(x,columns), axis=None).format(lambda x: '''\n",
    "                                                                          <div style='max-width: 30%; min-width:30%;display:inline-block;'>{:,.2f}</div>\n",
    "                                                                          <div style='max-width: 30%; min-width:30%; display:inline-block;'>{:,.2f}</div>\n",
    "                                                                          <div style='max-width: 30%; min-width:30%; display:inline-block;color:blue'>{:,.2f}%</div>\n",
    "                                                                       '''.format(x[0],x[1],x[2]*100)).render())\n",
    "        display_compare(df1,cmpcolumns)\n",
    "\n",
    "        df3 = pandas.DataFrame('', index=sarcmp.index, columns=sarcolumns)\n",
    "        for l in sarcolumns:\n",
    "            for j in df3.index:\n",
    "                df3[l][j]=[sarcmp[l+\"_\"+lrun][j],sarcmp[l+\"_\"+rrun][j],sarcmp[l+\"_\"+lrun][j]/sarcmp[l+\"_\"+rrun][j]-1]\n",
    "        display_compare(df3,sarcolumns)\n",
    "\n",
    "        \n",
    "        df2 = pandas.DataFrame('', index=emoncmp.index, columns=emoncolumns)\n",
    "        for l in emoncolumns:\n",
    "            for j in df2.index:\n",
    "                df2[l][j]=[emoncmp[l+\"_\"+lrun][j],emoncmp[l+\"_\"+rrun][j],emoncmp[l+\"_\"+lrun][j]/emoncmp[l+\"_\"+rrun][j]-1]\n",
    "        display_compare(df2,emoncolumns)\n",
    "\n",
    "        print(\"time breakdown\")\n",
    "        ################################ time breakdown ##################################################################################################\n",
    "        timel=appals.show_time_metric(plot=False)\n",
    "        timer=appals2.show_time_metric(plot=False)\n",
    "        timer.columns=[l.replace(\"scan time\",\"totaltime_batchscan\") for l in timer.columns]\n",
    "        timel.columns=[l.replace(\"scan time\",\"totaltime_batchscan\") for l in timel.columns]\n",
    "        rcols=timer.columns\n",
    "        lcols=[]\n",
    "        for c in [l.split(\"%\")[1][1:] for l in rcols]:\n",
    "            for t in timel.columns:\n",
    "                if t.endswith(c):\n",
    "                    lcols.append(t)\n",
    "        for t in timel.columns:\n",
    "            if t not in lcols:\n",
    "                lcols.append(t)\n",
    "        timel_adj=timel[lcols]\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "        ax=timel_adj.plot.bar(ax=axs[0],stacked=True)\n",
    "        list_values=timel_adj.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "        ax=timer.plot.bar(ax=axs[1],stacked=True)\n",
    "        list_values=timer.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "\n",
    "################################ critical time breakdown ##################################################################################################\n",
    "        timel=appals.show_time_metric(plot=False,taskids=[l[0].item() for l in appals.criticaltasks])\n",
    "        timer=appals2.show_time_metric(plot=False,taskids=[l[0].item() for l in appals2.criticaltasks])\n",
    "        timer.columns=[l.replace(\"scan time\",\"totaltime_batchscan\") for l in timer.columns]\n",
    "        timel.columns=[l.replace(\"scan time\",\"totaltime_batchscan\") for l in timel.columns]\n",
    "        rcols=timer.columns\n",
    "        lcols=[]\n",
    "        for c in [l.split(\"%\")[1][1:] for l in rcols]:\n",
    "            for t in timel.columns:\n",
    "                if t.endswith(c):\n",
    "                    lcols.append(t)\n",
    "        for t in timel.columns:\n",
    "            if t not in lcols:\n",
    "                lcols.append(t)\n",
    "        timel_adj=timel[lcols]\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "        ax=timel_adj.plot.bar(ax=axs[0],stacked=True)\n",
    "        list_values=timel_adj.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "        ax=timer.plot.bar(ax=axs[1],stacked=True)\n",
    "        list_values=timer.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "\n",
    "\n",
    "        ################################ hot stage ##########################################################################################################\n",
    "\n",
    "        hotstagel=appals.get_hottest_stages(plot=False)\n",
    "        hotstager=appals2.get_hottest_stages(plot=False)\n",
    "        hotstagel.style.format(lambda x: '''{:,.2f}'''.format(x))\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=max(hotstager.queryid))\n",
    "        cmap = matplotlib.cm.get_cmap('brg')\n",
    "        def setbkcolor(x):\n",
    "            rgba=cmap(norm(x['queryid']))\n",
    "            return ['background-color:rgba({:d},{:d},{:d},1); color:white'.format(int(rgba[0]*255),int(rgba[1]*255),int(rgba[2]*255))]*9\n",
    "\n",
    "        output.append(\"<table><tr><td>\" + hotstagel.style.apply(setbkcolor,axis=1).format({\"total_time\":lambda x: '{:,.2f}'.format(x),\"stdev_time\":lambda x: '{:,.2f}'.format(x),\"acc_total\":lambda x: '{:,.2%}'.format(x),\"total\":lambda x: '{:,.2%}'.format(x)}).render()+\n",
    "             \"</td><td>\" +  hotstager.style.apply(setbkcolor,axis=1).format({\"total_time\":lambda x: '{:,.2f}'.format(x),\"stdev_time\":lambda x: '{:,.2f}'.format(x),\"acc_total\":lambda x: '{:,.2%}'.format(x),\"total\":lambda x: '{:,.2%}'.format(x)}).render()+             \"</td></tr></table>\")\n",
    "\n",
    "        if not show_queryplan_diff:\n",
    "            return \"\\n\".join(output)\n",
    "        \n",
    "        print(\"hot stage\")\n",
    "\n",
    "        loperators=appals.getOperatorCount()\n",
    "        roperators=appals2.getOperatorCount()\n",
    "        loperators_rowcnt=appals.get_metric_output_rowcnt()\n",
    "        roperators_rowcnt=appals2.get_metric_output_rowcnt()\n",
    "        \n",
    "        def show_query_diff(queryid):\n",
    "            lops=pandas.DataFrame(loperators[queryid])\n",
    "            lops.columns=['calls_l']\n",
    "            lops=lops.loc[lops['calls_l'] >0]\n",
    "\n",
    "            rops=pandas.DataFrame(roperators[queryid])\n",
    "            rops.columns=[\"calls_r\"]\n",
    "            rops=rops.loc[rops['calls_r'] >0]\n",
    "            lops_row=pandas.DataFrame(loperators_rowcnt[queryid])\n",
    "            lops_row.columns=[\"rows_l\"]\n",
    "            lops_row=lops_row.loc[lops_row['rows_l'] >0]\n",
    "\n",
    "            rops_row=pandas.DataFrame(roperators_rowcnt[queryid])\n",
    "            rops_row.columns=[\"rows_r\"]\n",
    "            rops_row=rops_row.loc[rops_row['rows_r'] >0]\n",
    "\n",
    "            opscmp=pandas.merge(pandas.merge(pandas.merge(lops,rops,how=\"outer\",left_index=True,right_index=True),lops_row,how=\"outer\",left_index=True,right_index=True),rops_row,how=\"outer\",left_index=True,right_index=True)\n",
    "            opscmp=opscmp.fillna(\"\")\n",
    "\n",
    "            def set_bk_color_opscmp(x):\n",
    "                calls_l= 0 if x['calls_l']==\"\" else x['calls_l']\n",
    "                calls_r= 0 if x['calls_r']==\"\" else x['calls_r']\n",
    "                rows_l= 0 if x['rows_l']==\"\" else x['rows_l']\n",
    "                rows_r= 0 if x['rows_r']==\"\" else x['rows_r']\n",
    "\n",
    "                if calls_l > calls_r or rows_l > rows_r:\n",
    "                    return ['background-color:#eb6b34']*4\n",
    "                if calls_l < calls_r or rows_l < rows_r:\n",
    "                    return ['background-color:#8ad158']*4\n",
    "                return ['color:#dbd4d0']*4\n",
    "\n",
    "            output.append(opscmp.style.apply(set_bk_color_opscmp,axis=1).render())\n",
    "\n",
    "            planl=appals.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            planr=appals2.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            output.append(\"<table><tr><td>\"+planl+\"</td><td>\"+planr+\"</td></tr></table>\")\n",
    "\n",
    "        outputx=df1['output rows']\n",
    "        runtimex = df1['runtime']\n",
    "        for x in outputx.index:\n",
    "            if outputx[x][0]!=outputx[x][1] or runtimex[x][0]/runtimex[x][1]<0.95 or runtimex[x][0]/runtimex[x][1]>1.05:\n",
    "                print(\"query \",x,\" queryplan diff \")\n",
    "                output.append(f\"<p><font size=4 color=red>query{x} is different,{lrun} time: {df1['runtime'][x][0]}, {rrun} time: {df1['runtime'][x][1]}</font></p>\")\n",
    "                show_query_diff(x)\n",
    "        return \"\\n\".join(output)\n",
    "                              \n",
    "    def show_queryplan_diff(app2, queryid,**kwargs):\n",
    "        lbasedir=kwargs.get(\"basedir\",app2.basedir)\n",
    "        r_appid=kwargs.get(\"r_appid\",app2.appid)\n",
    "        \n",
    "        app=kwargs.get(\"rapp\",Application_Run(r_appid,basedir=lbasedir))\n",
    "\n",
    "        appals=app.analysis[\"app\"][\"als\"]\n",
    "        appals2=app2.analysis[\"app\"][\"als\"]\n",
    "\n",
    "        hotstagel=appals.get_hottest_stages(plot=False)\n",
    "        hotstager=appals2.get_hottest_stages(plot=False)\n",
    "        hotstagel.style.format(lambda x: '''{:,.2f}'''.format(x))\n",
    "\n",
    "        loperators=appals.getOperatorCount()\n",
    "        roperators=appals2.getOperatorCount()\n",
    "        loperators_rowcnt=appals.get_metric_output_rowcnt()\n",
    "        roperators_rowcnt=appals2.get_metric_output_rowcnt()\n",
    "\n",
    "        lrun=app.appid\n",
    "        rrun=app2.appid\n",
    "\n",
    "        output=[]\n",
    "\n",
    "        def show_query_diff(queryid):\n",
    "            lops=pandas.DataFrame(loperators[queryid])\n",
    "            lops.columns=['calls_l']\n",
    "            lops=lops.loc[lops['calls_l'] >0]\n",
    "\n",
    "            rops=pandas.DataFrame(roperators[queryid])\n",
    "            rops.columns=[\"calls_r\"]\n",
    "            rops=rops.loc[rops['calls_r'] >0]\n",
    "            lops_row=pandas.DataFrame(loperators_rowcnt[queryid])\n",
    "            lops_row.columns=[\"rows_l\"]\n",
    "            lops_row=lops_row.loc[lops_row['rows_l'] >0]\n",
    "\n",
    "            rops_row=pandas.DataFrame(roperators_rowcnt[queryid])\n",
    "            rops_row.columns=[\"rows_r\"]\n",
    "            rops_row=rops_row.loc[rops_row['rows_r'] >0]\n",
    "\n",
    "            opscmp=pandas.merge(pandas.merge(pandas.merge(lops,rops,how=\"outer\",left_index=True,right_index=True),lops_row,how=\"outer\",left_index=True,right_index=True),rops_row,how=\"outer\",left_index=True,right_index=True)\n",
    "            opscmp=opscmp.fillna(\"\")\n",
    "\n",
    "            def set_bk_color_opscmp(x):\n",
    "                calls_l= 0 if x['calls_l']==\"\" else x['calls_l']\n",
    "                calls_r= 0 if x['calls_r']==\"\" else x['calls_r']\n",
    "                rows_l= 0 if x['rows_l']==\"\" else x['rows_l']\n",
    "                rows_r= 0 if x['rows_r']==\"\" else x['rows_r']\n",
    "\n",
    "                if calls_l > calls_r or rows_l > rows_r:\n",
    "                    return ['background-color:#eb6b34']*4\n",
    "                if calls_l < calls_r or rows_l < rows_r:\n",
    "                    return ['background-color:#8ad158']*4\n",
    "                return ['color:#dbd4d0']*4\n",
    "\n",
    "            output.append(opscmp.style.apply(set_bk_color_opscmp,axis=1).render())\n",
    "\n",
    "            planl=appals.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            planr=appals2.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            output.append(\"<table><tr><td>\"+planl+\"</td><td>\"+planr+\"</td></tr></table>\")\n",
    "\n",
    "        x=queryid\n",
    "        print(\"query \",x,\" queryplan diff \")\n",
    "        #output.append(f\"<p><font size=4 color=red>query{x} is different,{lrun} time: {df1['runtime'][x][0]}, {rrun} time: {df1['runtime'][x][1]}</font></p>\")\n",
    "        show_query_diff(x)\n",
    "        display(HTML(\"\\n\".join(output)))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_rst(pdrst):\n",
    "    html='''<style type=\"text/css\">\n",
    "    .tg  {border-collapse:collapse;border-spacing:0;border-color:#aabcfe;}\n",
    "    .tg td{font-family:Courier New;font-size:18px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aabcfe;color:#669;background-color:#e8edff;}\n",
    "    .tg th{font-family:Courier New;font-size:18px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aabcfe;color:#039;background-color:#b9c9fe;}\n",
    "    .tg .tg-phtq{background-color:#D2E4FC;border-color:inherit;text-align:left;vertical-align:top}\n",
    "    .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    "    .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "    .tg .tg-phtq_v{background-color:#D2E4FC;border-color:inherit;text-align:left;vertical-align:top; color:#FF0000}\n",
    "    .tg .tg-0pky_v{border-color:inherit;text-align:left;vertical-align:top; color:#FF0000}\n",
    "    </style>\n",
    "    <table class=\"tg\">\n",
    "      <tr>\n",
    "    '''\n",
    "    cols=pdrst.columns  \n",
    "    html=html+''.join(['<th class=\"tg-c3ow\">{:s}</th>'.format(l) for l in cols if l!='app_id'])\n",
    "    html=html+'<th class=\"tg-c3ow\">spark log</th><th class=\"tg-c3ow\">trace_view</th></tr>'\n",
    "    for idx, r in pdrst.iterrows():\n",
    "        html=html+\"<tr>\"\n",
    "        html=html+\"\".join(['<td class=\"{:s}\">{:s}</td>'.format('tg-phtq' if l!='elapsed time' else 'tg-phtq_v', str(r[l])) for l in cols if l!='app_id'])\n",
    "        html=html+'''<td class=\"tg-phtq\"><a href=\"http://10.1.2.107:18080/history/{:s}/jobs\"> {:s}</a></td><td class=\"tg-phtq\"><a href=http://sr525:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{:s}.json> {:s}</a></td>'''.format(r['app_id'].appid,r['app_id'].appid,r['app_id'].appid,r['app_id'].appid)\n",
    "        html=html+\"</tr>\"\n",
    "    html=html+\"</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reduce_metric(pdrst,slave_id,metric,core,agg_func):\n",
    "    pdrst['rst']=pdrst.apply(lambda x:x['app_id'].get_reduce_metric(slave_id,metric,core,agg_func), axis=1)\n",
    "    for l in agg_func:\n",
    "        pdrst[get_alias_name(metric,l)]=pdrst.apply(lambda x:x['rst'].iloc[0][get_alias_name(metric,l)],axis=1)\n",
    "    return pdrst.drop(columns=['rst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_data(rsta):\n",
    "    for i in range(0,r):\n",
    "        m=rsta.loc[i,'emon_mem_bw']\n",
    "        if m>40000:\n",
    "            for j in range(i,i+5):\n",
    "                if j>=r:\n",
    "                    break\n",
    "                if rsta.loc[j,'emon_mem_bw']<40000:\n",
    "                    m=0\n",
    "                    break\n",
    "            rsta.loc[i,'emon_mem_bw']=m\n",
    "        else:\n",
    "            rsta.loc[i,'emon_mem_bw']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPCDS query map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "m='''1\tq01\n",
    "    2\tq02\n",
    "    3\tq03\n",
    "    4\tq04\n",
    "    5\tq05\n",
    "    6\tq06\n",
    "    7\tq07\n",
    "    8\tq08\n",
    "    9\tq09\n",
    "    10\tq10\n",
    "    11\tq11\n",
    "    12\tq12\n",
    "    13\tq13\n",
    "    14\tq14a\n",
    "    15\tq14b\n",
    "    16\tq15\n",
    "    17\tq16\n",
    "    18\tq17\n",
    "    19\tq18\n",
    "    20\tq19\n",
    "    21\tq20\n",
    "    22\tq21\n",
    "    23\tq22\n",
    "    24\tq23a\n",
    "    25\tq23b\n",
    "    26\tq24a\n",
    "    27\tq24b\n",
    "    28\tq25\n",
    "    29\tq26\n",
    "    30\tq27\n",
    "    31\tq28\n",
    "    32\tq29\n",
    "    33\tq30\n",
    "    34\tq31\n",
    "    35\tq32\n",
    "    36\tq33\n",
    "    37\tq34\n",
    "    38\tq35\n",
    "    39\tq36\n",
    "    40\tq37\n",
    "    41\tq38\n",
    "    42\tq39a\n",
    "    43\tq39b\n",
    "    44\tq40\n",
    "    45\tq41\n",
    "    46\tq42\n",
    "    47\tq43\n",
    "    48\tq44\n",
    "    49\tq45\n",
    "    50\tq46\n",
    "    51\tq47\n",
    "    52\tq48\n",
    "    53\tq49\n",
    "    54\tq50\n",
    "    55\tq51\n",
    "    56\tq52\n",
    "    57\tq53\n",
    "    58\tq54\n",
    "    59\tq55\n",
    "    60\tq56\n",
    "    61\tq57\n",
    "    62\tq58\n",
    "    63\tq59\n",
    "    64\tq60\n",
    "    65\tq61\n",
    "    66\tq62\n",
    "    67\tq63\n",
    "    68\tq64\n",
    "    69\tq65\n",
    "    70\tq66\n",
    "    71\tq67\n",
    "    72\tq68\n",
    "    73\tq69\n",
    "    74\tq70\n",
    "    75\tq71\n",
    "    76\tq72\n",
    "    77\tq73\n",
    "    78\tq74\n",
    "    79\tq75\n",
    "    80\tq76\n",
    "    81\tq77\n",
    "    82\tq78\n",
    "    83\tq79\n",
    "    84\tq80\n",
    "    85\tq81\n",
    "    86\tq82\n",
    "    87\tq83\n",
    "    88\tq84\n",
    "    89\tq85\n",
    "    90\tq86\n",
    "    91\tq87\n",
    "    92\tq88\n",
    "    93\tq89\n",
    "    94\tq90\n",
    "    95\tq91\n",
    "    96\tq92\n",
    "    97\tq93\n",
    "    98\tq94\n",
    "    99\tq95\n",
    "    100\tq96\n",
    "    101\tq97\n",
    "    102\tq98\n",
    "    103\tq99'''.split(\"\\n\")\n",
    "tpcds_query_map=[l.strip().split(\"\\t\") for l in m]\n",
    "tpcds_query_map={int(l[0]):l[1] for l in tpcds_query_map}"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "260.969px",
    "left": "1589.97px",
    "top": "105px",
    "width": "295px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
