{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import nested_scopes\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "#import pandas\n",
    "#pandas.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/xgboost/spark-3.0.0-bin-hadoop2.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, DateType,\n",
    "    TimestampType, StringType, LongType, IntegerType, DoubleType,FloatType)\n",
    "from pyspark.sql.functions import to_date, floor\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import lit\n",
    "import time, timeit\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import reduce\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/xgboost/xgboost4j/jars/xgboost4j_2.12-1.3.3.jar,/home/xgboost/xgboost4j/jars/xgboost4j-spark_2.12-1.3.3.jar,/home/xgboost/xgboost4j/jars/spark-arrow-datasource-0.9.0-jar-with-dependencies.jar pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/yuzhou/dev_spark/xgboost_hongze/baseline/xgboost4j_2.12-1.0.0-SNAPSHOT.jar,/home/yuzhou/dev_spark/xgboost_hongze/baseline/xgboost4j-spark_2.12-1.0.0-SNAPSHOT.jar pyspark-shell'\n",
    "os.environ['ARROW_LIBHDFS3_DIR'] = '/home/xgboost/miniconda3/lib'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/home/xgboost/miniconda3/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_datasource_jar = '/home/xgboost/xgboost4j/jars/spark-arrow-datasource-0.9.0-jar-with-dependencies.jar'\n",
    "\n",
    "def create_cntx(executors_per_node,cores_per_executor):\n",
    "    cache_size=30000\n",
    "    total_size=340000\n",
    "    print('executor per node: {:d}\\nparallelism: {:d}\\nmemory: {:d}m\\noffheap:{:d}m'.format(executors_per_node,nodes*executors_per_node*cores_per_executor,int(math.floor(nodes*total_size/(nodes*executors_per_node)))-1024-int(math.floor(cache_size/(nodes*executors_per_node))),int(math.floor(cache_size/(nodes*executors_per_node)))))\n",
    "    conf = SparkConf()\\\n",
    "        .set('spark.default.parallelism', '1')\\\n",
    "        .set('spark.executor.instances', '{:d}'.format(executors_per_node*nodes))\\\n",
    "        .set('spark.files.maxPartitionBytes', '256m')\\\n",
    "        .set('spark.app.name', 'test')\\\n",
    "        .set('spark.rdd.compress', 'False')\\\n",
    "        .set('spark.serializer','org.apache.spark.serializer.KryoSerializer')\\\n",
    "        .set('spark.executor.cores','{:d}'.format(cores_per_executor))\\\n",
    "        .set('spark.executor.memory', '{:d}m'.format(int(math.floor(total_size/(nodes*executors_per_node)))-1024-int(math.floor(cache_size/(nodes*executors_per_node)))))\\\n",
    "        .set('spark.task.cpus','1')\\\n",
    "        .set('spark.driver.memory','24g')\\\n",
    "        .set('spark.memory.offHeap.enabled','True')\\\n",
    "        .set('spark.memory.offHeap.size','{:d}m'.format(int(math.floor(cache_size/(nodes*executors_per_node)))))\\\n",
    "        .set('spark.executor.memoryOverhead','{:d}m'.format(int(math.floor(cache_size/(nodes*executors_per_node)))+3000))\\\n",
    "        .set('spark.sql.join.preferSortMergeJoin','False')\\\n",
    "        .set('spark.memory.storageFraction','0.5')\\\n",
    "        .set('spark.executor.extraJavaOptions',\\\n",
    "            '-XX:+UseParallelGC -XX:+UseParallelOldGC -verbose:gc -XX:+PrintGCDetails')\\\n",
    "        .set('spark.driver.maxResultSize', 0)\\\n",
    "        .set('spark.eventLog.enabled', 'True') \\\n",
    "        .set('spark.resourceProfile.executorReuse', 'True')\\\n",
    "        .set('spark.dynamicAllocation.enabled', 'True')\\\n",
    "        .set('spark.shuffle.service.enabled', 'False')\\\n",
    "        .set('spark.dynamicAllocation.shuffleTracking.enabled', 'True')\\\n",
    "        .set('spark.executorEnv.ARROW_LIBHDFS3_DIR', '/home/xgboost/miniconda3/lib')\\\n",
    "        .set('spark.executorEnv.LD_LIBRARY_PATH', '/home/xgboost/miniconda3/lib')\\\n",
    "        .set('spark.driver.extraClassPath', arrow_datasource_jar) \\\n",
    "        .set('spark.executor.extraClassPath', arrow_datasource_jar) \\\n",
    "        .set('spark.files', arrow_datasource_jar) \\\n",
    "        .set('spark.eventLog.dir','file:///home/xgboost/spark_events')\\\n",
    "        .set('spark.dynamicAllocation.exeutorIdleTimeout','10000')\\\n",
    "        .set('spark.dynamicAllocation.minExecutors', '{:d}'.format(executors_per_node*nodes))\\\n",
    "        .set('spark.dynamicAllocation.maxExecutors', '{:d}'.format(executors_per_node*nodes))\\\n",
    "        .set('spark.dynamicAllocation.initialExecutors', '{:d}'.format(executors_per_node*nodes))\\\n",
    "        .set('xgboost.spark.arrow.optimization.enabled', 'True') \\\n",
    "        .setAppName('spark-xgboost')\n",
    "\n",
    "    \n",
    "    spark = SparkSession.builder\\\n",
    "                .master(\"spark://sr572:7077\")\\\n",
    "                .config(conf=conf)\\\n",
    "                .getOrCreate()\n",
    "        \n",
    "    sc = spark.sparkContext\n",
    "    sc.setLogLevel('INFO')\n",
    "    sc.addPyFile('/home/xgboost/xgboost4j/sparkxgb_1.23.zip')\n",
    "    return sc, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bitdata_arrow_flattern(spark, path):\n",
    "    # spark.setConf('spark.sql.files.maxPartitionBytes', '1024')\n",
    "    #df = spark.read.format('arrow').load('hdfs://sr507/user/yuzhou/xgboost_36_files_double_label.dataframe.parquet')\n",
    "  \n",
    "    df = spark.read.format('arrow').load(path)\n",
    "    #df = spark.read.parquet(path)\n",
    "    print(\"DataFrame schma: \", df.schema)\n",
    "    #df.groupBy(F.spark_partition_id()).count().show(300)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(train_data, params):\n",
    "    t1 = timeit.default_timer()\n",
    "    print(\"{} : {}\".format(\"Start time\", t1))\n",
    "    xgboost =  XGBoostClassifier(**params)\n",
    "    # xgboost.setNumClass(3)\n",
    "    model = xgboost.fit(train_data)\n",
    "    t2 = timeit.default_timer()\n",
    "    train_time = t2 - t1\n",
    "    return model, train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /home/xgboost/spark-3.0.0-bin-hadoop2.7/logs/spark-xgboost-org.apache.spark.deploy.master.Master-1-sr572.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /home/xgboost/spark-3.0.0-bin-hadoop2.7/logs/spark-xgboost-org.apache.spark.deploy.worker.Worker-1-sr572.out\n"
     ]
    }
   ],
   "source": [
    "SPARK_HOME='/home/xgboost/spark-3.0.0-bin-hadoop2.7'\n",
    "!$SPARK_HOME/sbin/start-master.sh\n",
    "!$SPARK_HOME/sbin/start-slave.sh spark://sr572:7077 -c 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone Delpoy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Parameters: \n",
      " {'labelCol': 'delinquency_12', 'numRound': 100, 'maxDepth': 8, 'maxLeaves': 256, 'alpha': 0.9, 'eta': 0.1, 'gamma': 0.1, 'subsample': 1.0, 'reg_lambda': 1.0, 'scalePosWeight': 2.0, 'minChildWeight': 30.0, 'treeMethod': 'hist', 'objective': 'reg:squarederror', 'growPolicy': 'lossguide', 'numWorkers': 1, 'nthread': 8, 'verbosity': 3}\n",
      "executor per node: 1\n",
      "parallelism: 8\n",
      "memory: 308976m\n",
      "offheap:30000m\n",
      "DataFrame schma:  StructType(List(StructField(interest_rate,FloatType,true),StructField(current_actual_upb,FloatType,true),StructField(loan_age,FloatType,true),StructField(remaining_months_to_legal_maturity,FloatType,true),StructField(adj_remaining_months_to_maturity,FloatType,true),StructField(msa,FloatType,true),StructField(current_loan_delinquency_status,FloatType,true),StructField(foreclosure_costs,FloatType,true),StructField(prop_preservation_and_repair_costs,FloatType,true),StructField(asset_recovery_costs,FloatType,true),StructField(misc_holding_expenses,FloatType,true),StructField(holding_taxes,FloatType,true),StructField(net_sale_proceeds,FloatType,true),StructField(credit_enhancement_proceeds,FloatType,true),StructField(repurchase_make_whole_proceeds,FloatType,true),StructField(other_foreclosure_proceeds,FloatType,true),StructField(non_interest_bearing_upb,FloatType,true),StructField(principal_forgiveness_upb,FloatType,true),StructField(foreclosure_principal_write_off_amount,FloatType,true),StructField(servicer_idx,FloatType,true),StructField(mod_flag_idx,FloatType,true),StructField(zero_balance_code_idx,FloatType,true),StructField(repurchase_make_whole_proceeds_flag_idx,FloatType,true),StructField(servicing_activity_indicator_idx,FloatType,true),StructField(delinquency_12,DoubleType,true),StructField(orig_interest_rate,FloatType,true),StructField(orig_upb,FloatType,true),StructField(orig_loan_term,FloatType,true),StructField(orig_ltv,FloatType,true),StructField(orig_cltv,FloatType,true),StructField(num_borrowers,FloatType,true),StructField(dti,FloatType,true),StructField(borrower_credit_score,FloatType,true),StructField(num_units,FloatType,true),StructField(zip,FloatType,true),StructField(mortgage_insurance_percent,FloatType,true),StructField(coborrow_credit_score,FloatType,true),StructField(mortgage_insurance_type,FloatType,true),StructField(datatime,FloatType,true),StructField(orig_channel_idx,FloatType,true),StructField(first_home_buyer_idx,FloatType,true),StructField(loan_purpose_idx,FloatType,true),StructField(property_type_idx,FloatType,true),StructField(occupancy_status_idx,FloatType,true),StructField(property_state_idx,FloatType,true),StructField(relocation_mortgage_indicator_idx,FloatType,true),StructField(product_type_idx,FloatType,true)))\n",
      "Start time : 3193870.4472706\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: interest_rate, current_actual_upb, loan_age, remaining_months_to_legal_maturity, adj_remaining_months_to_maturity, msa, current_loan_delinquency_status, foreclosure_costs, prop_preservation_and_repair_costs, asset_recovery_costs, misc_holding_expenses, holding_taxes, net_sale_proceeds, credit_enhancement_proceeds, repurchase_make_whole_proceeds, other_foreclosure_proceeds, non_interest_bearing_upb, principal_forgiveness_upb, foreclosure_principal_write_off_amount, servicer_idx, mod_flag_idx, zero_balance_code_idx, repurchase_make_whole_proceeds_flag_idx, servicing_activity_indicator_idx, delinquency_12, orig_interest_rate, orig_upb, orig_loan_term, orig_ltv, orig_cltv, num_borrowers, dti, borrower_credit_score, num_units, zip, mortgage_insurance_percent, coborrow_credit_score, mortgage_insurance_type, datatime, orig_channel_idx, first_home_buyer_idx, loan_purpose_idx, property_type_idx, occupancy_status_idx, property_state_idx, relocation_mortgage_indicator_idx, product_type_idx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-759a87287685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mnRuns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnRuns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#trainingData, params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Completed training the model. Time(sec): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-392bf8584988>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(train_data, params)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mxgboost\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mXGBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# xgboost.setNumClass(3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: interest_rate, current_actual_upb, loan_age, remaining_months_to_legal_maturity, adj_remaining_months_to_maturity, msa, current_loan_delinquency_status, foreclosure_costs, prop_preservation_and_repair_costs, asset_recovery_costs, misc_holding_expenses, holding_taxes, net_sale_proceeds, credit_enhancement_proceeds, repurchase_make_whole_proceeds, other_foreclosure_proceeds, non_interest_bearing_upb, principal_forgiveness_upb, foreclosure_principal_write_off_amount, servicer_idx, mod_flag_idx, zero_balance_code_idx, repurchase_make_whole_proceeds_flag_idx, servicing_activity_indicator_idx, delinquency_12, orig_interest_rate, orig_upb, orig_loan_term, orig_ltv, orig_cltv, num_borrowers, dti, borrower_credit_score, num_units, zip, mortgage_insurance_percent, coborrow_credit_score, mortgage_insurance_type, datatime, orig_channel_idx, first_home_buyer_idx, loan_purpose_idx, property_type_idx, occupancy_status_idx, property_state_idx, relocation_mortgage_indicator_idx, product_type_idx"
     ]
    }
   ],
   "source": [
    "nodes=1\n",
    "executors_per_node=1\n",
    "cores_per_executor=8\n",
    "\n",
    "overall_start_time = timeit.default_timer()\n",
    "params = {  'labelCol': \"delinquency_12\",\n",
    "            'numRound': 100,\n",
    "            'maxDepth': 8,\n",
    "            'maxLeaves': 256,\n",
    "          'alpha': 0.9,\n",
    "          'eta': 0.1,\n",
    "          'gamma': 0.1,\n",
    "          'subsample': 1.0,\n",
    "          'reg_lambda': 1.0,\n",
    "          'scalePosWeight': 2.0,\n",
    "          'minChildWeight': 30.0,\n",
    "          'treeMethod': 'hist',\n",
    "          'objective': 'reg:squarederror', #if xgboost v0.82 needs to use 'reg:linear'. If >= 0.9, uses squarederror\n",
    "          'growPolicy': 'lossguide',  #depthwise\n",
    "          'numWorkers': 1, # the number of executor number\n",
    "          'nthread': 8, # the thread number in the executor process.\n",
    "          'verbosity': 3\n",
    "}\n",
    "print(\"XGBoost Parameters: \\n\", params)\n",
    "\n",
    "sc, spark=create_cntx(executors_per_node,cores_per_executor)\n",
    "#spark.sparkSession.conf.set('org.apache.spark.example.columnar.enabled', 'True')\n",
    "\n",
    "\n",
    "from sparkxgb import XGBoostClassifier\n",
    "\n",
    "path = 'file:///home/xgboost/data/xgboost_4M_float.dataframe.parquet'\n",
    "\n",
    "df=load_bitdata_arrow_flattern(spark, path)\n",
    "nRuns = 1\n",
    "for i in range(0, nRuns):\n",
    "    model, train_time  = run_train(df, params) #trainingData, params\n",
    "    print('Completed training the model. Time(sec): ', train_time)\n",
    "\n",
    "appid = sc.applicationId\n",
    " \n",
    "print('Overall time for {} runs: {}'.format(nRuns, timeit.default_timer() - overall_start_time))\n",
    "print(appid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping org.apache.spark.deploy.worker.Worker\n",
      "stopping org.apache.spark.deploy.master.Master\n"
     ]
    }
   ],
   "source": [
    "!$SPARK_HOME/sbin/stop-slave.sh\n",
    "!$SPARK_HOME/sbin/stop-master.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
